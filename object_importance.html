<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>On-Road Object Importance Estimation</title>	

<!--
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-40627132-1', 'ucsd.edu');
  ga('send', 'pageview');

</script>
!-->

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-40627132-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-40627132-1');
</script>


</head>
<body>
<center>
<h1 style="font-size: 25pt">On-Road Object Importance Estimation</h1>
<br>
<img src="images/w1.png" width=800px>
<br>
<br>
<hr style="height:3px;border:none;color:#808080;background-color:#808080;margin-top: -12px;margin-left: 0px;top: 50%;width: 100%;content: "";display: inline-block;"/>
<h1 style="font-size: 25pt">Example Video</h1>
<table border="0" align="center">
		<tr>
		<td> <img src="images/imp1.gif" width=400px>
<td> <img src="images/imp2.gif" width=400px>		
		</tr>
		</table>
<br>
<br>
<br>
<hr style="height:3px;border:none;color:#808080;background-color:#808080;margin-top: -12px;margin-left: 0px;top: 50%;width: 100%;content: "";display: inline-block;"/>
<h1 style="font-size: 25pt"><center>Paper</center></h1>
<h1 style="font-size: 15pt">
	    <p><strong><a href="./papers/areall.pdf">Are All Objects Equal? Deep Spatio-Temporal Importance Prediction in Driving Videos</a></strong><br>
	    <strong>E. Ohn-Bar</strong> and M. Trivedi<br>
	    <em>Pattern Recognition</em>, 2017<br>
		
		<a href="./papers/areall.pdf">
		<table border="0" align="center">
		<tr>
		<td> <img src="images/pr_Page_01.jpg" width=100px>
		<td> <img src="images/pr_Page_02.jpg" width=100px>
		<td> <img src="images/pr_Page_03.jpg" width=100px>
		<td> <img src="images/pr_Page_04.jpg" width=100px>
		<td> <img src="images/pr_Page_05.jpg" width=100px>
		<td> <img src="images/pr_Page_06.jpg" width=100px>
		<td> <img src="images/pr_Page_07.jpg" width=100px>
		<td> <img src="images/pr_Page_08.jpg" width=100px>
		<td> <img src="images/pr_Page_09.jpg" width=100px>
		<td> <img src="images/pr_Page_10.jpg" width=100px>
		<td> <img src="images/pr_Page_11.jpg" width=100px>
		<td> <img src="images/pr_Page_14.jpg" width=100px>
		</tr>
		</table>
		</a>
		<br>
		
		<br>
		<p><strong><a href="./papers/imp_icpr.pdf">What Makes an On-road Object Important?</a> (Oral, Best paper award finalist) </strong><br>
	    <strong>E. Ohn-Bar</strong> and M. Trivedi<br>
	    <em>IEEE International Conference on Pattern Recognition</em>, 2016<br> 
		</h1>
		
		<a href="./papers/imp_icpr.pdf">
		<table border="0" align="center">
		<tr>
		<td> <img src="images/bare_conf_Page_1.jpg" width=100px>
		<td> <img src="images/bare_conf_Page_2.jpg" width=100px>
		<td> <img src="images/bare_conf_Page_3.jpg" width=100px>
		<td> <img src="images/bare_conf_Page_4.jpg" width=100px>
		<td> <img src="images/bare_conf_Page_5.jpg" width=100px>
		<td> <img src="images/bare_conf_Page_6.jpg" width=100px>
		</tr>
		</table>
		</a>
		<br>
		
		
		
<hr style="height:3px;border:none;color:#808080;background-color:#808080;margin-top: -12px;margin-left: 0px;top: 50%;width: 100%;content: "";display: inline-block;"/>
<h1 style="font-size: 25pt"><center>Abstract</center></h1>	
<left>
<div align="left">
<h1 style="font-size: 15pt">Human drivers continuously attend to important scene elements in order to safely and smoothly navigate in intricate environments and under uncertainty. This paper develops a human-centric framework for object recognition by analyzing a notion of object importance, as measured in a spatio-temporal context of driving a vehicle. Given a video, a main research question in this paper is - which of the surrounding agents are most important? The answer inherently requires complex reasoning over the current driving task, object properties, scene context, intent, and possible future actions. Therefore, we find that various spatio-temporal cues are relevant for the importance classification task. Furthermore, we demonstrate the usefulness of the importance annotations in evaluating vision algorithms (specifically, for the task of object detection) in an application where trust in automation is imperative and errors are costly. Finally, we show that importance-guided training of object detection models results in improved detection performance of surrounding objects of higher importance. Hence, such models may be better suited for use in representing safety-critical situations, predicting surrounding agents' intentions, and in human-robot interactivity.
</div>
<center>
<h1 style="font-size: 25pt"><a href='https://github.com/eshed1/Object_Importance'>Code and Data</a></h1>	
	</center>
	
</body>
</html>