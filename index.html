<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<title>Eshed Ohn-Bar</title>
<link href='//fonts.googleapis.com/css?family=Lato:300,400,700' rel='stylesheet' type='text/css'>

<!--
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-40627132-1', 'ucsd.edu');
  ga('send', 'pageview');

	
</script>
!-->

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-40627132-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-40627132-1');
</script>


<script src="courses/jquery-1.js"></script>
  <link rel="stylesheet" href="bootstrap.css">
  <link rel="stylesheet" href="bootstrap-theme.css">
  <script src="courses/bootstrap.js"></script>

  <style>
     .panel-heading .accordion-toggle:before {
      font-family: 'Glyphicons Halflings';
      content: "\e114";
      float: left;
      color: grey;
      padding-right: 6px;
    }
    .panel-heading .accordion-toggle.collapsed:before {
      content: "\e080";
    }
table, th, td {
    border: 0px solid black;
    border-collapse: collapse;
}
img:hover{
  transition-duration: 0.0s;
  transition-timing-function: linear;
opacity:0.5;
}

  </style>

<style type="text/css">
</style>

<style>
@media screen and (max-device-width: 480px){
  body{
      -webkit-text-size-adjust: 100%;
        }
        }
p { font-size : 17px; }
h1 { padding : 0; margin : 0; font-size : 36px; }
h2 { font-size : 22px; margin : 0; padding : 0; }
body { padding : 0; font-family: 'Lato', sans-serif; font-size : 17px; background-color : rgb(25, 30, 30); }
body {
background: rgb(255,255,255);
}
.title { width : 1070px; margin : 20px auto; color : #000; margin-top : 30px; }
.title a, .title a:visited {color : #0080ff; position : relative;}
.container { width : 1200px; margin : 10px auto; border-radius: 20px; padding : 15px;  clear:both; }
#bio { height : 350px; position: relative; float : left; width : 650px; }
#me { border : 0 solid black; margin-bottom : 30px; border-radius : 20px; }
#sidebar { margin-left : 100px; margin-top : 100px; border : 0 solid black; float : left; margin-bottom : 0;}
#sidebar2 { margin-right : 50px; border : 0 solid black; float : left; margin-bottom : 0;}
a { text-decoration : none; }
a:hover { text-decoration : underline; }
a, a:visited { color : #0080ff; }
.publogo { margin-right : 20px; float : left; border : 0;}
.publication { clear : left; padding-bottom : 3px; }
.publication p { height : 100px; padding-top : 0px;}
.publication strong a { color : #000; }
.publication .links { position : relative; top : 15px }
.publication .links a { margin-right : 20px; }
.publication img { border-radius : 5px; }
.codelogo { margin-right : 50px; float : left; border : 0;}
.code { padding-bottom : 10px; vertical-align :middle; height : 150px !important; width : 550px;}
.code .download a { display : block; margin : 0 30px 0 0; float : left;}
.code strong { display : block; padding-bottom : 10px;}
.code strong a { color : #000; }
.code img { border-radius : 5px; }
.external a { margin : 0 10px; }
.external a.first { margin : 0 10px 0 0; }
</style>

<script type="text/javascript" src="//code.jquery.com/jquery-1.11.0.min.js"></script>
<script type="text/javascript" src="//code.jquery.com/jquery-migrate-1.2.1.min.js"></script>

<style>
* {
	outline:none;
 -webkit-box-sizing: border-box;
	-moz-box-sizing: border-box;
		 box-sizing: border-box;
}

html, body {min-height: 100vh;}

.timeline {
	width:800px;
	height: 20px;
	list-style: none;
	text-align: justify;
	position: relative;
	left: 50%;
	top: 50%;
	-webkit-transform: translate(-50%, -50%);
	   -moz-transform: translate(-50%, -50%);
	    -ms-transform: translate(-50%, -50%);
	     -o-transform: translate(-50%, -50%);
	        transform: translate(-50%, -50%);
	background: -moz-linear-gradient(top, rgba(255,255,255,0) 0%, rgba(255,255,255,0) 45%, rgba(255,255,255,1) 51%, rgba(255,255,255,0) 57%, rgba(255,255,255,0) 100%);
	background: -webkit-gradient(left top, left bottom, color-stop(0%, rgba(255,255,255,0)), color-stop(45%, rgba(255,255,255,0)), color-stop(51%, rgba(255,255,255,0)), color-stop(57%, rgba(255,255,255,0)), color-stop(100%, rgba(255,255,255,0)));
	background: -webkit-linear-gradient(top, rgba(255,255,255,0) 0%, rgba(255,255,255,0) 45%, rgba(255,255,255,1) 51%, rgba(255,255,255,0) 57%, rgba(255,255,255,0) 100%);
	background: -o-linear-gradient(top, rgba(255,255,255,0) 0%, rgba(255,255,255,0) 45%, rgba(255,255,255,1) 51%, rgba(255,255,255,0) 57%, rgba(255,255,255,0) 100%);
	background: -ms-linear-gradient(top, rgba(255,255,255,0) 0%, rgba(255,255,255,0) 45%, rgba(255,255,255,1) 51%, rgba(255,255,255,0) 57%, rgba(255,255,255,0) 100%);
	background: linear-gradient(to bottom, rgba(255,255,255,0) 0%, rgba(255,255,255,0) 45%, rgba(0,0,0,1) 51%, rgba(255,255,255,0) 57%, rgba(255,255,255,0) 100%);
}

.timeline:after {display: inline-block; content: ""; width: 100%;}

.timeline li {
	display: inline-block;
	width: 20px;
	height: 20px;
	background: #09795D;
	text-align: center;
	line-height: 1.2;
	position: relative;
	-webkit-border-radius: 50%;
	        border-radius: 50%;
}

.timeline li:before {
	display: inline-block;
	content: attr(data-year);
	font-size: 18px;
	position: absolute;
	left: 50%;
	-webkit-transform: translateX(-50%);
	   -moz-transform: translateX(-50%);
	    -ms-transform: translateX(-50%);
	     -o-transform: translateX(-50%);
	        transform: translateX(-50%);
}

.timeline li:nth-child(odd):before {
	top: -20px;
}
.timeline li:nth-child(even):before {
	top: -20px;
}

.timeline li:after {
	display: inline-block;
	content: attr(data-text);
	font-size: 16px;
	position: absolute;
	left: 50%;
	-webkit-transform: translateX(-50%);
	   -moz-transform: translateX(-50%);
	    -ms-transform: translateX(-50%);
	     -o-transform: translateX(-50%);
	        transform: translateX(-50%);
}

.timeline li:nth-child(odd):after {
	bottom: 0;
	margin-bottom: -5px;
	-webkit-transform: translate(-50%, 100%);
	   -moz-transform: translate(-50%, 100%);
	    -ms-transform: translate(-50%, 100%);
	     -o-transform: translate(-50%, 100%);
	        transform: translate(-50%, 100%);
}
.timeline li:nth-child(even):after {
	bottom: 0;
	margin-bottom: -5px;
	-webkit-transform: translate(-50%, 100%);
	   -moz-transform: translate(-50%, 100%);
	    -ms-transform: translate(-50%, 100%);
	     -o-transform: translate(-50%, 100%);
	        transform: translate(-50%, 100%);
}
</style>

</head>
<body>

<div class="title">
    <div id="bio">
        <div style="position:absolute;top:70px;">
		<img src="./images/H2Xlogo.png" width="90%">
		<br>
			<a style="text-decoration: underline; text-underline: single" href="#About">About</a> |
    <a style="text-decoration: underline; text-underline: single" href="#News">News</a> |
	<a style="text-decoration: underline; text-underline: single" href="#Team">Team</a> |
	<a style="text-decoration: underline; text-underline: single" href="#Publications">Publications</a> |
	<a style="text-decoration: underline; text-underline: single" href="#ProfessionalActivity">Activity</a> |
<a style="text-decoration: underline; text-underline: single" href="https://scholar.google.com/citations?user=p9zVBV4AAAAJ&hl=en&oi=ao">Scholar</a>
			<br><br>		
            <p style="line-height:23px;">Eshed Ohn-Bar<br>
            Assistant Professor<br>
			Boston University<br>
            Email: <a href="mailto:eohnbar@gmail.com">eohnbar [at] gmail [dot] com</a></p>
            <!-- <p class="external"><a href="https://scholar.google.com/citations?user=p9zVBV4AAAAJ&hl=en&oi=ao">Google Scholar</a></p>
			<a href="eohnbar_cv.pdf" class="first">CV</a>&bull; !-->
           
<br>
        </div>
    </div>
<!--   
   <div id="sidebar" style="position:absolute;top:125px;">&nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;	&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;   <img src="images/fig_website2.jpg" id="eshed" width="200" itemprop="photo" style="border-radius: 30px 30px 30px 30px; box-shadow: 0px 0px 10px #888;"  /></div>
!-->
 <div id="sidebar"> <img src="images/fig_website2.jpg" id="eshed" width="200" itemprop="photo" style="border-radius: 30px 30px 30px 30px; box-shadow: 0px 0px 10px #888;"  /></div>

</div>


<!-- left:1200px; In particular, we are interested in advancing the state-of-the-art of systems that can efficiently understand and adaptively interact with humans in their environment. 
   I am interested in machine intelligence for real-world, embodied, assistive and autonomous systems.
	   !-->
<div class="container">
    <a name="About"></a>
    <h2>About</h2>
	At Boston University, I'm heading the <strong>Human-to-Everything (H2X) Lab</strong>. 
	Our overarching goal is to develop intelligent technologies with robust autonomy and real-time assistance capabilities.
	Our research spans machine intelligence, computer vision, and systems for human-machine interaction (particularly for accessibility). 
	<br>
	<br> 
	<strong>I am looking for two PhD students to join the lab in 2025. If you are interested in joining the lab to research machine learning and perception for intelligent systems, please email me your CV.</strong>  
</div>

	<!--
<div class="container">
	<br>  
	<ul class="timeline">
	<a href="https://www.krafael.org.il/en/"><li data-year="2005" data-text="Rafael Village Volunteer"></li></a>
	<li data-year="2010" data-text="UCLA B.S.Math, M.Ed."></li>
	<a href="https://en.wikipedia.org/wiki/Theodore_Roosevelt_High_School_(Los_Angeles)"><li data-year="2011" data-text="Roosevelt High Teacher"></li></a>
	<a href="https://www.apple.com/"><li data-year="2015" data-text="Apple SPG"></li></a>
	<li data-year="2017" data-text="UCSD Ph.D. EE"></li></a>
	<a href="http://www.cs.cmu.edu/~kkitani/"><li data-year="2018" data-text="CMU Robotics Institute"></li></a>
	<a href="https://avg.is.tuebingen.mpg.de/"><li data-year="2019" data-text="MPI Intelligent Systems"></li></a>
	<a href="https://www.bu.edu/eng/profile/ohn-bar-eshed/"><li data-year="2020" data-text="H2X Lab Boston University"></li></a>
</ul>
<br>
</div>
--!>

	<a name="News"></a>
<div class="container">
    <h2>News</h2>
   <ul>
	<li>We are co-organizing the third workshop on accessibility, computer vision, and agent systems at CVPR 2024 [<a href="https://accessibility-cv.github.io/">website and challenge</a>]  
<li>We are co-organizing the second workshop on accessibility, computer vision, and autonomous systems at CVPR 2023 [<a href="https://accessibility-cv.github.io/">website and challenge</a>]  
 <li>Thank you RedHat for <a href="https://research.redhat.com/blog/2023/01/26/meet-the-new-2023-red-hat-collaboratory-research-incubation-award-recipients/">research awards on scalable systems!</a>
   <li><a href="https://motional.com/news/motional-boston-university-partner-on-navigational-assistance-research-for-the-blind">Read about our collaboration with Motional</a> to ensure autonomous vehicles are accessible to individuals with visual impairments.
   <li>Thank you NSF for supporting our research in accessibility and mobile systems (grant IIS-2152077). 
   <li>1 paper accepted to ECCV 2022 on assistive mobility for individuals with visual impairments. <a href="https://youtu.be/bcgD5wHZmSs">Video</a>
   <li>Thank you RedHat for a Student Research Award!
   <li>Our Senior Design Project received the BU Engineering Design Excellence Award! 
<li>We are co-organizing the first workshop on accessibility, computer vision, and autonomous systems at CVPR 2022 [<a href="https://accessibility-cv.github.io/">website and challenge</a>]  
<li>1 paper accepted to CVPR 2022 on learning large-scale policies for navigation systems. <a href="https://www.youtube.com/watch?v=nWhdxx_9o58">Video</a>
<li>Thank you RedHat for a Research Incubation Award!
<li>Thank you BU for a Junior Faculty Fellowship (Hariri Institute for Computing) and a Peter J. Levine Career Development Award.
<li>1 paper accepted to ICCV 2021 on computer vision and accessibility <a href="https://www.youtube.com/watch?v=z_YwWIZWg58">Video</a>
<li>1 paper accepted to CVPR 2021 <a href="https://www.bu.edu/articles/2021/self-taught-self-driving-cars/">Read more</a> <a href="https://www.youtube.com/watch?v=ApyIcoTDyc8"> Video</a>
<li>We are semifinalists in the DOT Inclusive Design Challenge! <a href="https://www.transportation.gov/briefing-room/us-department-transportation-announces-over-41-million-awards-innovative-technologies">Press release</a> &nbsp;  <a href="https://www.bu.edu/eng/2021/02/01/accessible-autonomous-vehicle-system-wins-semifinalist-position-in-dot-competition/">More info</a>
<li>Thank you BU CISE for a grant for interactive learning of assistive systems.
<li>Best paper award at the Web for All conference! 
<li>Received a Humboldt fellowship, visiting MPI and working with <a href = "http://www.cvlibs.net/">Andreas Geiger</a>.
<li>I am honored to have received the <a href="https://www.ieee-itss.org/awards-best-dissertation"> 2017 IEEE Intelligent Transportation Systems Society best PhD dissertation award</a>.
</ul>
</div>
<!--
<li>Like challenging trajectory prediction problems? Check out the <a href="https://github.com/eshed1/PING">Github</a> from our CoRL paper.
<li>Participated in the CVPR 2017 doctoral consortium.
<li>We are co-organizing the <a href="http://icvl.ee.ic.ac.uk/hands17/">3rd workshop on observing and understanding hands in action (HANDS)</a> at ICCV 2017.
<li>Gave at talk at the CMU VASC seminar about <a href="https://www.cs.cmu.edu/calendar/thu-2017-02-16-1500/vision-and-autonomous-systems-seminar">human-centered autonomous vehicles</a>.
<li>Best paper award runner-up at ICPR 2016</a>!
<li>Gave a keynote talk about hands and intelligent vehicles at the CVPR-HANDS workshop.
<li>We are co-organizing the 2nd workshop on observing and understanding hands in action (HANDS) at CVPR 2016.
<li>We are co-organizing the 1st workshop on observing and understanding hands in action (HANDS) at CVPR 2015.
<li>First place in the KITTI car detection and orientation estimation challenge at ECCV 2014!
<li>Best industry related paper award (BIRPA) runner up at ICPR 2014!
<li>Best paper award at the IEEE Workshop on Analysis and Modeling of Faces and Gestures (with CVPR, 2013)!
</div>
-->
	
<div class="container">
	<a name="Team"></a>
	<h2>Team</h2>
	<ul>
	<li><a href="https://jimuyangz.github.io/">Jimuyang (Jim) Zhang (PhD)</a>
	<li><a href="https://www.linkedin.com/in/zhongkai-leon-shangguan-52b465152">Zhongkai Shangguan (PhD)</a>
	<li><a href="https://ruizhaoz.github.io/">Ruizhao Zhu (PhD)</a>
	<li><a href="https://sites.google.com/view/heejae-kim/">Hee Jae Kim (PhD)</a>
	<li><a href="https://cs-people.bu.edu/leilai/">Lei Lai (PhD)</a>
	<li><a href="https://www.masakikuribayashi.com/">Masaki Kuribayashi (PhD, Visiting from Waseda University)</a></li>
	<li>Zanming Huang (MS)
	<li>Animikh Aich (MS)
	<li>Shuhei Fujita (MS)
	<li>Himanshu Patil (MS)
	<li>Kathakoli Sengupta (MS)
	<li>Sandesh Bharadwaj (MS)
	<li>Fadi Kidess (BS)
	<li>Jason Lee (BS)
	<li>Christian So (BS)
	</ul>
	<br>
	<h2>Past Members</h2>
	<ul>
	<li>Mariia Kharchenko (now at UMD)</li>
	<li>Kevin Vogt-Lowell (now at MIT)</li>
	<li>Shun Zhang (now at UCSD)</li>
	<li>Luca Guidi (now at UIUC)</li>
	<li>Matt Boyd (now at MIT)</li>
	<li>Minglan Zheng (now at CMU)</li>
	<li>Ameera Iftekhar (now at Anduril)</li>
	<li>Anish Yennapusa (now at AWS)</li>
</ul>
<!--<img src="images/lab.jpg" id="lab" width="600" itemprop="photo" style="border-radius: 30px 30px 30px 30px; box-shadow: 0px 0px 10px #888;"/>
		 <div id="sidebar2" style="position:absolute;left:900px;"><img src="images/lab.jpg" id="lab" width="480" itemprop="photo" style="border-radius: 30px 30px 30px 30px; box-shadow: 0px 0px 10px #888;"/></div>
<div id="sidebar" style="position:absolute;top:125px;"> style="position:absolute;left:700px;" !-->
</div>


<div class="container">
For code and project pages, please see our <a href="https://github.com/h2xlab">GitHub repo.</a> 
</div>


<div class="container">
	<a name="Publications"></a>
<h2>Publications <a style="text-decoration: underline; text-underline: single" href="#2024">2024</a> | <a style="text-decoration: underline; text-underline: single" href="#2023">2023</a> | <a style="text-decoration: underline; text-underline: single" href="#2022">2022</a> | <a style="text-decoration: underline; text-underline: single" href="#2021">2021</a> | <a style="text-decoration: underline; text-underline: single" href="#2020">2020</a> | <a style="text-decoration: underline; text-underline: single" href="#2019">2019</a> | <a style="text-decoration: underline; text-underline: single" href="#2018">2018</a> | <a style="text-decoration: underline; text-underline: single" href="#2017">2017</a> | <a style="text-decoration: underline; text-underline: single" href="#2016">2016</a> | <a style="text-decoration: underline; text-underline: single" href="#2015">2015</a> |
    <a style="text-decoration: underline; text-underline: single" href="#2014">2014</a> |
	<a style="text-decoration: underline; text-underline: single" href="#2013">2013</a></h2>

	
<a name="2024">2024</a> <hr style="height:3px;border:none;color:#808080;background-color:#808080;margin-top: -12px;margin-left: 50px;top: 50%;width: 100%;content: "";display: inline-block;"/>

	<div class="publication">
	    <a href=""><img src="./images/blindways.gif" class="publogo" width="150" height ="110"></a>
	    <p><strong><a href="">Text to Blind Motion</a></strong><br>
	H. Kim, K. Sengupta, M. Kuribayashi, H. Kacorri, E. Ohn-Bar<br>
	    <em>Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2024 <br>
		    [<a href="https://blindways.github.io/">project page</a>]
    </div>

	<div class="publication">
	    <a href=""><img src="./images/ecri.jpg" class="publogo" width="150" height ="110"></a>
	    <p><strong><a href="">Scalable Early Childhood Reading Performance Prediction</a></strong><br>
	Z. Shangguan, Z. Huang, E. Ohn-Bar, O. Ozernov-Palchik, D. Kosty, M. Stoolmiller, H. Fien<br>
	    <em>Neural Information Processing Systems (<strong>NeurIPS</strong>)</em>, 2024 <br>
		    [<a href="https://ecri-data.github.io/">project page</a>]
    </div>
	
	
	<div class="publication">
	    <a href="https://unilcd.github.io/"><img src="./images/ulcd.png" class="publogo" width="150" height ="110"></a>
	    <p><strong><a href="https://unilcd.github.io/">Unified Local-Cloud Decision-Making via Reinforcement Learning</a></strong><br>
		  K. Sengupta, Z. Shangguan, S. Bharadwaj, S. Arora, E. Ohn-Bar, and R. Mancuso<br>
	    <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2024 <br>
		    [<a href="https://unilcd.github.io/">project page</a>]
    </div>
		<div class="publication">
	    <a href="./papers/nemo.pdf"><img src="./images/nemo.png" class="publogo" width="150" height ="110"></a>
	    <p><strong><a href="https://eccv.ecva.net/virtual/2024/poster/250">Neural Volumetric World Models for Autonomous Driving</a></strong><br>
		  Z. Huang, J. Zhang, and E. Ohn-Bar<br>
	    <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2024<br>
		    [<a href="https://eccv.ecva.net/virtual/2024/poster/250/">project page</a>]
    </div>

	
	<div class="publication">
	    <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Kim_Motion_Diversification_Networks_CVPR_2024_paper.pdf"><img src="./images/MDN_s.gif" class="publogo" width="150" height ="110"></a>
	    <p><strong><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Kim_Motion_Diversification_Networks_CVPR_2024_paper.pdf">Motion Diversification Networks</a></strong><br>
		     H. Kim and E. Ohn-Bar<br>
	    <em>Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2024<br>
		[<a href="https://mdncvpr.github.io/">project page</a>]
    </div>	

		<div class="publication">
	    <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Feedback-Guided_Autonomous_Driving_CVPR_2024_paper.pdf"><img src="./images/fed.jpg" class="publogo" width="150" height ="110"></a> 
	    <p><strong><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Zhang_Feedback-Guided_Autonomous_Driving_CVPR_2024_paper.pdf">Feedback-Guided Autonomous Driving</a></strong><font color="red"> (highlight, top 2.8% of submissions)</font><br>
		     J. Zhang, Z. Huang, A. Ray, and E. Ohn-Bar<br>
	    <em>Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2024<br>
		[<a href="https://fedaltothemetal.github.io/">project page</a>]
    </div>	

	<div class="publication">
	    <a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Lai_Uncertainty-Guided_Never-Ending_Learning_to_Drive_CVPR_2024_paper.pdf"><img src="./images/infdriver.jpg" class="publogo" width="150" height ="110"></a>
	    <p><strong><a href="https://openaccess.thecvf.com/content/CVPR2024/papers/Lai_Uncertainty-Guided_Never-Ending_Learning_to_Drive_CVPR_2024_paper.pdf">Uncertainty-Guided Never-Ending Learning to Drive</a></strong><br>
		     L. Lai, E. Ohn-Bar, S. Arora, and J. Yi<br>
	    <em>Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2024<br>
		    [<a href="https://infdriver.github.io/">project page</a>]
    </div>	


	
	
<a name="2023">2023</a> <hr style="height:3px;border:none;color:#808080;background-color:#808080;margin-top: -12px;margin-left: 50px;top: 50%;width: 100%;content: "";display: inline-block;"/>

	<div class="publication">
	    <a href="https://arxiv.org/abs/2309.12295"><img src="./images/lda.jpg" class="publogo" width="150" height ="110"></a>
	    <p><strong><a href="https://arxiv.org/abs/2309.12295">Learning to Drive Anywhere</a></strong><br>
		     R. Zhu, P. Huang, E. Ohn-Bar, and V. Saligrama<br>
	    <em>Conference on Robot Learning (<strong>CoRL</strong>)</em>, 2023<br>
		[<a href="https://any-d.github.io/">project page</a>]
    </div>	

	<div class="publication">
	    <a href="https://genxvo.github.io/"><img src="./images/xvo2.jpg" class="publogo" width="150" height ="110"></a>
	    <p><strong><a href="https://genxvo.github.io/">XVO: Generalized Visual Odometry via Cross-Modal Self-Training</a></strong><br>
		    L. Lai, Z. Shangguan, J. Zhang, and E. Ohn-Bar<br>
	    <em>International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2023<br>
		[<a href="https://genxvo.github.io/">project page</a>]
    </div>	

	
<div class="publication">
	    <a href="https://catdrive.github.io/resources/CaT.pdf"><img src="./papers/cat-driving-serious.gif" class="publogo" width="150" height ="110"></a>
	    <p><strong><a href="https://catdrive.github.io/resources/CaT.pdf">Coaching a Teachable Student</a></strong> <font color="red">(highlight, top 2.6% of submissions)</font><br>
		J. Zhang, Z. Huang, and E. Ohn-Bar<br>
	    <em>Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2023<br>
		[<a href="https://catdrive.github.io/">project page</a>]
    </div>	
	
<a name="2022">2022</a> <hr style="height:3px;border:none;color:#808080;background-color:#808080;margin-top: -12px;margin-left: 50px;top: 50%;width: 100%;content: "";display: inline-block;"/>

<div class="publication">
	    <a href="./papers/assister_eccv2022.pdf"><img src="./images/assister.png" class="publogo" width="150" height ="110"></a>
	    <p><strong><a href="./papers/assister_eccv2022.pdf">ASSISTER: Assistive Navigation via Conditional Instruction Generation</a></strong><br>
		Z. Huang*, Z. Shangguan*, J. Zhang, G. Bar, M. Boyd, and E. Ohn-Bar<br>
	    <em>European Conference on Computer Vision (<strong>ECCV</strong>)</em>, 2022<br>
		[<a href="https://github.com/h2xlab/ASSISTER">code</a>]
    </div>
	
<div class="publication">
	    <a href="https://arxiv.org/abs/2204.10320"><img src="./images/selfd.jpg" class="publogo" width="150" height ="150"></a>
	    <p><strong><a href="https://arxiv.org/abs/2204.10320">SelfD: Self-Learning Large-Scale Driving Policies From the Web</a></strong><br>
	    J. Zhang, R. Zhu, and E. Ohn-Bar<br>
	    <em>Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2022<br>
		[<a href="https://drive.google.com/drive/folders/1bHyWJTFLxQHrBo7PCtfPXEzwKxDa7W5X?usp=sharing">data</a>]
    </div>
	
<a name="2021">2021</a> <hr style="height:3px;border:none;color:#808080;background-color:#808080;margin-top: -12px;margin-left: 50px;top: 50%;width: 100%;content: "";display: inline-block;"/>

<div class="publication">
	    <a href="https://bit.ly/2X8sYoX"><img src="./images/xworld.jpg" class="publogo" width="150" height ="110"></a>
	    <p><strong><a href="https://bit.ly/2X8sYoX">X-World: Accessibility, Vision, and Autonomy Meet</a></strong><br>
	    J. Zhang*, M. Zheng*, M. Boyd, and E. Ohn-Bar<br>
	    <em>International Conference on Computer Vision (<strong>ICCV</strong>)</em>, 2021<br> 
		[<a href="https://eval.ai/web/challenges/challenge-page/1690/overview">evaluation server and data</a>]
    </div>
	
<div class="publication">
	    <a href="./papers/lbw.pdf"><img src="./images/lbw2.jpg" class="publogo" width="150" height ="110"></a>
	    <p><strong><a href="./papers/lbw.pdf">Learning by Watching</a></strong><br>
	    J. Zhang and E. Ohn-Bar<br>
	    <em>Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2021<br>  
    </div>
	
	
	<a name="2020">2020</a> <hr style="height:3px;border:none;color:#808080;background-color:#808080;margin-top: -12px;margin-left: 50px;top: 50%;width: 100%;content: "";display: inline-block;"/>

	
<div class="publication">
	    <a href="./papers/Ohn-Bar2020CVPR.pdf"><img src="./images/intro.png" class="publogo" width="150" height ="110"></a>
	    <p><strong><a href="./papers/Ohn-Bar2020CVPR.pdf">Learning Situational Driving</a></strong><br>
	    E. Ohn-Bar, A. Prakash, A. Behl, K. Chitta, and A. Geiger<br>
	    <em>Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2020<br>
    </div>
	
	<div class="publication">
	    <a href="./papers/Prakash2020CVPR.pdf"><img src="./images/prakash.png" class="publogo" width="150" height ="110"></a>
	    <p><strong><a href="./papers/Prakash2020CVPR.pdf">Exploring Data Aggregation in Policy Learning for Vision-based Urban Autonomous Driving</a></strong><br>
	    A. Prakash, A. Behl, E. Ohn-Bar, K. Chitta, and A. Geiger<br>
	    <em>Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2020<br>
    </div>
	
	<div class="publication">
	    <a href="./papers/Behl2020IROS.pdf"><img src="./images/Behl2020IROS.png" class="publogo" width="150" height ="110"></a>
	    <p><strong><a href="./papers/Behl2020IROS.pdf">Label Efficient Visual Abstractions for Autonomous Driving</a></strong><br>
	    A. Behl, K. Chitta, A. Prakash, E. Ohn-Bar, and A. Geiger<br>
	    <em>International Conference on Intelligent Robots and Systems (<strong>IROS</strong>)</em>, 2020<br>
    </div>
	
	<div class="publication">
	    <a href="./papers/guerreiro2020virtual.pdf"><img src="./images/ijhcs.png" class="publogo" width="150" height ="110"></a>
	    <p><strong><a href="./papers/guerreiro2020virtual.pdf">Virtual Navigation for Blind People: Transferring Route Knowledge to the Real-World</a></strong><br>
	    J. Guerreiro, D. Sato, D. Ahmetovic, E. Ohn-Bar, K. Kitani, and C. Asakawa<br>
	    <em>International Journal of Human-Computer Studies (<strong>IJHCS</strong>)</em>, 2020<br>
    </div>
	
	
	<div class="publication">
	    <a href="./papers/Higuchi2020.pdf"><img src="./images/tiis4.png" class="publogo" width="150" height ="110"></a>
	    <p><strong><a href="./papers/Higuchi2020.pdf">Learning Context-Dependent Personal Preferences for Adaptive Recommendation</a></strong><br>
	    K. Higuchi, H. Tsuchida, E. Ohn-Bar, Y. Sato, and K. Kitani<br>
	    <em>ACM Transaction on Interactive Intelligent Systems (<strong>T-IIS</strong>)</em>, 2020<br>
    </div>
	
<a name="2019">2019</a> <hr style="height:3px;border:none;color:#808080;background-color:#808080;margin-top: -12px;margin-left: 50px;top: 50%;width: 100%;content: "";display: inline-block;"/>

<div class="publication">
	    <a href="./papers/aexp4_IROS19.pdf"><img src="./images/aexp4a.png" class="publogo" width="150" height ="110"></a>
	    <p><strong><a href="./papers/aexp4_IROS19.pdf">A-EXP4: Online Social Policy Learning for Adaptive Robot-Pedestrian Interaction</a></strong><br>
	    P. Jin, <strong>E. Ohn-Bar</strong>, K. Kitani, and C. Asakawa<br>
	    <em>International Conference on Intelligent Robots and Systems (<strong>IROS</strong>)</em>, 2019<br>
		[<a href="./papers/aexp4_IROS19.pdf">pdf</a> | <a href="./papers/aexp4.txt">bibtex</a>]
    </div>
	<br>
	
 <div class="publication">
	    <a href="./papers/Time2Col_IROS19.pdf"><img src="./images/p.jpg" class="publogo" width="150" height ="110"></a>
	    <p><strong><a href="./papers/Time2Col_IROS19.pdf">Forecasting Time-To-Collision from Monocular Video: Feasibility, Dataset, and Challenges</a></strong><br>
	    A. Manglik, X. Weng, <strong>E. Ohn-Bar</strong>, and K. Kitani<br>
	    <em>International Conference on Intelligent Robots and Systems (<strong>IROS</strong>)</em>, 2019<br>
		[<a href="./papers/Time2Col_IROS19.pdf">pdf</a> | <a href="https://www.youtube.com/watch?v=I0cw872DXLg">demo</a> | <a href="https://github.com/aashi7/NearCollision">code</a> | <a href="https://aashi7.github.io/NearCollision.html">project</a> | <a href="./papers/collision.txt">bibtex</a>]
    </div>
	<br>
	
		 <div class="publication">
	    <a href="./papers/peds_RSS.pdf"><img src="./images/pedst.png" class="publogo" width="150" height ="110"></a>
	    <p><strong><a href="./papers/peds_RSS.pdf">Towards Understanding Interaction of Visually Impaired Navigators with Surrounding Pedestrians</a></strong><br>
	    <strong>E. Ohn-Bar</strong>, J. Guerreiro, D. Ahmetovic, K. Kitani, and C. Asakawa<br>
	    <em>H-Augmented Workshop, Robotics: Science and Systems (<strong>RSS</strong>)</em>, 2019<br>
		[<a href="./papers/peds_RSS.pdf">pdf</a> | <a href="./papers/peds_RSS.txt">bibtex</a>]
    </div>
	<br>
	<br>
	
	
		 <div class="publication">
	    <a href="https://www.researchgate.net/publication/331876129_Impact_of_Expertise_on_Interaction_Preferences_for_Navigation_Assistance_of_Visually_Impaired_Individuals"><img src="./images/w4.png" class="publogo" width="150" height ="110"></a>
	    <p><strong><a href="https://www.researchgate.net/publication/331876129_Impact_of_Expertise_on_Interaction_Preferences_for_Navigation_Assistance_of_Visually_Impaired_Individuals">Impact of Expertise on Interaction Preferences for Navigation Assistance of Visually Impaired Individuals</a></strong><br>
	    D. Ahmetovic, J. Guerreiro, <strong>E. Ohn-Bar</strong>, K. Kitani, and C. Asakawa<br>
	    <em>Web for All Conference (<strong>W4A</strong>)</em>, 2019<br>
		<font color="red"><strong>Best Paper Award</font></a></strong><br>
	    [<a href="https://www.researchgate.net/publication/331876129_Impact_of_Expertise_on_Interaction_Preferences_for_Navigation_Assistance_of_Visually_Impaired_Individuals">pdf</a> | <a href="./papers/w4a2019.txt">bibtex</a>]
    </div>
	
<a name="2018">2018</a> <hr style="height:3px;border:none;color:#808080;background-color:#808080;margin-top: -12px;margin-left: 50px;top: 50%;width: 100%;content: "";display: inline-block;"/>

		 <div class="publication">
	    <a href="https://arxiv.org/pdf/1804.04118.pdf"><img src="./images/pingc.png" class="publogo" width="150" height ="110"></a>
	    <p><strong><a href="https://arxiv.org/pdf/1804.04118.pdf">Personalized Dynamics Models for Adaptive Assistive Navigation Systems <font color="red"> (oral presentation)</font></a></strong><br>
	    <strong>E. Ohn-Bar</strong>, K. Kitani, and C. Asakawa<br>
	    <em>2nd Conference on Robot Learning (<strong>CoRL</strong>), Proceedings of Machine Learning Research</em>, 2018<font color="gray">  Acceptance rate: ~7%</font><br>
	    [<a href="https://arxiv.org/abs/1804.04118">pdf</a> | <a href="https://github.com/eshed1/PING">data </a> | <a href="./papers/ping.txt">bibtex</a>]
    </div>

	 <div class="publication">
	    <a href="./papers/var_ubicomp.pdf"><img src="./images/ubi.png" class="publogo" width="150" height ="110"></a>
	    <p><strong><a href="./papers/var_ubicomp.pdf">Variability in Reactions to Instructional Guidance during Smartphone-Based Assisted Navigation of Blind Users</a></strong><br>
	    <strong>E. Ohn-Bar</strong>, J. Guerreiro, K. Kitani, and C. Asakawa<br>
	    <em>International Joint Conference on Pervasive and Ubiquitous Computing (<strong>UbiComp / IMWUT</strong>)</em>, 2018<font color="gray">  Acceptance rate: ~20%</font><br>
	    [<a href="./papers/var_ubicomp.pdf">pdf</a> | <a href="./papers/variability_ubicomp.txt">bibtex</a>]
    </div>
	
	 <div class="publication">
	    <a href="https://arxiv.org/abs/1806.08479"><img src="./images/hiirl.png" class="publogo" width="150" height ="110"></a>
	    <p><strong><a href="https://arxiv.org/abs/1806.08479">Human-Interactive Subgoal Supervision for Efficient Inverse Reinforcement Learning</a></strong><br>
	    X. Pan, <strong>E. Ohn-Bar</strong>, N. Rhinehart, Y. Xu, Y. Shen, and K. M. Kitani<br>
	    <em>International Conference on Autonomous Agents and Multiagent Systems (<strong>AAMAS</strong>)</em>, 2018<font color="gray">  Acceptance rate: 25%</font><br>
	    [<a href="https://arxiv.org/abs/1806.08479">pdf</a> | <a href="./papers/hiirl.txt">bibtex</a>]
    </div>

    <div class="publication">
	    <a href="./papers/userbehavior_w4a.pdf"><img src="./images/navcog.jpg" class="publogo" width="150" height ="110"></a>
	    <p><strong><a href="./papers/userbehavior_w4a.pdf">How Context and User Behavior Affect Indoor Navigation Assistance for Blind People</a></strong><br>
	    J. Guerreiro, <strong>E. Ohn-Bar</strong>, D. Ahmetovic, K. Kitani, and C. Asakawa<br>
	    <em>Web for All Conference (<strong>W4A</strong>)</em>, 2018<br>
	    [<a href="./papers/userbehavior_w4a.pdf">pdf</a> | <a href="./papers/userbehavior_w4a.txt">bibtex</a>]
    </div>

    <div class="publication">
	    <a href="./papers/ExpertiseIUI.pdf"><img src="./images/expert.png" class="publogo" width="150" height ="110"></a>
	    <p><strong><a href="./papers/ExpertiseIUI.pdf">Modeling Expertise in Assistive Navigation Interfaces for Blind People</a></strong><br>
	    <strong>E. Ohn-Bar</strong>, J. Guerreiro, D. Ahmetovic, K. Kitani, and C. Asakawa<br>
	    <em>International Conference on Intelligent User Interfaces (<strong>IUI</strong>)</em>, 2018<font color="gray">  Acceptance rate: 23%</font><br>
	    [<a href="./papers/ExpertiseIUI.pdf">pdf</a> | <a href="./papers/ExpertiseIUI.txt">bibtex</a>]
    </div>


    <div class="publication">
	    <a href="./papers/envfact.pdf"><img src="./images/chi.png" class="publogo" width="150" height ="110"></a>
	    <p><strong><a href="./papers/envfact.pdf">Environmental Factors in Indoor Navigation Based on Real-World Trajectories of Blind Users</a></strong><br>
	    H. Kacorri, <strong>E. Ohn-Bar</strong>, K. Kitani, and C. Asakawa<br>
	    <em>Conference on Human Factors in Computing Systems (<strong>CHI</strong>)</em>, 2018<font color="gray">  Acceptance rate: 26%</font><br>
	    [<a href="./papers/envfact.pdf">pdf</a> | <a href="./papers/envfact.txt">bibtex</a>]
    </div>

    <div class="publication">
	    <a href="./papers/smartpartnet.pdf"><img src="./images/wacv.png" class="publogo" width="150" height ="110"></a>
	    <p><strong><a href="./papers/smartpartnet.pdf">SmartPartNet: Part-Informed Person Detection for Body-Worn Smartphones</a></strong><br>
	    H. Yu, <strong>E. Ohn-Bar</strong>, D. Yoo, and K. Kitani<br>
	    <em>Winter Conf. on Applications of Computer Vision (<strong>WACV</strong>)</em>, 2018<font color="gray">  Acceptance rate: 37%</font><br>
	    [<a href="./papers/smartpartnet.pdf">pdf</a> | <a href="./papers/smartpartnet.txt">bibtex</a>]
    </div>
<br>
<a name="2017">2017</a> <hr style="height:3px;border:none;color:#808080;background-color:#808080;margin-top: -12px;margin-left: 50px;top: 50%;width: 100%;content: "";display: inline-block;"/>


    <div class="publication">
	    <a href="./papers/areall.pdf"><img src="./images/imp2.png" class="publogo" width="150" height ="110"></a>
	    <p><strong><a href="./papers/areall.pdf">Are All Objects Equal? Deep Spatio-Temporal Importance Prediction in Driving Videos</a></strong><br>
	    <strong>E. Ohn-Bar</strong> and M. Trivedi<br>
	    <em>Pattern Recognition (<strong>PR</strong>)</em>, 2017<font color="gray">  Impact factor: 5.898</font><br>
	    [<a href="./papers/areall.pdf">pdf</a> | <a href="https://github.com/eshed1/Object_Importance">data and code</a> | <a href="object_importance.html">project</a> | <a href="./papers/areall_PR.txt">bibtex</a>]
    </div>

     <div class="publication">
			<a href="./papers/refineNet.pdf"><img src="./images/RefineTIV.png" class="publogo" width="150" height="110"></a>
		    <p><strong><a href="./papers/refineNet.pdf">RefineNet: Refining Object Detectors for Autonomous Driving</a></strong><br>
			R. N. Rajaram, <strong>E. Ohn-Bar</strong>, and M. Trivedi<br>
			<em>IEEE Transactions on Intelligent Vehicles (<strong>T-IV</strong>)</em>, 2017<br>
			[<a href="./papers/refineNet.pdf">pdf</a> | <a href="./papers/refineNet.txt">bibtex</a>]
    </div>
	
	 <div class="publication">
	    <a href="./papers/Dissertation_Eshed.pdf"><img src="./images/phd.png" class="publogo" width="150" height ="110"></a>
	    <p><strong><a href="./papers/Dissertation_Eshed.pdf">Contextual Visual Object Recognition and Behavior Modeling for Human-Robot Interactivity</a></strong><br>
	    <strong>E. Ohn-Bar</strong><br>
	    <em>PhD Thesis</em>, 2017<br>
	    [<a href="./papers/Dissertation_Eshed.pdf">pdf</a>]
    </div>
	
	  <div class="publication">
	    <a href="./papers/mss_PR.pdf"><img src="./images/outofbox.png" class="publogo" width="150" height ="110"></a>
	    <p><strong><a href="./papers/mss_PR.pdf">Multi-scale Volumes for Deep Object Detection and Localization</a></strong><br>
	    <strong>E. Ohn-Bar</strong> and M. Trivedi<br>
	    <em>Pattern Recognition (<strong>PR</strong>)</em>, 2017<font color="gray">  Impact factor: 5.898</font><br>
	    [<a href="./papers/mss_PR.pdf">pdf</a> | <a href="contact.txt">data </a> | <a href="./papers/mss_PR.txt">bibtex</a>]
    </div>

<a name="2016">2016</a> <hr style="height:3px;border:none;color:#808080;background-color:#808080;margin-top: -12px;margin-left: 50px;top: 50%;width: 100%;content: "";display: inline-block;"/>

    <div class="publication">
	    <a href="./papers/humansTIV.pdf"><img src="./images/humansTIV.png" class="publogo" width="150" height ="110"></a>
	    <p><strong><a href="./papers/humansTIV.pdf">Looking at Humans in the Age of Self-Driving and Highly Automated Vehicles</a></strong><br>
	    <strong>E. Ohn-Bar</strong> and M. Trivedi<br>
	    <em>IEEE Transactions on Intelligent Vehicles (<strong>T-IV</strong>)</em>, 2016<br>
	    [<a href="./papers/humansTIV.pdf">pdf</a> | <a href="./papers/humansTIV.txt">bibtex</a>]
    </div>

   <div class="publication">
     <a href="papers/Rangesh_T_ITS_2016.pdf"><img width="150" height = "110" class="publogo" src="images/handtrack_b.png"></a>
	<!--    <a href="./papers/Rangesh_T_ITS_2016.pdf"><img src="./images/handtrack.png" class="publogo" width="150" height ="50"></a>!-->
	    <p><strong><a href="./papers/Rangesh_T_ITS_2016.pdf">Long-term, Multi-Cue Tracking of Hands in Vehicles</a></strong><br>
	    A. Rangesh, <strong>E. Ohn-Bar</strong>, and M. Trivedi<br>
	    <em>IEEE Transactions on Intelligent Transportation Systems (<strong>T-ITS</strong>)</em>, 2016<font color="gray">  Impact factor: 5.74</font><br>
	    [<a href="./papers/Rangesh_T_ITS_2016.pdf">pdf</a> | <a href="http://cvrr.ucsd.edu/vivachallenge">data </a> | <a href="./papers/Rangesh_T_ITS_2016.txt">bibtex</a>]
    </div>

                <div class="publication">
		    <a href="./papers/Multires_Peds.pdf"><img width="150" height ="150" src="./images/multires.png" class="publogo"></a>
		    <p><strong><a href="./papers/Multires_Peds.pdf">Looking at Pedestrians at Different Scales: A Multiresolution Approach and Evaluations</a></strong><br>
		    R. N. Rajaram, <strong>E. Ohn-Bar</strong>, and M. Trivedi<br>
		    <em>IEEE Transactions on Intelligent Transportation Systems (<strong>T-ITS</strong>)</em>, 2016<font color="gray">  Impact factor: 5.74</font><br>
		    [<a href="./papers/Multires_Peds.pdf">pdf</a> | <a href="./papers/Multires_Peds.txt">bibtex</a>]
    </div>

        <div class="publication">
			<a href="./papers/imp_icpr.pdf"><img src="./images/imp2a.png" class="publogo" width="150" height="110"></a>
		    <p><strong><a href="./papers/imp_icpr.pdf">What Makes an On-road Object Important? <font color="red">(oral presentation)</font></a></strong><br>
			<strong>E. Ohn-Bar</strong> and M. Trivedi<br>
			<em>IEEE International Conference on Pattern Recognition (<strong>ICPR</strong>)</em>, 2016<font color="gray">  Acceptance rate: 14%</font><br>
			<b><font color="red">Best student paper award finalist</font></b><br>
			[<a href="./papers/imp_icpr.pdf">pdf</a> | <a href="https://github.com/eshed1/Object_Importance">data and code</a> | <a href="object_importance.html">project</a> | <a href="./papers/imp_icpr.txt">bibtex</a>]
    </div>


    <br>
        <div class="publication">
			<a href="./papers/boostornot.pdf"><img src="./images/boostn1.png" class="publogo" width="150" height="110"></a>
		    <p><strong><a href="./papers/boostornot.pdf">To Boost or Not to Boost? On the Limits of Boosted Trees for Object Detection <font color="red">(oral presentation)</font></a></strong><br>
			<strong>E. Ohn-Bar</strong> and M. Trivedi<br>
			<em>IEEE International Conference on Pattern Recognition (<strong>ICPR</strong>)</em>, 2016<font color="gray">  Acceptance rate: 14%</font><br>
			<b><font color="red">Best student paper award finalist</font></b><br>
			[<a href="./papers/boostornot.pdf">pdf</a> |  <a href="./code/BoostICPR.zip">Models, Results on Caltech/FDDB/WIDER and training code</a> | <a href="./papers/boostornot.txt">bibtex</a>]
    </div>

    <!--
<p style="margin:-12px 0px 0px 0px;"></p>
    <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.0%;">
<div class="media">
<div class="pull-left text-center" style="padding:11px 3.4% 0 0;">
   <a href="papers/refinenet_TIV.pdf"><img width="300" height = "220" src="images/refinenet.png"></a>
</div>
   <br>
   <h4 style="font-size:18px; line-height:120%">RefineNet: Iterative Refinement for Accurate Object Localization</h4>
   <p style="margin:-8px 0px 0px 0px;"></p>
   <p style="font-size:18px" style="padding:0px 0px 0px 0px;">R. N. Rajaram, <strong>E. Ohn-Bar</strong> and M. Trivedi</p>
   <p style="margin:-10px 0px 0px 0px;"></p><p style="font-size:18px" style="padding:0px 0px 0px 0px;">In <em>Submitted to IEEE Transactions on Intelligent Vehicles</em> (<strong>T-IV</strong>), 2016</p>
   <p style="margin:-8px 0px 0px 0px;"></p>[<a href="papers/refinenet_TIV.pdf">pdf</a>]
</div>
      </li>
    </ul>
<p style="margin:-8px 0px 0px 0px;"></p>
!-->

<br>
  <div class="publication">
			<a href="./papers/refineNetITSC.pdf"><img src="./images/refinenet.png" class="publogo" width="150" height="110"></a>
		    <p><strong><a href="./papers/refineNetITSC.pdf">RefineNet: Iterative Refinement for Accurate Object Localization</a></strong><br>
			R. N. Rajaram, <strong>E. Ohn-Bar</strong>, and M. Trivedi<br>
			<em>IEEE Intelligent Transportation Systems Conference (<strong>ITSC</strong>)</em>, 2016<br>
			[<a href="./papers/refineNetITSC.pdf">pdf</a> | <a href="./papers/refineNetITSC.txt">bibtex</a>]
    </div>
  <div class="publication">
			<a href="./papers/surrRNN.pdf"><img src="./images/sur3.png" class="publogo" width="150" height="110"></a>
		    <p><strong><a href="./papers/surrRNN.pdf">Surround Vehicles Trajectory Analysis with Recurrent Neural Networks</a></strong><br>
			A. Khosroshahi, <strong>E. Ohn-Bar</strong>, and M. Trivedi<br>
			<em>IEEE Intelligent Transportation Systems Conference (<strong>ITSC</strong>)</em>, 2016<br>
			[<a href="./papers/surrRNN.pdf">pdf</a> | <a href="./papers/surrRNN.txt">bibtex</a>]
    </div>
<br>
      <div class="publication">
				<a href="./papers/pedsphones.pdf"><img src="./images/pedsactivity.png" class="publogo" width="150" height="110"></a>
			    <p><strong><a href="./papers/pedsphones.pdf">Pedestrians and their Phones - Detecting Phone-based Activities of Pedestrians for Autonomous Vehicles</a></strong><br>
				A. Rangesh, <strong>E. Ohn-Bar</strong>, K. Yuen, and M. Trivedi<br>
				<em>IEEE Intelligent Transportation Systems Conference (<strong>ITSC</strong>)</em>, 2016<br>
				[<a href="./papers/pedsphones.pdf">pdf</a> | <a href="./papers/pedsphones.txt">bibtex</a>]
	    </div>

      <div class="publication">
				<a href="./papers/0575.pdf"><img src="./images/pano.png" class="publogo" width="150" height="110"></a>
			    <p><strong><a href="./papers/0575.pdf">Multi-Perspective Vehicle Detection and Tracking: Challenges, Dataset, and Metrics</a></strong><br>
				J. V. Dueholm, M. S. Kristoffersen, R. Satzoda, <strong>E. Ohn-Bar</strong>, T. B. Moeslund, and M. Trivedi<br>
				<em>IEEE Intelligent Transportation Systems Conference (<strong>ITSC</strong>)</em>, 2016<br>
				[<a href="./papers/0575.pdf">pdf</a> | <a href="./papers/pano.txt">bibtex</a>]
	    </div>

          <div class="publication">
			<a href="./papers/mss_icpr.pdf"><img src="./images/mss.png" class="publogo" width="150" height="110"></a>
		    <p><strong><a href="./papers/mss_icpr.pdf">Detection and Localization with Multi-scale Models <font color="red">(oral presentation)</font></a></strong><br>
			<strong>E. Ohn-Bar</strong> and M. Trivedi<br>
			<em>IEEE International Conference on Pattern Recognition (<strong>ICPR</strong>)</em>, 2016<font color="gray">  Acceptance rate: 14%</font><br>
			[<a href="./papers/mss_icpr.pdf">pdf</a> | <a href="./papers/mss_icpr.txt">bibtex</a>]
    </div>

<a name="2015">2015</a> <hr style="height:3px;border:none;color:#808080;background-color:#808080;margin-top: -12px;margin-left: 50px;top: 50%;width: 100%;content: "";display: inline-block;"/>

<!--
    <div class="publication">
	    <a href="./papers/outofbox.pdf"><img src="./images/outofbox.png" class="publogo" width="150" height ="110"></a>
	    <p><strong><a href="./papers/outofbox.pdf">Looking outside of the Box: Object Detection and Localization with Multi-scale Patterns</a></strong><br>
	    <strong>E. Ohn-Bar</strong> and M. Trivedi<br>
	    <em>arXiv</em>, 2015<br>
	    [<a href="./papers/outofbox.pdf">pdf</a> | <a href="./papers/outofbox.txt">bibtex</a>]
    </div>
    -->
	
		<div class="publication">
	    <a href="./papers/CVIUpredict.pdf"><img src="./images/cviu.jpg" class="publogo" width="150"></a>
	    <p><strong><a href="./papers/CVIUpredict.pdf">On Surveillance for Safety Critical Events: In-Vehicle Video Networks for Predictive Driver Assistance Systems</a></strong><br>
	    <strong>E. Ohn-Bar</strong>, A. Tawari, S. Martin, and M. Trivedi<br>
	    <em>Computer Vision and Image Understanding (<strong>CVIU</strong>)</em>, 2015<font color="gray">  Impact factor: 2.77</font><br>
	    [<a href="./papers/CVIUpredict.pdf">pdf</a> | <a href="./papers/cviu.txt">bibtex</a>]
    </div>

	<div class="publication">
	    <a href="./papers/OhnBar_TITS15_wsupplement.pdf"><img height="350" width="150" src="./images/subcat2_long.png" class="publogo"></a>
	    <p><strong><a href="./papers/OhnBar_TITS15_wsupplement.pdf">Learning to Detect Vehicles by Clustering Appearance Patterns</a></strong><br>
	    <strong>E. Ohn-Bar</strong> and M. Trivedi<br>
	    <em>IEEE Transactions on Intelligent Transportation Systems (<strong>T-ITS</strong>)</em>, 2015<font color="gray">  Impact factor: 5.74</font><br>
	    [<a href="./papers/OhnBar_TITS15_wsupplement.pdf">pdf (with supplementary)</a> | <a href="subcat/index.html"><font color="red">code</font></a> | <a href="./papers/OhnBar_TITS15_wsupplement.txt">bibtex</a>]

    </div>

	<div class="publication">
	    <a href="./papers/Das_ITSC2015.pdf"><img src="./images/viva.png" class="publogo" width="150" height = "105"></a>
	    <p><strong><a href="./papers/Das_ITSC2015.pdf">On Performance Evaluation of Driver Hand Detection Algorithms: Challenges, Dataset, and Metrics</a></strong><br>
	    N. Das, <strong>E. Ohn-Bar</strong>, and M. Trivedi<br>
	    <em>IEEE Conference on Intelligent Transportation Systems (<strong>ITSC</strong>)</em>, 2015<br>
	    [<a href="./papers/Das_ITSC2015.pdf">pdf</a> | <a href="http://cvrr.ucsd.edu/vivachallenge/index.php/hands/hand-detection/">data</a> | <a href="./papers/Das_ITSC15.txt">bibtex</a>]
    </div>

	<div class="publication">
	    <a href="./papers/Nattoji_ITSC2015.pdf"><img src="./images/peds_notxt.png" class="publogo" width="150" height = "100"></a>
	    <p><strong><a href="./papers/Nattoji_ITSC2015.pdf">An Exploration of Why and When Pedestrian Detection Fails</a></strong><br>
	    R. N. Rajaram, <strong>E. Ohn-Bar</strong>, and M. Trivedi<br>
	    <em>IEEE Conference on Intelligent Transportation Systems (<strong>ITSC</strong>)</em>, 2015<br>
	    [<a href="./papers/Nattoji_ITSC2015.pdf">pdf</a> | <a href="./papers/Nattoji_ITSC15.txt">bibtex</a>]
    </div>
<br>
	<div class="publication">
	    <a href="./papers/OhnBar_IV15_hands.pdf"><img src="./images/handscomp_long.png" class="publogo" width="150" height = "300"></a>
	    <p><strong><a href="./papers/OhnBar_IV15_hands.pdf">A Comparative Study of Color and Depth Features for Hand Gesture Recognition in Naturalistic Driving Settings</a></strong><br>
	    <strong>E. Ohn-Bar</strong> and M. Trivedi<br>
	    <em>IEEE Intelligent Vehicles Symposium (<strong>IV</strong>)</em>, 2015<br>
	    [<a href="./papers/OhnBar_IV15_hands.pdf">pdf</a> | <a href="./papers/OhnBar_IV15_hands.txt">bibtex</a>]
    </div>
    <br>

	<div class="publication">
	    <a href="./papers/OhnBar_IV15_peds.pdf"><img src="./images/subcatpeds.png" class="publogo" width="150" height = "90"></a>
	    <p><strong><a href="./papers/OhnBar_IV15_peds.pdf">Can Appearance Patterns Improve Pedestrian Detection?</a></strong><br>
	    <strong>E. Ohn-Bar</strong> and M. Trivedi<br>
	    <em>IEEE Intelligent Vehicles Symposium (<strong>IV</strong>)</em>, 2015<br>
	    [<a href="./papers/OhnBar_IV15_peds.pdf">pdf</a> | <a href="./papers/OhnBar_IV15_peds.txt">bibtex</a>]
    </div>

<a name="2014">2014</a> <hr style="height:3px;border:none;color:#808080;background-color:#808080;margin-top: -12px;margin-left: 50px;top: 50%;width: 100%;content: "";display: inline-block;"/>

	<div class="publication">
		    <a href="./papers/OhnBar_IEEETITS2014.pdf"><img src="./images/itshands_l.png" class="publogo" width="150"></a>
		    <p><strong><a href="./papers/OhnBar_IEEETITS2014.pdf">Hand Gesture Recognition in Real-Time for Automotive Interfaces: A Multimodal Vision-based Approach and Evaluations</a></strong><br>
		    <strong>E. Ohn-Bar</strong> and M. Trivedi<br>
		    <em>IEEE Transactions on Intelligent Transportation Systems (<strong>T-ITS</strong>)</em>, 2014<font color="gray">  Impact factor: 5.74</font><br>
		    [<a href="./papers/OhnBar_IEEETITS2014.pdf">pdf</a> | <a href="./papers/OhnBar_IEEETITS2014.txt">bibtex</a>]
    </div>
<br>
    <div class="publication">
			<a href="./papers/beyondHands.pdf"><img src="./images/beyond.png" class="publogo" width="150" height="110"></a>
		    <p><strong><a href="./papers/beyondHands.pdf">Beyond Just Keeping Hands on the Wheel: Towards Visual Interpretation of Driver Hand Motion Patterns</a></strong><br>
			<strong>E. Ohn-Bar</strong> and M. Trivedi<br>
			<em>IEEE Conference on Intelligent Transportation Systems (<strong>ITSC</strong>)</em>, 2014<br>
			[<a href="./papers/beyondHands.pdf">pdf</a> | <a href="./papers/hands_ITSC14.txt">bibtex</a>]
    </div>
<br>
	<div class="publication">
			<a href="./papers/vow.pdf"><img src="./images/vow_1l.png" class="publogo" width="150" height = "110"></a>
		    <p><strong><a href="./papers/vow.pdf">Vision on Wheels: Looking at Driver, Vehicle, and Surround for On-Road Maneuver Analysis <font color="red">(oral presentation)</font></a></strong><br>
			<strong>E. Ohn-Bar</strong>, A. Tawari, S. Martin, and M. Trivedi<br>
			<em>Mobile Vision Workshop, IEEE Conf. Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2014<br>
			[<a href="./papers/vow.pdf">pdf</a> | <a href="./papers/vow.txt">bibtex</a>]
    </div>

    <div class="publication">
			<a href="./papers/headhandeye.pdf"><img src="./images/conceptHeadHandl.png" class="publogo" width="150" height="110"></a>
		    <p><strong><a href="./papers/headhandeye.pdf">Head, Eye, and Hand Patterns for Driver Activity Recognition <font color="red">(oral presentation)</font></a></strong><br>
			<strong>E. Ohn-Bar</strong>, S. Martin, A. Tawari, and M. Trivedi<br>
			<em>IEEE International Conference on Pattern Recognition (<strong>ICPR</strong>)</em>, 2014<font color="gray">  Acceptance rate: 14%</font><br>
			<b><font color="red">Best Industry Related Paper Award Runner Up</font></b><br>
			[<a href="./papers/headhandeye.pdf">pdf</a> | <a href="./papers/headhandeye.txt">bibtex</a>]
    </div>
<br>
    <div class="publication">
			<a href="./papers/subcat.pdf"><img src="./images/o3l.png" class="publogo" width="150" height="110"></a>
		    <p><strong><a href="./papers/subcat.pdf">Fast and Robust Object Detection Using Visual Subcategories <font color="red">(oral presentation)</font></a></strong><br>
			<strong>E. Ohn-Bar</strong> and M. Trivedi<br>
			<em>Mobile Vision Workshop, IEEE Conf. Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2014<br>
			[<a href="./papers/subcat.pdf">pdf</a> | <a href="subcat/index.html"><font color="red">code</font></a> | <a href="./papers/subcat.txt">bibtex</a>]
    </div>

    <div class="publication">
			<a href="./papers/holistic.pdf"><img src="./images/holistic.png" class="publogo" width="150" height="110"></a>
		    <p><strong><a href="./papers/holistic.pdf">Predicting Driver Maneuvers by Learning Holistic Features</a></strong><br>
			<strong>E. Ohn-Bar</strong>, A. Tawari, S. Martin, and M. Trivedi<br>
			<em>IEEE Intelligent Vehicles Symposium (<strong>IV</strong>)</em>, 2014<br>
			[<a href="./papers/holistic.pdf">pdf</a> | <a href="./papers/holistic.txt">bibtex</a>]
    </div>

    <div class="publication">
			<a href="./papers/goflow.pdf"><img src="./images/o4_1.png" class="publogo" width="150" height="110"></a>
		    <p><strong><a href="./papers/goflow.pdf">Go with the Flow: Improving Multi-View Vehicle Detection with Motion Cues</a></strong><br>
			A. Ramirez, <strong>E. Ohn-Bar</strong>, and M. Trivedi<br>
			<em>IEEE International Conference on Pattern Recognition (<strong>ICPR</strong>)</em>, 2014<br>
			[<a href="./papers/goflow.pdf">pdf</a> | <a href="./papers/goflow.txt">bibtex</a>]
    </div>


<!--	<li><p align="left"><strong>Integrating Motion and Appearance for Overtaking Vehicle Detection</strong><br/>
	A. Ramirez, <strong><strong>E. Ohn-Bar</strong></strong>, and M. Trivedi, <em>IEEE Intelligent Vehicles Symposium</em>, 2014. [<a href="./papers/vehiclemotion.pdf">pdf</a> | <a href="./papers/vehiclemotion.txt">bibtex</a>]</p></li>
-->





<a name="2013">2013</a> <hr style="height:3px;border:none;color:#808080;background-color:#808080;margin-top: -12px;margin-left: 50px;top: 50%;width: 100%;content: "";display: inline-block;"/>
<!--
<li><p align="left"><strong>Driver Hand Activity Analysis in Naturalistic Driving Studies: Issues, Algorithms and Experimental Studies</strong><br/>
<strong><strong>E. Ohn-Bar</strong></strong>, S. Martin, and M. Trivedi, <em>Journal of Electronic Imaging: special section on Video Surveillance and Transportation Imaging Applications</em>, vol. 22, No. 4, 2013.[<a href="./papers/hand_JEI13.pdf">pdf</a> | <a href="./papers/handJEI13.txt">bibtex</a> ]   </p></li>
-->
    <div class="publication">
			<a href="./papers/hand_JEI13.pdf"><img src="./images/jei.png" class="publogo" width="150" height = "120"></a>
		    <p><strong><a href="./papers/hand_JEI13.pdf">Driver Hand Activity Analysis in Naturalistic Driving Studies: Issues, Algorithms and Experimental Studies</a></strong><br>
			<strong>E. Ohn-Bar</strong>, S. Martin, and M. Trivedi<br>
			<em>Journal of Electronic Imaging: Special Section on Video Surveillance and Transportation Imaging Applications (<strong>JEI</strong>)</em>, 2013<br>
			[<a href="./papers/hand_JEI13.pdf">pdf</a> | <a href="./papers/handJEI13.txt">bibtex</a>]
    </div>

    <div class="publication">
			<a href="./papers/OhnBarHAU3D13.pdf"><img src="./images/jas2l.png" class="publogo" width="150" height = "50"></a>
		    <p><strong><a href="./papers/OhnBarHAU3D13.pdf">Joint Angles Similarities and HOG2 for Action Recognition <font color="red">(oral presentation)</font></a></strong><br>
			<strong>E. Ohn-Bar</strong> and M. Trivedi<br>
			<em>Human Activity Understanding from 3D Data Workshop, IEEE Conf. Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2013<br>
			[<a href="./papers/OhnBarHAU3D13.pdf">pdf</a> | <a href="./code/HOG2code.zip"><font color="red">code</font></a> |  <a href="./papers/HAU3D_slides.pdf">slides</a> | <a href="./papers/OhnBarHAU3D13.txt">bibtex</a>]
    </div>

    <div class="publication">
			<a href="./papers/OhnBarAMFG13.pdf"><img src="./images/kinect2.png" class="publogo" width="150" height = "120"></a>
		    <p><strong><a href="./papers/OhnBarAMFG13.pdf">The Power is in Your Hands: 3D Analysis of Hand Gestures in Naturalistic Video <font color="red">(oral presentation, <b><font color="red">Best Paper Award</font></b>)</font></a></strong><br>
			<strong>E. Ohn-Bar</strong> and M. Trivedi<br>
			<em>Analysis and Modeling of Faces and Gestures Workshop, IEEE Conf. Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2013<font color="gray">  Acceptance rate: 30%</font><br>
			[<a href="./papers/OhnBarAMFG13.pdf">pdf</a> | <a href="http://cvrr.ucsd.edu/LISA/hand.html">dataset </font></a> |  <a href="./papers/OhnBarAMFG13.pdf">slides</a> | <a href="./papers/OhnBarAMFG13.txt">bibtex</a>]
    </div>
<br>
    <div class="publication">
			<a href="./papers/OhnbarSivaramanTrivediIV13.pdf"><img src="./images/overt.png" class="publogo" width="150" height="110"></a>
		    <p><strong><a href="./papers/OhnbarSivaramanTrivediIV13.pdf">Partially Occluded Vehicle Recognition and Tracking in 3D</a></strong><br>
			<strong>E. Ohn-Bar</strong>, S. Sivaraman, and M. Trivedi<br>
			<em>IEEE Intelligent Vehicles Symposium (<strong>IV</strong>)</em>, 2013<br>
			[<a href="./papers/OhnbarSivaramanTrivediIV13.pdf">pdf</a> | <a href="papers/iv2013_overt_poster.pdf">poster</a> | <a href="./papers/OhnBar_IV13_overt.txt">bibtex</a>]
    </div>

<li><p align="left"><strong>In-Vehicle Hand Activity Recognition Using Integration of Regions <font color="red">(oral presentation)</font></strong><br/>
  <strong><strong>E. Ohn-Bar</strong></strong> and M. Trivedi, <em>IEEE Intelligent Vehicles Symposium (<strong>IV</strong>)</em>, 2013. [<a href="./papers/ohnbar_IV13.pdf">pdf</a> | <a href="./papers/ohnbar_IV13.txt">bibtex</a>]</p></li>

<li><p align="left"><strong>Hand Gesture-based Visual User Interface for Infotainment <font color="red">(oral presentation)</font></strong><br/>
<strong><strong>E. Ohn-Bar</strong></strong>, C. Tran, M. Trivedi, <em>Automotive User Interfaces and Interactive Vehicular Applications (<strong>AUTO-UI</strong>)</em>, 2012. [<a href="./papers/OhnBar_AutoUI12.pdf">pdf</a> | <a href="./papers/OhnBar_AutoUI12.txt">bibtex</a>]</p></li>


</div>



<div class="container">
    <a name="ProfessionalActivity"></a>
    <h2>Professional Activity</h2>
	Area Chair
	<li>CVPR 2024, 2025</li>
	<li>CHI 2025</li>	
	<li>ICRA 2017, 2025
	<li>IEEE Intelligent Vehicles Symposium (2017-2024)
	<br>
	<br>
	Program Committee/Workshop Organization
		<li>Workshop on Accessibility, Vision, and Autonomy at CVPR 2022, 2023, 2024
	<li>Workshop on Human Behavior Understanding at ICCV 2019, FG 2018
    <li>Workshop on Observing and Understanding Hands at CVPR, ICCV, ECCV (2015-2019)
    <li>Workshop on Automatic Traffic Surveillance at CVPR
	<br>
	<br>
	Reviewer
	<li>Computer Vision: CVPR, SIGGRAPH, PAMI, CVIU, CVPRW-ATS, CVPRW-HANDS, IMAVIS, T-SMC, T-CSVT, JEI 
	<li>Accessibility: CHI, ASSETS, T-ACCESS
	<li>Intelligent Vehicles: IV, ITSC, T-ITS, T-VT
	<li>Robotics and Systems: HRI, IROS, T-HMS, T-IE, T-II
	<br>
</div>

<div class="container">

</div>
</body>


<!--My research agenda involves two main components: 1) Development of new technologies for autonomous robots and driving, capable of operating in real-world environments, and 2) Human-centric robots, including human observing (e.g. driver or pedestrian behavior studies) and human-like (e.g. contextual, attentive, multi-modal). Addressing these objectives requires a multi-disciplinary effort for establishing novel benchmarks and metrics, as well as designing novel learning algorithms for semantic video understanding, situational awareness, human state modeling, information fusion, predictive systems, and decision making. Specifically, I wish to design machine learning algorithm which can robustly predict what is going to happen in a scene in the near future given multiple cameras and sensors.
-->

</html>



