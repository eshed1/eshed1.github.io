
<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml"><head>
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
  <meta http-equiv="Content-Style-Type" content="text/css">
  <meta name="generator" content="pandoc">
  <title>Sanja Fidler</title>
  <script src="courses/jquery-1.js"></script>
  <link rel="stylesheet" href="bootstrap.css">
  <link rel="stylesheet" href="bootstrap-theme.css">
  <script src="courses/bootstrap.js"></script>
  <style>
    /* http://stackoverflow.com/questions/18325779/bootstrap-3-collapse-show-state-with-chevron-icon */
    .panel-heading .accordion-toggle:before {
      font-family: 'Glyphicons Halflings';
      content: "\e114";
      float: left;
      color: grey;
      padding-right: 6px;
    }
    .panel-heading .accordion-toggle.collapsed:before {
      content: "\e080";
    }
table, th, td {
    border: 0px solid black;
    border-collapse: collapse;
}
img:hover{
  transition-duration: 0.0s;
  transition-timing-function: linear;
opacity:0.5;
}
  </style>

<style type="text/css">
</style>

</head>

<body>

<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-11803764-2', 'uchicago.edu');
  ga('require', 'displayfeatures');
  ga('send', 'pageview');

</script>

  <nav class="navbar navbar-default navbar-static-top" role="navigation">
    <div class="container">
      <ul class="nav navbar-nav">
        <li><a href="index.html"><font style="font-size:20px; font-weight:500;" color="#003380">Sanja Fidler</font></a></li>
        <li><a href="students.html"><font size="4px">Students</font></a></li>
        <li><a href="research.html"><font size="4px" color="#E62E00"><b>Research</b></font></a></li>
        <li><a href="publications.html"><font size="4px">Publications</font></a></li>
        <li><a href="teaching.html"><font size="4px">Teaching</font></a></li>
        <li><a href="data.html"><font size="4px">Data</font></a></li>
        <li><a href="code.html"><font size="4px">Code</font></a></li>
      </ul>
    </div>
  </nav>

  <div class="container">

    <a name="Research"></a>
        <p style="margin:-15px 0px 0px 0px;"></p>
   <div class="page-header"><h2 id="research">Research</h2></div>


<div class="row">
<div class="pull-left" style="width:1.4%;">
&nbsp
</div>

      <div class="pull-left" style="width:29%;">
        <div class="box" style="width:100%;">
<div align="center">
          <a data-toggle="collapse" data-parent="#detection" href="#detection-info"><img width="100%" src="papers/figs/dets_med1_small.png"></a>
</br>
</div>
</div>
<p style="margin:5px 0px 0px 0px;"></p>
<font style="font-size:18px; font-weight:500;">Object Detection</font><a data-toggle="collapse" data-parent="#detection" href="#detection-info"><img style="height:26px; margin-top:0px;" align="right" src="images/info.png"></a><br/>
</div>

<div class="pull-left" style="width:4%;">
&nbsp
</div>

      <div class="pull-left" style="width:29%;">
        <div class="box" style="width:100%;">
<div align="center">
<a data-toggle="collapse" data-parent="#moviebook" href="#moviebook-info">
          <img width="95.8%" style="margin-top:0.5px; margin-bottom:0.5px;" src="images/mb1.jpg">
          </a>
</br>
</div>
</div>
<p style="margin:5px 0px 0px 0px;"></p>
<font style="font-size:18px; font-weight:500;">Movies and Books</font><a data-toggle="collapse" data-parent="#moviebook" href="#moviebook-info"><img style="height:26px; margin-top:0px;" align="right" src="images/info.png"></a><br/>
</div>

<div class="pull-left" style="width:4%;">
&nbsp
</div>

      <div class="pull-left" style="width:29%;">
        <div class="box" style="width:100%;">
<div align="center">
         <a data-toggle="collapse" data-parent="#scene" href="#scene-info"><img width="100%" style="margin-top:5px; margin-bottom:5px;" src="papers/figs/cvpr15aptlayout_med.png"></a>
</br>
</div>
</div>
<p style="margin:5px 0px 0px 0px;"></p>
<font style="font-size:18px; font-weight:500;">Scene Understanding</font><a data-toggle="collapse" data-parent="#scene" href="#scene-info"><img style="height:26px; margin-top:0px;" align="right" src="images/info.png"></a><br/>
</div>

<br/>
</div>
<p style="margin:30px 0px 0px 0px;"></p>

<div class="row">

<div class="pull-left" style="width:1.5%;">
&nbsp
</div>

      <div class="pull-left" style="width:29%;">
       <div class="box" style="width:100%;">
<div align="center">
          <a data-toggle="collapse" data-parent="#imagetext" href="#imagetext-info"><img width="100%" src="papers/figs/text1_small.png"></a>
</br>
</div>
</div>
<p style="margin:5px 0px 0px 0px;"></p>
<font style="font-size:18px; font-weight:500;">Vision and Language</font><a data-toggle="collapse" data-parent="#imagetext" href="#imagetext-info"><img style="height:26px; margin-top:0px;" align="right" src="images/info.png"></a><br/>
</div>

<!--
      <div class="col-md-4" style="width:33%;">
        <span href="#" class="thumbnail text-center">
<div container style="font-size:20px; color:#000000; background-color:#FFAC75; border-radius:2px; border-style:solid; border-color:#666666; border-width:1.5px;  margin:-5px -5px -0px -5px; padding:6px 0% 0px 0%; height:37px;">
<p style="margin-top:-2px;"><font style="font-size:18px;">Scene Understanding (RGBD)</font></p>
</div>
</br>
<p style="margin:-13px 0px 0px 0px;"></p>
<div align="center">
          <img width="100%" src="papers/figs/rgbd_det_small.png">
</br>
<p style="margin:-2px 0px 0px 0px;"></p>
</div>
</span>
</div>
-->

<div class="pull-left" style="width:4%;">
&nbsp
</div>

      <div class="pull-left" style="width:29%;">
     <div class="box" style="width:100%;">
<div align="center">
          <a data-toggle="collapse" data-parent="#fashion" href="#fashion-info"><img width="100%" style="margin-top:1px; margin-bottom:1px;" src="papers/figs/clothing.png"></a>
</br>
</div>
</div>
<p style="margin:5px 0px 0px 0px;"></p>
<font style="font-size:18px; font-weight:500;">Clothing and Fashion</font><a data-toggle="collapse" data-parent="#fashion" href="#fashion-info"><img style="height:26px; margin-top:0px;" align="right" src="images/info.png"></a><br/>
</div>


<div class="pull-left" style="width:4%;">
&nbsp
</div>

      <div class="pull-left" style="width:29%;">
     <div class="box" style="width:100%;">
<div align="center">
          <a data-toggle="collapse" data-parent="#segmentation" href="#segmentation-info"><img width="100%" style="margin-top:1px; margin-bottom:1px;"  src="papers/figs/segm3_small.png"></a>
</br>
</div>
</div>
<p style="margin:5px 0px 0px 0px;"></p>
<font style="font-size:18px; font-weight:500;">Segmentation</font><a data-toggle="collapse" data-parent="#segmentation" href="#segmentation-info"><img style="height:26px; margin-top:0px;" align="right" src="images/info.png"></a><br/>
</div>


<br/>
</div>

<br/>
<br/>

<div id="detection-info" class="panel-collapse collapse out" style="padding:0% 3% 1% 3%; border-radius:6px; border-style: solid; border-color: #666666;">

<p style="margin:10px 0px 0px 0px;"></p>
<font style="font-size:1.34em; font-weight:500;">Object Detection</font>
      <div class="media">
      <p align="justify">Object class recognition and detection is one of the main challenges in computer vision and also one of my greatest passions. In my PhD work I proposed
      multi-class hierarchical compositional representations of objects. The main advantage of the representation at the time was that detection time grew sub-linearly with the number of classes, the representation was learned from data, and was capable of modeling contour deformation at multiple levels. </p>
      
      <p align="justify">Combining different sources of information such as appearance and segmentation is also important. For example, articulated objects may more accurately be recognized at a smaller scale (texture patches) which is well captured by local image-labeling approaches, while shape is a more global cue and thus better modeled by a representation of a more global, object-size, region/window. In recent work we showed that combining appearance, segmentation and contextual information improved 5-8% over DPM and R-CNN. As part of this work, we also labeled PASCAL VOC with dense pixel labels of over 400 classes (<a href="http://www.cs.stanford.edu/~roozbeh/pascal-context/">PASCAL-Context</a> dataset), as well as detailed masks of semantic object parts (<a href="http://www.stat.ucla.edu/~xianjie.chen/pascal_part_dataset/pascal_part.html">PASCAL-Part</a> Dataset). </p>
      
      <p>We live in a 3D world, and thus our models should also reason in 3D. This is particularly important for robotics applications where estimating accurate 3D location and pose is crucial. Our work focuses on 3D object detection from single monocular images as well as RGB-D data.</p>
  
  <p style="margin:30px 0px 0px 0px;"></p>
      
      <h3> Relevant Publications</h3>

<br/>
<p style="margin:-10px 0px 0px 0px;"></p>

<font style="font-size:1.11em; font-weight:500;">Multi-cue Detection</font>
<br/>
<br/>
<p style="margin:-12px 0px 0px 0px;"></p>
    <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.0%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:11px 3.4% 0 0; width:14.2%;" href="http://arxiv.org/abs/1502.04275"><img width="100%" src="papers/figs/segdeepm.jpg"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
            <h4 style="font-size:14.5px; line-height:120%">segDeepM: Exploiting Segmentation and Context in Deep Neural Networks for Object Detection</h4>

<p style="margin:-8px 0px 0px 0px;"></p>
            <p style="font-size:14.0px" style="padding:0px 0px 0px 0px;">Yukun Zhu, Raquel Urtasun, Ruslan Salakhutdinov, <strong>Sanja Fidler</strong></p>

		<p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px" style="padding:0px 0px 0px 0px;">In <em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>), Boston, 2015</p>
<p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px; color:#E62E00;" style="padding:0px 0px 0px 0px;"> Currently third in detection on PASCAL VOC <a href="http://host.robots.ox.ac.uk:8080/leaderboard/displaylb.php?challengeid=6&compid=4">Leaderboard</a></p>
   
   <p style="margin:-8px 0px 0px 0px;"></p>         
            <a href="http://arxiv.org/abs/1502.04275" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#ZhuSegDeepM15abs" href="#ZhuSegDeepM15abs-list">Abstract</a>&nbsp
<a href="http://www.cs.toronto.edu/~yukun/segdeepm.html" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#ZhuSegDeepM15" href="#ZhuSegDeepM15-list">Bibtex</a><br/>
 
<p style="margin:2px 0px 0px 0px;"></p>
<div id="ZhuSegDeepM15-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{ZhuSegDeepM15,<br/>
    title = {segDeepM: Exploiting Segmentation and Context in Deep Neural Networks for Object Detection},<br/>
    author = {Yukun Zhu and Raquel Urtasun and Ruslan Salakhutdinov and Sanja Fidler},<br/>
    booktitle = {CVPR},<br/>
    year = {2015}<br/>
}
</p>      
</div>
<div id="ZhuSegDeepM15abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.5px;">
In this paper, we propose an approach that exploits object segmentation in order to improve the accuracy of object detection. 
We frame the problem as inference in a Markov Random Field, in which each detection hypothesis scores object appearance as well as 
contextual information using Convolutional Neural Networks, and allows the hypothesis to choose and score a segment out of a large 
pool of accurate object segmentation proposals. This enables the detector to incorporate additional evidence when it is available 
and thus results in more accurate detections. Our experiments show an improvement of 4.1% in mAP over the R-CNN baseline on PASCAL 
VOC 2010, and 3.4% over the current state-of-the-art, demonstrating the power of our approach.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
<!--
<div class="pull-right text-center" style="padding:0.5% 1% 0% 0%; width:11%;">
<a href="http://arxiv.org/abs/1502.04275" class="buttonT" style="width:100%;">Paper</a><br/>
<p style="margin:0px 0px 0px 0px;"></p>
<a class="buttonA" data-toggle="collapse" data-parent="#ZhuSegDeepM15abs" href="#ZhuSegDeepM15abs-list" style="width:100%; margin-top:6px; text-decoration: none; color:#00CFCF;">Abstract</a><br/>
<p style="margin:1.5px 0px 0px 0px;"></p>
<a href="http://www.cs.toronto.edu/~yukun/segdeepm.html" class="buttonP" style="width:100%; padding:0 0 0px 0;">Project page</a>
<p style="margin:0px 0px 0px 0px;"></p>
<a class="buttonS" data-toggle="collapse" data-parent="#ZhuSegDeepM15" href="#ZhuSegDeepM15-list" style="width:100%; margin-top:6px; text-decoration: none; color:#FF9933;">Bibtex</a><br/>
        </div>
        -->
      </li>
    </ul>
    

<p style="margin:-8px 0px 0px 0px;"></p>



    <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.0%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:10px 2.6% 0 0; width:14.2%;" href="papers/Mottaghi14cvpr.pdf"><img width="100%" src="papers/figs/cvpr14contexta.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
            <h4 style="font-size:14.5px; line-height:120%">The Role of Context for Object Detection and Semantic Segmentation in the Wild</h4>

	     <p style="margin:-8px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">Roozbeh Mottaghi,  Xianjie Chen, Xiaobai Liu, <strong>Sanja Fidler</strong>, Raquel Urtasun, Alan Yuille</p>

		<p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">In <em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>), Columbus, USA, June, 2014</p>
<p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px; color:#E62E00;"> PASCAL VOC with dense segmentation labels for 400+ classes in Project page</p>
<p style="margin:-8px 0px 0px 0px;"></p>
<a href="papers/Mottaghi14cvpr.pdf" class="buttonTT">Paper</a>&nbsp
<a href="papers/errataMottaghi2014.pdf" class="buttonTT">Errata</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#MottaghiCVPR14abs" href="#MottaghiCVPR14abs-list">Abstract</a>&nbsp
<a href="http://www.cs.stanford.edu/~roozbeh/pascal-context/" class="buttonPP">Project page</a>&nbsp
<a href="papers/cvpr14_context_supplemental.pdf" class="buttonMM">Suppl. Mat.</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#MottaghiCVPR14" href="#MottaghiCVPR14-list">Bibtex</a><br/>
 
<p style="margin:2px 0px 0px 0px;"></p>
<div id="MottaghiCVPR14-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{MottaghiCVPR14,<br/>
    author = {Roozbeh Mottaghi and  Xianjie Chen and Xiaobai Liu and Sanja Fidler and Raquel Urtasun and Alan Yuille},<br/>
    title = {The Role of Context for Object Detection and Semantic Segmentation in the Wild},<br/>
    booktitle = {CVPR},<br/>
    year = {2014}<br/>}</p>
</div>
<div id="MottaghiCVPR14abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.5px;">
In this paper we study the role of context in existing stateof-the-art
detection and segmentation approaches. Towards
this goal, we label every pixel of PASCAL VOC 2010 detection
challenge with a semantic category. We believe this
data will provide plenty of challenges to the community, as
it contains 520 additional classes for semantic segmentation
and object detection. Our analysis shows that nearest
neighbor based approaches perform poorly on semantic
segmentation of contextual classes, showing the variability
of PASCAL imagery. Furthermore, improvements of existing
contextual models for detection is rather modest. In
order to push forward the performance in this difficult scenario,
we propose a novel deformable part-based model,
which exploits both local context around each candidate detection
as well as global context at the level of the scene.
We show that this contextual reasoning significantly helps
in detecting objects at all scales.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
<!--
<div class="pull-right text-center" style="padding:0.1% 1% 0% 0%; width:11%;">
<a href="papers/Mottaghi14cvpr.pdf" class="buttonT" style="width:100%; height:15.0px; font-size:11.0px;">Paper</a><br/>
<p style="margin:-1.9px 0px 0px 0px;"></p>
<a href="papers/errataMottaghi2014.pdf" class="buttonT" style="width:100%; height:15.0px; font-size:11px;">Errata</a><br/>
<p style="margin:-1.9px 0px 0px 0px;"></p>
<a href="http://www.cs.stanford.edu/~roozbeh/pascal-context/" class="buttonP" style="width:100%; height:15.0px; font-size:11.0px;">Project page</a><br/>
<p style="margin:-1.4px 0px 0px 0px;"></p>
<a href="papers/cvpr14_context_supplemental.pdf" class="buttonM" style="width:100%; height:15.0px; font-size:11.0px;">Suppl. Mat.</a><br/>
<p style="margin:-2.7px 0px 0px 0px;"></p>
<a class="buttonS" data-toggle="collapse" data-parent="#MottaghiCVPR14" href="#MottaghiCVPR14-list" style="width:100%; height:15.0px; font-size:11px; margin-top:4px; text-decoration: none; color:#FF9933;">Bibtex</a><br/>
        </div>
        -->
      </li>
    </ul>

<p style="margin:-8px 0px 0px 0px;"></p>


    <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.0%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:12px 3% 0 0; width:14.2%;" href="papers/chen_et_al_cvpr14.pdf"><img width="100%" src="papers/figs/cvpr14parts.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
            <h4 style="font-size:14.5px; line-height:120%">Detect What You Can: Detecting and Representing Objects using Holistic Models and Body Parts</h4>

	     <p style="margin:-8px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">Xianjie Chen, Roozbeh Mottaghi, Xiaobai Liu, Nam-Gyu Cho, <strong>Sanja Fidler</strong>, Raquel Urtasun, Alan Yuille</p>

		<p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">In <em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>), Columbus, USA, June, 2014</p>
<p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px; color:#E62E00;"> PASCAL VOC with object parts segmentations available in Project page</p>
<p style="margin:-8px 0px 0px 0px;"></p>
 <a href="papers/chen_et_al_cvpr14.pdf" class="buttonTT">Paper</a>&nbsp
 <a class="buttonAA" data-toggle="collapse" data-parent="#PartsCVPR14abs" href="#PartsCVPR14abs-list">Abstract</a>&nbsp
<a href="http://www.stat.ucla.edu/~xianjie.chen/pascal_part_dataset/pascal_part.html" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#PartsCVPR14" href="#PartsCVPR14-list">Bibtex</a><br/>

<p style="margin:2px 0px 0px 0px;"></p>
<div id="PartsCVPR14-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
           <p style="font-size:15px;">@inproceedings{PartsCVPR14,<br/>
    author = {Xianjie Chen and Roozbeh Mottaghi and  Xiaobai Liu and Nam-Gyu Cho and Sanja Fidler and Raquel Urtasun and Alan Yuille},<br/>
    title = {Detect What You Can: Detecting and Representing Objects using Holistic Models and Body Parts},<br/>
    booktitle = {CVPR},<br/>
    year = {2014}<br/>}</p>
</div>
<div id="PartsCVPR14abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.5px;">
Detecting objects becomes difficult when we need to deal
with large shape deformation, occlusion and low resolution.
We propose a novel approach to i) handle large deformations
and partial occlusions in animals (as examples
of highly deformable objects), ii) describe them in terms of
body parts, and iii) detect them when their body parts are
hard to detect (e.g., animals depicted at low resolution). We
represent the holistic object and body parts separately and
use a fully connected model to arrange templates for the
holistic object and body parts. Our model automatically
decouples the holistic object or body parts from the model
when they are hard to detect. This enables us to represent a
large number of holistic object and body part combinations
to better deal with different "detectability" patterns caused
by deformations, occlusion and/or low resolution.

We apply our method to the six animal categories in the
PASCAL VOC dataset and show that our method significantly improves state-of-the-art (by 4.1% AP) and provides
a richer representation for objects. During training we use
annotations for body parts (e.g., head, torso, etc), making
use of a new dataset of fully annotated object parts for PASCAL
VOC 2010, which provides a mask for each part.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
      </li>
    </ul>

<p style="margin:-8px 0px 0px 0px;"></p>

  <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.0%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:11px 1.7% 0 0; width:14.2%;" href="papers/fidler_et_al_cvpr13a.pdf"><img width="100%" src="papers/figs/cvpr13segdpma.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
            <h4 style="font-size:14.5px; line-height:120%">Bottom-up Segmentation for Top-down Detection</h4>

	     <p style="margin:-8px 0px 0px 0px;"></p>

            <p style="font-size:14.0px"><strong>Sanja Fidler</strong>, Roozbeh Mottaghi, Alan Yuille, Raquel Urtasun</p>

		<p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">In <em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>), Portland, USA, June 2013</p>
<p style="margin:-10px 0px 0px 0px;"></p>
            <p style="font-size:14.0px; color:#E62E00;">8% over DPM and 4% over the state-of-the-art on PASCAL VOC at the time. Data available.</p>
            <p style="margin:-8px 0px 0px 0px;"></p>
            <a href="papers/fidler_et_al_cvpr13a.pdf" class="buttonTT">Paper</a>&nbsp
            <a class="buttonAA" data-toggle="collapse" data-parent="#segdpmCVPR13abs" href="#segdpmCVPR13abs-list">Abstract</a>&nbsp
<a href="projects/segDPM.html" class="buttonPP">Project page</a>&nbsp
<a href="papers/segdpm_supplemental.pdf" class="buttonPP">Suppl. Mat.</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#segdpmCVPR13" href="#segdpmCVPR13-list">Bibtex</a><br/>
 
<p style="margin:2px 0px 0px 0px;"></p>
<div id="segdpmCVPR13-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
           <p style="font-size:15px;">@inproceedings{segdpmCVPR13,<br/>
    author = {Sanja Fidler and Roozbeh Mottaghi and Alan Yuille and Raquel Urtasun},<br/>
    title = {Bottom-up Segmentation for Top-down Detection},<br/>
    booktitle = {CVPR},<br/>
    year = {2013}<br/>}
</p>
</div>
<div id="segdpmCVPR13abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.5px;">
In this paper we are interested in how semantic segmentation
can help object detection. Towards this goal, we
propose a novel deformable part-based model which exploits
region-based segmentation algorithms that compute
candidate object regions by bottom-up clustering followed
by ranking of those regions. Our approach allows every
detection hypothesis to select a segment (including void),
and scores each box in the image using both the traditional
HOG filters as well as a set of novel segmentation features.
Thus our model "blends" between the detector and segmentation
models. Since our features can be computed very efficiently given the segments, we maintain the same complexity
as the original DPM. We demonstrate the effectiveness
of our approach in PASCAL VOC 2010, and show that
when employing only a root filter our approach outperforms
Dalal & Triggs detector on all classes, achieving 13%
higher average AP. When employing the parts, we outperform
the original DPM in 19 out of 20 classes, achieving
an improvement of 8% AP. Furthermore, we outperform
the previous state-of-the-art on VOC'10 test by 4%.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
      </li>
    </ul>		

<br/>
<p style="margin:-15px 0px 0px 0px;"></p>


<font style="font-size:1.11em; font-weight:500;">3D Object Detection in RGB-D</font>
<br/>
<br/>
<p style="margin:-12px 0px 0px 0px;"></p>

    <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.0%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:11px 1.7% 0 0; width:14.2%;" href="papers/lin_et_al_iccv13.pdf"><img width="100%" src="papers/figs/iccv13cuboids.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
            <h4 style="font-size:14.5px; line-height:120%">Holistic Scene Understanding for 3D Object Detection with RGBD cameras<em>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp(<b>oral presentation</b>)</em></h4>

	     <p style="margin:-8px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">Dahua Lin, <strong>Sanja Fidler</strong>, Raquel Urtasun</p>

		<p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">In <em>International Conference on Computer Vision</em> (<strong>ICCV</strong>), Sydney, Australia, 2013</p>
<p style="margin:-10px 0px 0px 0px;"></p>
            <p style="font-size:14.0px; color:#E62E00;">Code, models and ground-truth cuboids for NYU-v2 in Project page</p>
<p style="margin:-8px 0px 0px 0px;"></p>
<a href="papers/lin_et_al_iccv13.pdf" class="buttonTT">Paper</a>&nbsp
 <a class="buttonAA" data-toggle="collapse" data-parent="#LinICCV13abs" href="#LinICCV13abs-list">Abstract</a>&nbsp
<a href="projects/scenes3D.html" class="buttonPP">Project page</a>&nbsp
<a href="data/scenes3d/iccv2013_talk.pdf" class="buttonPP">Talk slides</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#LinICCV13" href="#LinICCV13-list">Bibtex</a><br/>
 
<p style="margin:2px 0px 0px 0px;"></p>
<div id="LinICCV13-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
           <p style="font-size:15px;">@inproceedings{LinICCV13,<br/>
    author = {Dahua Lin and Sanja Fidler and Raquel Urtasun},<br/>
    title = {Holistic Scene Understanding for 3D Object Detection with RGBD cameras},<br/>
    booktitle = {ICCV},<br/>
    year = {2013}<br/>}
</p>
</div>
<div id="LinICCV13abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.5px;">
In this paper, we tackle the problem of indoor scene understanding
using RGBD data. Towards this goal, we propose
a holistic approach that exploits 2D segmentation, 3D
geometry, as well as contextual relations between scenes
and objects. Specifically, we extend the CPMC framework
to 3D in order to generate candidate cuboids, and
develop a conditional random field to integrate information
from different sources to classify the cuboids. With this
formulation, scene classification and 3D object recognition
are coupled and can be jointly solved through probabilistic
inference. We test the effectiveness of our approach on
the challenging NYU v2 dataset. The experimental results
demonstrate that through effective evidence integration and
holistic reasoning, our approach achieves substantial improvement
over the state-of-the-art.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
      </li>
    </ul>

<br/>
<p style="margin:-15px 0px 0px 0px;"></p>

<font style="font-size:1.11em; font-weight:500;">3D Object Detection in Monocular Imagery</font>
<br/>
<br/>
<p style="margin:-12px 0px 0px 0px;"></p>


   <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.0%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:11px 1.7% 0 0; width:14.2%;" href="papers/Schwing_etal2013.pdf"><img width="100%" src="papers/figs/iccv13box.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
            <h4 style="font-size:14.5px; line-height:120%">Box In the Box: Joint 3D Layout and Object Reasoning from Single Images</h4>

	     <p style="margin:-8px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">Alex Schwing, <strong>Sanja Fidler</strong>, Marc Pollefeys, Raquel Urtasun</p>

		<p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">In <em>International Conference on Computer Vision</em> (<strong>ICCV</strong>), Sydney, Australia, 2013</p>
<p style="margin:-10px 0px 0px 0px;"></p>
            <p style="font-size:14.0px; color:#E62E00;">Parallel and improved implementation of Structured SVMs available</p>
            <p style="margin:-8px 0px 0px 0px;"></p>
<a href="papers/Schwing_etal2013.pdf" class="buttonTT">Paper</a>&nbsp
 <a class="buttonAA" data-toggle="collapse" data-parent="#SchwingICCV13abs" href="#SchwingICCV13abs-list">Abstract</a>&nbsp
<a href="http://www.alexander-schwing.de/projectsGeneralStructuredPredictionLatentVariables.php" class="buttonPP">Learning code</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#SchwingICCV13" href="#SchwingICCV13-list">Bibtex</a><br/>
 
<p style="margin:2px 0px 0px 0px;"></p>
<div id="SchwingICCV13-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
           <p style="font-size:15px;">@inproceedings{SchwingICCV13,<br/>
    author = {Alex Schwing and Sanja Fidler and Marc Pollefeys and Raquel Urtasun},<br/>
    title = {Box In the Box: Joint 3D Layout and Object Reasoning from Single Images},<br/>
    booktitle = {ICCV},<br/>
    year = {2013}<br/>}
</p>
</div>
<div id="SchwingICCV13abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.5px;">
In this paper we propose an approach to jointly infer the
room layout as well as the objects present in the scene. Towards
this goal, we propose a branch and bound algorithm
which is guaranteed to retrieve the global optimum of the
joint problem. The main difficulty resides in taking into
account occlusion in order to not over-count the evidence.
We introduce a new decomposition method, which generalizes
integral geometry to triangular shapes, and allows us
to bound the different terms in constant time. We exploit
both geometric cues and object detectors as image features
and show large improvements in 2D and 3D object detection
over state-of-the-art deformable part-based models.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
      </li>
    </ul>

<p style="margin:-8px 0px 0px 0px;"></p>


    <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.0%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:11px 1.2% 0 0; width:14.2%;" href="papers/fidler_et_al_nips12.pdf"><img width="100%" src="papers/figs/nips12boxy2.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
            <h4 style="font-size:14.5px; line-height:120%">3D Object Detection and Viewpoint Estimation with a Deformable 3D Cuboid Model&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp<em>(<b>spotlight presentation</b>)</em></h4>

	     <p style="margin:-8px 0px 0px 0px;"></p>

            <p style="font-size:14.0px"><strong>Sanja Fidler</strong>, Sven Dickinson, Raquel Urtasun</p>

		<p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px"><em>Neural Information Processing Systems (<strong>NIPS</strong>)</em>, Lake Tahoe, USA, December 2012</p>
<p style="margin:-10px 0px 0px 0px;"></p>
            <p style="font-size:14.0px; color:#E62E00;">800 CAD models registered to canonical viewpoint available!</p>
<p style="margin:-8px 0px 0px 0px;"></p>
 <a href="papers/fidler_et_al_nips12.pdf" class="buttonTT">Paper</a>&nbsp
  <a class="buttonAA" data-toggle="collapse" data-parent="#FidlerNIPS12abs" href="#FidlerNIPS12abs-list">Abstract</a>&nbsp
<a href="projects/CAD.html" class="buttonPP">CAD dataset</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#FidlerNIPS12" href="#FidlerNIPS12-list">Bibtex</a><br/>

<p style="margin:2px 0px 0px 0px;"></p>
<div id="FidlerNIPS12-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
<p style="font-size:15px;">@inproceedings{FidlerNIPS12,<br/>
    author = {Sanja Fidler and Sven Dickinson and Raquel Urtasun},<br/>
    title = {3D Object Detection and Viewpoint Estimation with a Deformable 3D Cuboid Model},<br/>
    booktitle = {NIPS},<br/>
    year = {2012}<br/>}
</p>
</div>
<div id="FidlerNIPS12abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.5px;">
This paper addresses the problem of category-level 3D object detection. Given
a monocular image, our aim is to localize the objects in 3D by enclosing them
with tight oriented 3D bounding boxes. We propose a novel approach that extends
the well-acclaimed deformable part-based model [1] to reason in 3D. Our model
represents an object class as a deformable 3D cuboid composed of faces and parts,
which are both allowed to deform with respect to their anchors on the 3D box. We
model the appearance of each face in fronto-parallel coordinates, thus effectively
factoring out the appearance variation induced by viewpoint. Our model reasons
about face visibility patters called aspects. We train the cuboid model jointly and
discriminatively and share weights across all aspects to attain efficiency. Inference
then entails sliding and rotating the box in 3D and scoring object hypotheses.
While for inference we discretize the search space, the variables are continuous
in our model. We demonstrate the effectiveness of our approach in indoor and
outdoor scenarios, and show that our approach significantly outperforms the state of-the-art
in both 2D and 3D object detection.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
      </li>
    </ul>

<br/>
<p style="margin:-15px 0px 0px 0px;"></p>


<font style="font-size:1.11em; font-weight:500;">Compositional Models (selected publications)</font>
<br/>
<br/>
<p style="margin:-12px 0px 0px 0px;"></p>

  <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.0%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:9px 3.9% 0 0; width:14.2%;" href="http://arxiv.org/pdf/1408.5516v1.pdf"><img width="100%" src="papers/figs/hierarchy14a.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
            <h4 style="font-size:14.5px; line-height:120%">Learning a Hierarchical Compositional Shape Vocabulary for Multi-class Object Representation</h4>

	     <p style="margin:-8px 0px 0px 0px;"></p>

            <p style="font-size:14.0px"><strong>Sanja Fidler</strong>, Marko Boben, Ales Leonardis</p>

	     <p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px"><em>arXiv preprint arXiv:1408.5516</em>, 2014</p>

<p style="margin:-10px 0px 0px 0px;"></p>
            <p style="font-size:14.0px; color:#E62E00;">Journal version of my PhD work on learning compositional hierarchies encoding spatial relations</p>
<p style="margin:-8px 0px 0px 0px;"></p> 

<a href="http://arxiv.org/pdf/1408.5516v1.pdf" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#FidlerArxiv14abs" href="#FidlerArxiv14abs-list">Abstract</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#FidlerArxiv14" href="#FidlerArxiv14-list">Bibtex</a><br/>

 
<p style="margin:2px 0px 0px 0px;"></p>
<div id="FidlerArxiv14-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{FidlerArxiv14,<br/>
    title = {Learning a Hierarchical Compositional Shape Vocabulary for Multi-class Object Representation},<br/>
    author = {Sanja Fidler and Marko Boben and Ale\v{s} Leonardis},<br/>
    booktitle = {ArXiv:1408.5516},<br/>
    year = {2014}<br/>}
</p>       
</div>
<div id="FidlerArxiv14abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.5px;">
Hierarchies allow feature sharing between objects at multiple levels of representation, can code exponential variability in
a very compact way and enable fast inference. This makes them potentially suitable for learning and recognizing a higher number of
object classes. However, the success of the hierarchical approaches so far has been hindered by the use of hand-crafted features or
predetermined grouping rules. This paper presents a novel framework for learning a hierarchical compositional shape vocabulary for
representing multiple object classes. The approach takes simple contour fragments and learns their frequent spatial configurations.
These are recursively combined into increasingly more complex and class-specific shape compositions, each exerting a high degree of
shape variability. At the top-level of the vocabulary, the compositions are sufficiently large and complex to represent the whole shapes
of the objects. We learn the vocabulary layer after layer, by gradually increasing the size of the window of analysis and reducing the
spatial resolution at which the shape configurations are learned. The lower layers are learned jointly on images of all classes, whereas
the higher layers of the vocabulary are learned incrementally, by presenting the algorithm with one object class after another. The
experimental results show that the learned multi-class object representation scales favorably with the number of object classes and
achieves a state-of-the-art detection performance at both, faster inference as well as shorter training times.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
<!--
<div class="pull-right text-center" style="padding:0.5% 1% 0% 0%; width:11%;">
<a href="http://arxiv.org/pdf/1408.5516v1.pdf" class="buttonT" style="width:100%;">Paper</a><br/>
<p style="margin:0px 0px 0px 0px;"></p>
<a class="buttonS" data-toggle="collapse" data-parent="#FidlerArxiv14" href="#FidlerArxiv14-list" style="width:100%; margin-top:6px; text-decoration: none; color:#FF9933;">Bibtex</a><br/>
        </div>
        -->
      </li>
    </ul>

<p style="margin:-8px 0px 0px 0px;"></p>

  <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.0%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:9px 2.8% 0 0; width:14.2%;" href="papers/fidler_eccv10.pdf"><img width="100%" src="papers/figs/eccv10hier2.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
            <h4 style="font-size:14.5px; line-height:120%">A coarse-to-fine Taxonomy of Constellations for Fast Multi-class Object Detection</h4>

	     <p style="margin:-8px 0px 0px 0px;"></p>

            <p style="font-size:14.0px"><strong>Sanja Fidler</strong>, Marko Boben, Ales Leonardis</p>

		<p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">In <em>European Conference in Computer Vision (<strong>ECCV</strong>)</em>, 2010</p>
            <p style="margin:-8px 0px 0px 0px;"></p>
            <a href="papers/fidler_eccv10.pdf" class="buttonTT">Paper</a>&nbsp
            <a class="buttonAA" data-toggle="collapse" data-parent="#FidlerECCV10abs" href="#FidlerECCV10abs-list">Abstract</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#FidlerECCV10" href="#FidlerECCV10-list">Bibtex</a><br/>
 
<p style="margin:2px 0px 0px 0px;"></p>
<div id="FidlerECCV101-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
<p style="font-size:15px;">@inproceedings{FidlerECCV10,<br/>
    author = {Sanja Fidler and Marko Boben and Ales Leonardis},<br/>
    title = {A coarse-to-fine Taxonomy of Constellations for Fast Multi-class Object Detection},<br/>
    booktitle = {ECCV},<br/>
    year = {2010}<br/>}
</p>
</div>
<div id="FidlerECCV10abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.5px;">
In order for recognition systems to scale to a larger number of
object categories building visual class taxonomies is important to achieve
running times logarithmic in the number of classes [1, 2]. In this paper we
propose a novel approach for speeding up recognition times of multi-class
part-based object representations. The main idea is to construct a taxonomy
of constellation models cascaded from coarse-to-fine resolution and
use it in recognition with an efficient search strategy. The taxonomy is
built automatically in a way to minimize the number of expected computations
during recognition by optimizing the cost-to-power ratio [Blanchard and Geman, <em>Annals of Statistics</em>, 2005]. The
structure and the depth of the taxonomy is not pre-determined but is
inferred from the data. The approach is utilized on the hierarchy-of-parts
model achieving efficiency in both, the representation of the structure
of objects as well as in the number of modeled object classes. We achieve
speed-up even for a small number of object classes on the ETHZ and
TUD dataset. On a larger scale, our approach achieves detection time
that is logarithmic in the number of classes.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
      </li>
    </ul>
<p style="margin:-8px 0px 0px 0px;"></p>


  <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.0%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:8px 2.8% 0 0; width:14.2%;" href="papers/fidler_et_al_nips2009.pdf"><img width="100%" src="papers/figs/nips09hier.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
            <h4 style="font-size:14.5px; line-height:120%">Evaluating multi-class learning strategies in a generative hierarchical framework for object detection</h4>

	     <p style="margin:-8px 0px 0px 0px;"></p>

            <p style="font-size:14.0px"><strong>Sanja Fidler</strong>, Marko Boben, Ales Leonardis</p>

		<p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px"><em>Neural Information Processing Systems (<strong>NIPS</strong>)</em>, 2009</p>
            <p style="margin:-8px 0px 0px 0px;"></p>
            <a href="papers/fidler_et_al_nips2009.pdf" class="buttonTT">Paper</a>&nbsp
            <a class="buttonAA" data-toggle="collapse" data-parent="#FidlerNIPS09abs" href="#FidlerNIPS09abs-list">Abstract</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#FidlerNIPS09" href="#FidlerNIPS09-list">Bibtex</a><br/>
 
<p style="margin:2px 0px 0px 0px;"></p>
<div id="FidlerNIPS09-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
<p style="font-size:15px;">@inproceedings{FidlerNIPS09,<br/>
    author = {Sanja Fidler and Marko Boben and Ales Leonardis},<br/>
    title = {Evaluating multi-class learning strategies in a generative hierarchical framework for object detection},<br/>
    booktitle = {NIPS},<br/>
    year = {2009}<br/>}</p>
</div>
<div id="FidlerNIPS09abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.5px;">
Multi-class object learning and detection is a challenging problem due to the
large number of object classes and their high visual variability. Specialized detectors
usually excel in performance, while joint representations optimize sharing
and reduce inference time -- but are complex to train. Conveniently, sequential
class learning cuts down training time by transferring existing knowledge to novel
classes, but cannot fully exploit the shareability of features among object classes
and might depend on ordering of classes during learning. In hierarchical frameworks
these issues have been little explored. In this paper, we provide a rigorous
experimental analysis of various multiple object class learning strategies within a
generative hierarchical framework. Specifically, we propose, evaluate and compare
three important types of multi-class learning: 1.) independent training of
individual categories, 2.) joint training of classes, and 3.) sequential learning of
classes. We explore and compare their computational behavior (space and time)
and detection performance as a function of the number of learned object classes
on several recognition datasets. We show that sequential training achieves the best
trade-off between inference and training times at a comparable detection performance
and could thus be used to learn the classes on a larger scale.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
<!--
<div class="pull-right text-center" style="padding:0.5% 1% 0% 0%; width:11%;">
<a href="papers/fidler_et_al_nips2009.pdf" class="buttonT" style="width:100%;">Paper</a><br/>
<p style="margin:0px 0px 0px 0px;"></p>
<a class="buttonS" data-toggle="collapse" data-parent="#FidlerNIPS09" href="#FidlerNIPS09-list" style="width:100%; margin-top:6px; text-decoration: none; color:#FF9933;">Bibtex</a><br/>
        </div>
        -->
      </li>
    </ul>
<p style="margin:-8px 0px 0px 0px;"></p>

  <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.0%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:11px 1.7% 0 0; width:14.2%;" href=""></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
            <h4 style="font-size:14.5px; line-height:120%">Learning Hierarchical Compositional Representations of Object Structure</h4>

	     <p style="margin:-8px 0px 0px 0px;"></p>

            <p style="font-size:14.0px"><strong>Sanja Fidler</strong>, Marko Boben, Ales Leonardis</p>

		<p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px"><em>Object Categorization: Computer and Human Vision Perspectives</em></br>
	     Editors: S. Dickinson, A. Leonardis, B. Schiele and M. J. Tarr<br />
            Cambridge university press, 2009</p>
            <p style="margin:-8px 0px 0px 0px;"></p>
            <a class="buttonSS" data-toggle="collapse" data-parent="#FidlerChapter09" href="#FidlerChapter09-list">Bibtex</a><br/>
 
<p style="margin:2px 0px 0px 0px;"></p>
<div id="FidlerChapter09-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
            <p>@InCollection{FidlerChapter09,<br/>
    author = {Sanja Fidler and Marko Boben and Ales Leonardis},<br/>
    title = {Learning Hierarchical Compositional Representations of Object Structure},<br/>
    booktitle = {Object Categorization: Computer and Human Vision Perspectives},<br/>
    editor    = {Sven Dickinson and Ale\v{s} Leonardis and Bernt Schiele and Michael J. Tarr},<br/>
    year      = {2009},<br/>
    publisher = {Cambridge University Press},<br/>
    pages = {}<br/>}
</p>
</div>
</div>
<!--
<div class="pull-right text-center" style="padding:0.5% 1% 0% 0%; width:11%;">
<p style="margin:0px 0px 0px 0px;"></p>
<a class="buttonS" data-toggle="collapse" data-parent="#FidlerChapter09" href="#FidlerChapter09-list" style="width:100%; margin-top:6px; text-decoration: none; color:#FF9933;">Bibtex</a><br/>
        </div>
        -->
      </li>
    </ul>
<p style="margin:-8px 0px 0px 0px;"></p>

<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.0%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:11px 1.7% 0 0; width:14.2%;" href="papers/cvpr07fidler.pdf"><img width="100%" src="papers/figs/cvpr07hier2.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
            <h4 style="font-size:14.5px; line-height:120%">Towards Scalable Representations of Object Categories: Learning a Hierarchy of Parts</h4>

	     <p style="margin:-8px 0px 0px 0px;"></p>

            <p style="font-size:14.0px"><strong>Sanja Fidler</strong>, Ales Leonardis</p>

		<p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px"><em>Conference on Computer Vision and Pattern Recognition (<strong>CVPR</strong>)</em>, 2007</p>
<p style="margin:-10px 0px 0px 0px;"></p>
            <p style="font-size:14.0px; color:#E62E00;">Learning a deep hierarchy of interpretable features encoding spatial relations</p>
            <p style="margin:-8px 0px 0px 0px;"></p>
            <a href="papers/cvpr07fidler.pdf" class="buttonTT">Paper</a>&nbsp
            <a class="buttonAA" data-toggle="collapse" data-parent="#FidlerCVPR07abs" href="#FidlerCVPR07abs-list">Abstract</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#FidlerCVPR07" href="#FidlerCVPR07-list">Bibtex</a><br/>
 
<p style="margin:2px 0px 0px 0px;"></p>
<div id="FidlerCVPR07-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
<p style="font-size:15px;">@inproceedings{FidlerCVPR07,<br/>
    author = {Sanja Fidler and Ales Leonardis},<br/>
    title = {Towards Scalable Representations of Object Categories: Learning a Hierarchy of Parts},<br/>
    booktitle = {CVPR},<br/>
    year = {2007}<br/>}</p>
</div>
<div id="FidlerCVPR07abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.5px;">
This paper proposes a novel approach to constructing
a hierarchical representation of visual input that aims to
enable recognition and detection of a large number of object
categories. Inspired by the principles of efficient indexing
(bottom-up), robust matching (top-down), and ideas of
compositionality, our approach learns a hierarchy of spatially
flexible compositions, i.e. parts, in an unsupervised,
statistics-driven manner. Starting with simple, frequent features,
we learn the statistically most significant compositions
(parts composed of parts), which consequently define
the next layer. Parts are learned sequentially, layer after
layer, optimally adjusting to the visual data. Lower layers
are learned in a category-independent way to obtain complex,
yet sharable visual building blocks, which is a crucial
step towards a scalable representation. Higher layers of the
hierarchy, on the other hand, are constructed by using specific
categories, achieving a category representation with a
small number of highly generalizable parts that gained their
structural flexibility through composition within the hierarchy.
Built in this way, new categories can be efficiently and
continuously added to the system by adding a small number
of parts only in the higher layers. The approach is demonstrated
on a large collection of images and a variety of object
categories. Detection results confirm the effectiveness
and robustness of the learned parts.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
      </li>
    </ul>
<p style="margin:-8px 0px 0px 0px;"></p>

      <a data-toggle="collapse" data-parent="#detection" href="#detection-info">close window</a>
      </div>
      
      </div>
      
      <div id="moviebook-info" class="panel-collapse collapse out" style="padding:0% 3% 1% 3%; border-radius:6px; border-style: solid; border-color: #666666;">

<p style="margin:10px 0px 0px 0px;"></p>
<font style="font-size:1.34em; font-weight:500;">  Movies and Books</font>
      <div class="media">
      
      <p align="justify">One of our most recent interests is parsing movies and books. This entails designing semantic representation of video as well as text. Our first work on this topic aims to align movies and books with the goal of collecting large-scale multi-sentence, story-like captions of video clips. We also proposed a new neural representation of sentences called <i>ski-thoughts</i> trained unsupervised from a large collection of books (<a href="http://www.cs.toronto.edu/~mbweb/">Book Corpus</a> dataset). Our model represents each sentence as a vector which can be used for e.g. semantic relatedness, image-sentence ranking, paraphrase detection, and sentiment analysis.</p>
      
       <h3> Relevant Publications</h3>

<br/>
<p style="margin:-10px 0px 0px 0px;"></p>


       <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.4%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:11px 2% 0 0; width:14%;" href=""><img width="100%" src="papers/figs/skipthoughts.png"><br/><p style="margin:12px 0px 0px 0px;"></p><img width="100%" src="papers/figs/skipthoughts_res.jpg"></div>          
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:70%;">
            <h4 style="font-size:14.5px; line-height:120%">Skip-Thought Vectors</h4>

	     <p style="margin:-8px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard Zemel, Antonio Torralba, Raquel Urtasun, <strong>Sanja Fidler</strong></p>

	     <p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">arXiv preprint <em>arXiv:1506.06726</em>, 2015</p>
<p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px; color:#E62E00;"> Sent2vec neural representation trained on 11K books</p>

<p style="margin:-8px 0px 0px 0px;"></p>
            
            <a href="http://arxiv.org/abs/1506.06726" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#SkipThoughtsabs" href="#SkipThoughtsabs-list">Abstract</a>&nbsp
<a href="https://github.com/ryankiros/skip-thoughts" class="buttonPP">Code</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#SkipThoughts" href="#SkipThoughts-list">Bibtex</a><br/>

 
<p style="margin:2px 0px 0px 0px;"></p>
<div id="SkipThoughts-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{moviebook,<br/>
    title = {Skip-Thought Vectors},<br/>
    author = {Ryan Kiros and Yukun Zhu and Ruslan Salakhutdinov and Richard Zemel and Antonio Torralba and Raquel Urtasun and Sanja Fidler},<br/>
    booktitle = {arXiv preprint arXiv:1506.06726},<br/>
    year = {2015}</br>}
</p>      
</div>
<div id="SkipThoughtsabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.5px;">
coming soon 
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>  
</div>
                              <div class="pull-right text-center" style="padding:4px 0.5% 0 0; width:6.5%;" href=""><img width="100%" src="papers/figs/new_burst.jpg"></div> 
                              </div>
      </li>
    </ul>

<p style="margin:-8px 0px 0px 0px;"></p>

    <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.4%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:11px 1.5% 0 0; width:14%;" href=""><img width="100%" src="papers/figs/moviebook.jpg"><br/><p style="margin:4px 0px 0px 0px;"></p><img width="100%" src="papers/figs/harrypotter_small.png"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:70%;">
            <h4 style="font-size:14.5px; line-height:120%">Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books</h4>

	     <p style="margin:-8px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">Yukun Zhu, Ryan Kiros, Richard Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, <strong>Sanja Fidler</strong></p>

	     <p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">arXiv preprint <em>arXiv:1506.06724</em>, 2015</p>
<p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px; color:#E62E00;"> Aligning movies and books for story-like captioning</p>

<p style="margin:-8px 0px 0px 0px;"></p>
            
            <a href="http://arxiv.org/abs/1506.06724" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#MovieBookabs" href="#MovieBookabs-list">Abstract</a>&nbsp
<a href="http://www.cs.toronto.edu/~mbweb/" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#MovieBook" href="#MovieBook-list">Bibtex</a><br/>

 
<p style="margin:2px 0px 0px 0px;"></p>
<div id="MovieBook-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{moviebook,<br/>
    title = {Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books},<br/>
    author = {Yukun Zhu and Ryan Kiros and Richard Zemel and Ruslan Salakhutdinov and Raquel Urtasun and Antonio Torralba and Sanja Fidler},<br/>
    booktitle = {arXiv preprint arXiv:1506.06724},<br/>
    year = {2015}</br>}
</p>      
</div>
<div id="MovieBookabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.5px;">
coming soon 
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div> 
</div>
<div class="pull-right text-center" style="padding:4px 0.5% 0 0; width:6.5%;" href=""><img width="100%" src="papers/figs/new_burst.jpg"></div> 
      </li>
    </ul>

  
      <a data-toggle="collapse" data-parent="#moviebook" href="#moviebook-info">close window</a>
      </div>
      </div>
      
      
      
         <div id="scene-info" class="panel-collapse collapse out" style="padding:0% 3% 1% 3%; border-radius:6px; border-style: solid; border-color: #666666;">

<p style="margin:10px 0px 0px 0px;"></p>
<font style="font-size:1.34em; font-weight:500;"> Scene Understanding</font>
      <div class="media">
      
      <p align="justify">3D scene understanding requires reasoning about multiple related tasks: (3D) object detection and segmentation, relationships between objects (e.g., support), scene-type prediction, as well as inferring the structure of the scene as well (e.g. ground plane in outdoor scenarios and layout of the room in indoors). Our work focuses on designing holistic models that reason jointly about the related sub-tasks, and as such outperform the individual modules.</p>
     
     <p align="justify">Our most recent work in this domain aims to reconstruct apartments in 3D from rental ads, for an enhanced viewer's experience. This entails localizing each photo within the floor-plan by exploiting semantic, scene and geometric cues. Renting will never be the same again. ;)
      </p>
      
       <h3> Relevant Publications</h3>

<br/>
<p style="margin:-10px 0px 0px 0px;"></p>

<font style="font-size:1.11em; font-weight:500;">Indoor Scene Understanding (Monocular)</font>
<br/>
<br/>
<p style="margin:-12px 0px 0px 0px;"></p>
<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.0%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:11px 4% 0 0; width:14.2%;" href=""><img width="100%" src="papers/figs/cvpr15aptlayout_small.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:67%;">
            <h4 style="font-size:14.5px; line-height:120%">Rent3D: Floor-Plan  Priors for Monocular Layout Estimation &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp(<strong><em>oral presentation</em></strong>)</h4>

	     <p style="margin:-8px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">Chenxi Liu, Alex Schwing, Kaustav Kundu, Raquel Urtasun, <strong>Sanja Fidler</strong></p>

	     <p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">In <em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>), Boston, 2015</p>
<p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px; color:#E62E00;"> Rent an apartment in 3D!</p>

<p style="margin:-8px 0px 0px 0px;"></p>
            
            <a href="papers/rent3DCVPR15.pdf" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#ApartmentsCVPR15abs" href="#ApartmentsCVPR15abs-list">Abstract</a>&nbsp
<a href="papers/rent3d_suppl.pdf" class="buttonMM">Suppl. Mat.</a>&nbsp
<a href="http://www.cs.toronto.edu/~fidler/projects/rent3D.html" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#ApartmentsCVPR15" href="#ApartmentsCVPR15-list">Bibtex</a><br/>

 
<p style="margin:2px 0px 0px 0px;"></p>
<div id="ApartmentsCVPR15-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{ApartmentsCVPR15,<br/>
    title = {Rent3D: Floor-Plan  Priors for Monocular Layout Estimation},<br/>
    author = {Chenxi Liu and Alex Schwing and Kaustav Kundu and Raquel Urtasun and Sanja Fidler},<br/>
    booktitle = {CVPR},<br/>
    year = {2015}}
</p>      
</div>
<div id="ApartmentsCVPR15abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.5px;">
The goal of this paper is to enable a 3D "virtual-tour" of an apartment given a small set of monocular images of different rooms, 
as well as a 2D floor plan. We frame the problem as the one of inference in a Markov random field which  reasons about the layout 
of each room and its relative pose (3D rotation and translation) within the full apartment. This gives us information, for example, 
about in which room the picture was taken.  
What sets us apart from past work in layout estimation is the use of floor plans as a source of prior knowledge.  
In particular, we exploit the floor plan to impose aspect ratio constraints across the layouts of different rooms, as well as to 
extract  semantic information, e.g., the location of windows which are labeled in floor plans.  We show that this information 
can significantly help in resolving the challenging  room-apartment alignment problem. 
We also derive an efficient exact inference algorithm which takes only a few ms per apartment. This is due to the fact that we 
exploit integral geometry as well as our new bounds on the aspect ratio of rooms which allow us to carve the space, reducing 
significantly the number of physically possible configurations. 
We demonstrate the effectiveness of our approach in a new dataset which contains over 200 apartments. 
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
<div class="pull-right text-center" style="margin-top:3px; margin-right:4px; padding:5px 0.5% 1px 0.5%; width:13.5%; border-radius: 5px; border-style: solid; border-width: 1.5px; border-color: #666666;"><a href="http://techtalks.tv/talks/rent3d-floor-plan-priors-for-monocular-layout-estimation/61611/"><img width="100%" src="papers/figs/rent3d-talk1.jpg"></a><br/><font size="2.5">[<a href="http://techtalks.tv/talks/rent3d-floor-plan-priors-for-monocular-layout-estimation/61611/">talk</a>]&nbsp&nbsp[<a href="slides/rent3d_talk.pdf">slides</a>]</font></div>
</div>
<!--
<div class="pull-right text-center" style="padding:0.2% 1% 0% 0%; width:11%;">
<a href="papers/rent3DCVPR15.pdf" class="buttonT" style="width:100%; height:14.0px; font-size:10.5px;">Paper</a><br/>
<p style="margin:-5px 0px 0px 0px;"></p>
<a class="buttonA" data-toggle="collapse" data-parent="#ApartmentsCVPR15abs" href="#ApartmentsCVPR15abs-list" style="width:100%; margin-top:6px; text-decoration: none; color:#00CFCF; height:14.0px; font-size:10.5px;">Abstract</a><br/>
<p style="margin:0.0px 0px 0px 0px;"></p>
<a href="papers/rent3d_suppl.pdf" class="buttonM" style="width:100%; padding:0 0 0px 0; height:14.0px; font-size:10.5px;">Suppl. Mat.</a><br/>
<p style="margin:-3px 0px 0px 0px;"></p>
<a href="http://www.cs.toronto.edu/~fidler/projects/rent3D.html" class="buttonP" style="width:100%; height:14.0px; font-size:10.5px; padding:0 0 0px 0;">Project page</a>
<p style="margin:-3px 0px 0px 0px;"></p>
<a class="buttonS" data-toggle="collapse" data-parent="#ApartmentsCVPR15" href="#ApartmentsCVPR15-list" style="width:100%; margin-top:6px; text-decoration: none; color:#FF9933; height:14.0px; font-size:10.5px;">Bibtex</a><br/>
        </div>
        -->
      </li>
    </ul>

<p style="margin:-8px 0px 0px 0px;"></p>

<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.0%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:11px 1.7% 0 0; width:14.2%;" href="papers/Schwing_etal2013.pdf"><img width="100%" src="papers/figs/iccv13box.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
            <h4 style="font-size:14.5px; line-height:120%">Box In the Box: Joint 3D Layout and Object Reasoning from Single Images</h4>

	     <p style="margin:-8px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">Alex Schwing, <strong>Sanja Fidler</strong>, Marc Pollefeys, Raquel Urtasun</p>

		<p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">In <em>International Conference on Computer Vision</em> (<strong>ICCV</strong>), Sydney, Australia, 2013</p>
<p style="margin:-10px 0px 0px 0px;"></p>
            <p style="font-size:14.0px; color:#E62E00;">Parallel and improved implementation of Structured SVMs available</p>
            <p style="margin:-8px 0px 0px 0px;"></p>
<a href="papers/Schwing_etal2013.pdf" class="buttonTT">Paper</a>&nbsp
 <a class="buttonAA" data-toggle="collapse" data-parent="#SchwingICCV13abs" href="#SchwingICCV13abs-list">Abstract</a>&nbsp
<a href="http://www.alexander-schwing.de/projectsGeneralStructuredPredictionLatentVariables.php" class="buttonPP">Learning code</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#SchwingICCV13" href="#SchwingICCV13-list">Bibtex</a><br/>
 
<p style="margin:2px 0px 0px 0px;"></p>
<div id="SchwingICCV13-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
           <p style="font-size:15px;">@inproceedings{SchwingICCV13,<br/>
    author = {Alex Schwing and Sanja Fidler and Marc Pollefeys and Raquel Urtasun},<br/>
    title = {Box In the Box: Joint 3D Layout and Object Reasoning from Single Images},<br/>
    booktitle = {ICCV},<br/>
    year = {2013}<br/>}
</p>
</div>
<div id="SchwingICCV13abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.5px;">
In this paper we propose an approach to jointly infer the
room layout as well as the objects present in the scene. Towards
this goal, we propose a branch and bound algorithm
which is guaranteed to retrieve the global optimum of the
joint problem. The main difficulty resides in taking into
account occlusion in order to not over-count the evidence.
We introduce a new decomposition method, which generalizes
integral geometry to triangular shapes, and allows us
to bound the different terms in constant time. We exploit
both geometric cues and object detectors as image features
and show large improvements in 2D and 3D object detection
over state-of-the-art deformable part-based models.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
<!--
<div class="pull-right text-center" style="padding:0.5% 1% 0% 0%; width:11%;">
<a href="papers/Schwing_etal2013.pdf" class="buttonT" style="width:100%;">Paper</a><br/>
<p style="margin:1.5px 0px 0px 0px;"></p>
<a href="http://www.alexander-schwing.de/projectsGeneralStructuredPredictionLatentVariables.php" class="buttonP" style="width:100%;">Learning code</a><br/>
<p style="margin:0px 0px 0px 0px;"></p>
<a class="buttonS" data-toggle="collapse" data-parent="#SchwingICCV13" href="#SchwingICCV13-list" style="width:100%; margin-top:6px; text-decoration: none; color:#FF9933;">Bibtex</a><br/>
        </div>
        -->
      </li>
    </ul>
<br/>
<p style="margin:-15px 0px 0px 0px;"></p>

<font style="font-size:1.11em; font-weight:500;">Indoor Scene Understanding (RGB-D)</font>
<br/>
<br/>
<p style="margin:-12px 0px 0px 0px;"></p>
  <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.0%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:11px 2.2% 0 0; width:14.2%;" href=""><img width="100%" src="papers/figs/arxiv2015generation_small.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
            <h4 style="font-size:14.5px; line-height:120%">Generating Multi-Sentence Lingual Descriptions of Indoor Scenes&nbsp&nbsp&nbsp&nbsp&nbsp(<strong><em>oral presentation</em></strong>)</h4>

	     <p style="margin:-8px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">Dahua Lin, Chen Kong, <strong>Sanja Fidler</strong>, Raquel Urtasun</p>

	     <p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">In <em>British Machine Vision Conference</em> (<strong>BMVC</strong>), To appear, 2015</p>
            <!--<p style="font-size:14.0px"><em>arXiv:1503.00064</em>, Feb 28, 2015</p>-->
            
            <p style="margin:-8px 0px 0px 0px;"></p>
            
            
            <a href="http://arxiv.org/abs/1503.00064" class="buttonTT">Paper</a>&nbsp
 <a class="buttonAA" data-toggle="collapse" data-parent="#Lin15abs" href="#Lin15abs-list">Abstract</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#Lin15" href="#Lin15-list">Bibtex</a><br/>
 
<p style="margin:2px 0px 0px 0px;"></p>
<div id="Lin15-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{Lin15,<br/>
    title = {Generating Multi-Sentence Lingual Descriptions of Indoor Scenes},<br/>
    author = {Dahua Lin and Chen Kong and Sanja Fidler and Raquel Urtasun},<br/>
    booktitle = {arXiv:1503.00064},<br/>
    year = {2015}}
</p>      
</div>
<div id="Lin15abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.5px;">
This paper proposes a novel framework for generating lingual descriptions of
indoor scenes. Whereas substantial efforts have been made to tackle this
problem, previous approaches focusing primarily on generating a single sentence
for each image, which is not sufficient for describing complex scenes. We
attempt to go beyond this, by generating coherent descriptions with multiple
sentences. Our approach is distinguished from conventional ones in several
aspects: (1) a 3D visual parsing system that jointly infers objects,
attributes, and relations; (2) a generative grammar learned automatically from
training text; and (3) a text generation algorithm that takes into account the
coherence among sentences. Experiments on the augmented NYU-v2 dataset show
that our framework can generate natural descriptions with substantially higher
ROGUE scores compared to those produced by the baseline.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
<!--
<div class="pull-right text-center" style="padding:0.4% 1% 0% 0%; width:11%;">
<a href="http://arxiv.org/abs/1503.00064" class="buttonT" style="width:100%;">Paper</a><br/>
<p style="margin:0px 0px 0px 0px;"></p>
<a class="buttonA" data-toggle="collapse" data-parent="#Lin15abs" href="#Lin15abs-list" style="width:100%; margin-top:6px; text-decoration: none; color:#00CFCF;">Abstract</a><br/>
<p style="margin:0px 0px 0px 0px;"></p>
<a class="buttonS" data-toggle="collapse" data-parent="#Lin15" href="#Lin15-list" style="width:100%; margin-top:6px; text-decoration: none; color:#FF9933;">Bibtex</a><br/>
        </div>
        -->
      </li>
    </ul>
  <p style="margin:-8px 0px 0px 0px;"></p>  
  
  
    <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.0%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:10px 2.7% 0 0; width:14.2%;" href="papers/kong_et_al_cvpr14.pdf"><img src="papers/figs/cvpr14coref.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
            <h4 style="font-size:14.5px; line-height:120%">What are you talking about? Text-to-Image Coreference</h4>

	     <p style="margin:-8px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">Chen Kong, Dahua Lin, Mohit Bansal, Raquel Urtasun, <strong>Sanja Fidler</strong></p>

	     <p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">In <em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>), Columbus, USA, June, 2014</p>
<p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px; color:#E62E00;">Exploits text for visual parsing and aligns nouns to objects. Code and data out soon!</p>
            <p style="margin:-8px 0px 0px 0px;"></p>
            <a href="papers/kong_et_al_cvpr14.pdf" class="buttonTT">Paper</a>&nbsp
             <a class="buttonAA" data-toggle="collapse" data-parent="#KongCVPR14abs" href="#KongCVPR14abs-list">Abstract</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#KongCVPR14" href="#KongCVPR14-list">Bibtex</a><br/>
 
<p style="margin:2px 0px 0px 0px;"></p>
<div id="KongCVPR14-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{KongCVPR14,<br/>
    title = {What are you talking about? Text-to-Image Coreference},<br/>
    author = {Chen Kong and Dahua Lin and Mohit Bansal and Raquel Urtasun and Sanja Fidler},<br/>
    booktitle = {CVPR},<br/>
    year = {2014}<br/>}
</p>  
</div>
<div id="KongCVPR14abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.5px;">
In this paper we exploit natural sentential descriptions
of RGB-D scenes in order to improve 3D semantic parsing.
Importantly, in doing so, we reason about which particular
object each noun/pronoun is referring to in the image. This
allows us to utilize visual information in order to disambiguate
the so-called coreference resolution problem that
arises in text. Towards this goal, we propose a structure
prediction model that exploits potentials computed from text
and RGB-D imagery to reason about the class of the 3D objects,
the scene type, as well as to align the nouns/pronouns
with the referred visual objects. We demonstrate the effectiveness
of our approach on the challenging NYU-RGBD v2
dataset, which we enrich with natural lingual descriptions.
We show that our approach significantly improves 3D detection
and scene classification accuracy, and is able to reliably
estimate the text-to-image alignment. Furthermore,
by using textual and visual information, we are also able to
successfully deal with coreference in text, improving upon
the state-of-the-art Stanford coreference system.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
<!--
<div class="pull-right text-center" style="padding:0.5% 1% 0% 0%; width:11%;">
<a href="papers/kong_et_al_cvpr14.pdf" class="buttonT" style="width:100%;">Paper</a><br/>
<p style="margin:0px 0px 0px 0px;"></p>
<a class="buttonS" data-toggle="collapse" data-parent="#KongCVPR14" href="#KongCVPR14-list" style="width:100%; margin-top:6px; text-decoration: none; color:#FF9933;">Bibtex</a><br/>
        </div>
        -->
      </li>
    </ul>
    
    <p style="margin:-8px 0px 0px 0px;"></p>	
    
        <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.0%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:11px 1.7% 0 0; width:14.2%;" href="papers/lin_et_al_iccv13.pdf"><img width="100%" src="papers/figs/iccv13cuboids.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
            <h4 style="font-size:14.5px; line-height:120%">Holistic Scene Understanding for 3D Object Detection with RGBD cameras<em>&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp(<b>oral presentation</b>)</em></h4>

	     <p style="margin:-8px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">Dahua Lin, <strong>Sanja Fidler</strong>, Raquel Urtasun</p>

		<p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">In <em>International Conference on Computer Vision</em> (<strong>ICCV</strong>), Sydney, Australia, 2013</p>
<p style="margin:-10px 0px 0px 0px;"></p>
            <p style="font-size:14.0px; color:#E62E00;">Code, models and ground-truth cuboids for NYU-v2 in Project page</p>
<p style="margin:-8px 0px 0px 0px;"></p>
<a href="papers/lin_et_al_iccv13.pdf" class="buttonTT">Paper</a>&nbsp
 <a class="buttonAA" data-toggle="collapse" data-parent="#LinICCV13abs" href="#LinICCV13abs-list">Abstract</a>&nbsp
<a href="projects/scenes3D.html" class="buttonPP">Project page</a>&nbsp
<a href="data/scenes3d/iccv2013_talk.pdf" class="buttonPP">Talk slides</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#LinICCV13" href="#LinICCV13-list">Bibtex</a><br/>
 
<p style="margin:2px 0px 0px 0px;"></p>
<div id="LinICCV13-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
           <p style="font-size:15px;">@inproceedings{LinICCV13,<br/>
    author = {Dahua Lin and Sanja Fidler and Raquel Urtasun},<br/>
    title = {Holistic Scene Understanding for 3D Object Detection with RGBD cameras},<br/>
    booktitle = {ICCV},<br/>
    year = {2013}<br/>}
</p>
</div>
<div id="LinICCV13abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.5px;">
In this paper, we tackle the problem of indoor scene understanding
using RGBD data. Towards this goal, we propose
a holistic approach that exploits 2D segmentation, 3D
geometry, as well as contextual relations between scenes
and objects. Specifically, we extend the CPMC framework
to 3D in order to generate candidate cuboids, and
develop a conditional random field to integrate information
from different sources to classify the cuboids. With this
formulation, scene classification and 3D object recognition
are coupled and can be jointly solved through probabilistic
inference. We test the effectiveness of our approach on
the challenging NYU v2 dataset. The experimental results
demonstrate that through effective evidence integration and
holistic reasoning, our approach achieves substantial improvement
over the state-of-the-art.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
<!--
<div class="pull-right text-center" style="padding:0.5% 1% 0% 0%; width:11%;">
<a href="papers/lin_et_al_iccv13.pdf" class="buttonT" style="width:100%;">Paper</a><br/>
<p style="margin:1.5px 0px 0px 0px;"></p>
<a href="projects/scenes3D.html" class="buttonP" style="width:100%;">Project page</a><br/>
<p style="margin:1.5px 0px 0px 0px;"></p>
<a href="data/scenes3d/iccv2013_talk.pdf" class="buttonP" style="width:100%;">Talk slides</a><br/>
<p style="margin:0px 0px 0px 0px;"></p>
<a class="buttonS" data-toggle="collapse" data-parent="#LinICCV13" href="#LinICCV13-list" style="width:100%; margin-top:6px; text-decoration: none; color:#FF9933;">Bibtex</a><br/>
        </div>
        -->
      </li>
    </ul>
    
<br/>
<p style="margin:-15px 0px 0px 0px;"></p>


<font style="font-size:1.11em; font-weight:500;">Outdoor Scene Understanding (Monocular)</font>
<br/>
<br/>
<p style="margin:-12px 0px 0px 0px;"></p>
   <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.0%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:11px 2.2% 0 0; width:14.2%;" href=""><img width="100%" src="papers/figs/cvpr15osm_small.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:67%;">
            <h4 style="font-size:14.5px; line-height:120%">Holistic 3D Scene Understanding from a Single Geo-tagged Image &nbsp&nbsp(<strong><em>oral presentation</em></strong>)</h4>

	     <p style="margin:-8px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">Shenlong Wang, <strong>Sanja Fidler</strong>, Raquel Urtasun</p>

	     <p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">In <em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>), Boston, 2015</p>
            <p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px; color:#E62E00;">Exploiting map priors for segmentation and monocular depth estimation</p>
            <p style="margin:-8px 0px 0px 0px;"></p>

 <a href="papers/wangCVPR15.pdf" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#WangCVPR15abs" href="#WangCVPR15abs-list">Abstract</a>&nbsp
<a href="http://www.cs.toronto.edu/~slwang/kitti3d/" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#WangCVPR15" href="#WangCVPR15-list">Bibtex</a><br/>

<p style="margin:2px 0px 0px 0px;"></p>
<div id="WangCVPR15-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{WangCVPR15,<br/>
    title = {Holistic 3D Scene Understanding from a Single Geo-tagged Image},<br/>
    author = {Shenlong Wang and Sanja Fidler and Raquel Urtasun},<br/>
    booktitle = {CVPR},<br/>
    year = {2015}}
</p>      
</div>
<div id="WangCVPR15abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.5px;">
In this paper we are interested in exploiting geographic
priors to help outdoor scene understanding. Towards this
goal we propose a holistic approach that reasons jointly
about 3D object detection, pose estimation, semantic segmentation
as well as depth reconstruction from a single image.
Our approach takes advantage of large-scale crowdsourced
maps to generate dense geographic, geometric and
semantic priors by rendering the 3D world. We demonstrate
the effectiveness of our holistic model on the challenging
KITTI dataset, and show significant improvements
over the baselines in all metrics and tasks.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
<div class="pull-right text-center" style="margin-top:3px; margin-right:4px; padding:5px 0.5% 1px 0.5%; width:13.5%; border-radius: 5px; border-style: solid; border-width: 1.5px; border-color: #666666;"><a href="http://techtalks.tv/talks/holistic-3d-scene-understanding-from-a-single-geo-tagged-image/61614/"><img width="100%" src="papers/figs/holistic3d-talk.jpg"></a><br/><font size="2.5">[<a href="http://techtalks.tv/talks/holistic-3d-scene-understanding-from-a-single-geo-tagged-image/61614/">talk</a>]&nbsp&nbsp[<a href="http://www.cs.toronto.edu/~slwang/kitti3d/cvpr15.pptx">slides</a>]</font></div>
</div>
<!--
<div class="pull-right text-center" style="padding:0.5% 1% 0% 0%; width:11%;">
<a href="papers/wangCVPR15.pdf" class="buttonT" style="width:100%;">Paper</a><br/>
<p style="margin:1.5px 0px 0px 0px;"></p>
<a class="buttonA" data-toggle="collapse" data-parent="#WangCVPR15abs" href="#WangCVPR15abs-list" style="width:100%; margin-top:6px; text-decoration: none; color:#00CFCF;">Abstract</a><br/>
<p style="margin:1.5px 0px 0px 0px;"></p>
<a href="http://www.cs.toronto.edu/~slwang/kitti3d/" class="buttonP" style="width:100%; padding:0 0 0px 0;">Project page</a>
<p style="margin:0px 0px 0px 0px;"></p>
<a class="buttonS" data-toggle="collapse" data-parent="#WangCVPR15" href="#WangCVPR15-list" style="width:100%; margin-top:6px; text-decoration: none; color:#FF9933;">Bibtex</a><br/>
        </div>
        -->
      </li>
    </ul>
<br/>
<p style="margin:-15px 0px 0px 0px;"></p>


<font style="font-size:1.11em; font-weight:500;">2D Scene Understanding</font>
<br/>
<br/>
<p style="margin:-12px 0px 0px 0px;"></p>
  <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.0%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:11px 1.7% 0 0; width:14.2%;" href=""><img width="100%" src="papers/figs/pami15small.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
            <h4 style="font-size:14.5px; line-height:120%">Human-Machine CRFs for Identifying Bottlenecks in Scene Understanding</h4>

	     <p style="margin:-8px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">Roozbeh Mottaghi, <strong>Sanja Fidler</strong>, Alan Yuille, Raquel Urtasun, Devi Parikh</p>

	     <p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px"><em>Transactions on Pattern Analysis and Machine Intelligence</em> (<strong>TPAMI</strong>), To appear 2015</p>
            <p style="margin:-8px 0px 0px 0px;"></p>

    <a href="papers/mottaghiPAMI15.pdf" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#Mottaghi15abs" href="#Mottaghi15abs-list">Abstract</a>&nbsp
    <a href="papers/mottaghiPAMI15supmat.pdf" class="buttonMM">Suppl. Mat.</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#Mottaghi15" href="#Mottaghi15-list">Bibtex</a><br/>
 
<p style="margin:2px 0px 0px 0px;"></p>
<div id="Mottaghi15-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@article{MottaghiPAMI15,<br/>
    title = {Human-Machine CRFs for Identifying Bottlenecks in Scene Understanding},<br/>
    author = {Roozbeh Mottaghi and Sanja Fidler and Alan Yuille and Raquel Urtasun and Devi Parikh},<br/>
    journal = {Trans. on Pattern Analysis and Machine Intelligence},<br/>
    year = {2015}<br/>}
</p>      
</div>
<div id="Mottaghi15abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.5px;">
Recent trends in image understanding have pushed for scene understanding models that jointly reason about various
tasks such as object detection, scene recognition, shape analysis, contextual reasoning, and local appearance based classifiers.
In this work, we are interested in understanding the roles of these different tasks in improved scene understanding, in particular
semantic segmentation, object detection and scene recognition. Towards this goal, we "plug-in" human subjects for each of the
various components in a conditional random field model. Comparisons among various hybrid human-machine CRFs give us
indications of how much "head room" there is to improve scene understanding by focusing research efforts on various individual
tasks.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
      </li>
    </ul>

<p style="margin:-8px 0px 0px 0px;"></p>

   <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.0%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:11px 1.7% 0 0; width:14.2%;" href="papers/fidler_et_al_cvpr13b.pdf"><img width="100%" src="papers/figs/cvpr13sent.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
            <h4 style="font-size:14.5px; line-height:120%">A Sentence is Worth a Thousand Pixels</h4>

	     <p style="margin:-8px 0px 0px 0px;"></p>

            <p style="font-size:14.0px"><strong>Sanja Fidler</strong>, Abhishek Sharma,  Raquel Urtasun</p>

		<p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">In <em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>), Portland, USA, June 2013</p>
                        <p style="margin:-10px 0px 0px 0px;"></p>
            <p style="font-size:14.0px; color:#E62E00;">Reasons about object detection, segmentation, scene-type and sentence descriptions to improve image parsing.</p>
 <p style="margin:-8px 0px 0px 0px;"></p>
 <a href="papers/fidler_et_al_cvpr13b.pdf" class="buttonTT">Paper</a>&nbsp
 <a class="buttonAA" data-toggle="collapse" data-parent="#FidlerCVPR13abs" href="#FidlerCVPR13abs-list">Abstract</a>&nbsp
<a href="cvpr13_sent_supplemental.pdf" class="buttonMM">Suppl. Mat.</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#FidlerCVPR13" href="#FidlerCVPR13-list">Bibtex</a><br/>
 
<p style="margin:2px 0px 0px 0px;"></p>
<div id="FidlerCVPR13-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
           <p style="font-size:15px;">@inproceedings{FidlerCVPR13,<br/>
    author = {Sanja Fidler and Abhishek Sharma and Raquel Urtasun},<br/>
    title = {A Sentence is Worth a Thousand Pixels},<br/>
    booktitle = {CVPR},<br/>
    year = {2013}<br/>}
</p>
</div>
<div id="FidlerCVPR13abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.5px;">
We are interested in holistic scene understanding where
images are accompanied with text in the form of complex
sentential descriptions. We propose a holistic conditional
random field model for semantic parsing which reasons
jointly about which objects are present in the scene, their
spatial extent as well as semantic segmentation, and employs
text as well as image information as input. We automatically
parse the sentences and extract objects and their
relationships, and incorporate them into the model, both via
potentials as well as by re-ranking candidate detections. We
demonstrate the effectiveness of our approach in the challenging
UIUC sentences dataset and show segmentation improvements
of 12.5% over the visual only model and detection
improvements of 5% AP over deformable part-based
models.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
<!--
<div class="pull-right text-center" style="padding:0.5% 1% 0% 0%; width:11%;">
<a href="papers/fidler_et_al_cvpr13b.pdf" class="buttonT" style="width:100%;">Paper</a><br/>
<p style="margin:1.4px 0px 0px 0px;"></p>
<a href="cvpr13_sent_supplemental.pdf" class="buttonM" style="width:100%;">Suppl. Mat.</a><br/>
<p style="margin:0px 0px 0px 0px;"></p>
<a class="buttonS" data-toggle="collapse" data-parent="#FidlerCVPR13" href="#FidlerCVPR13-list" style="width:100%; margin-top:5px; text-decoration: none; color:#FF9933;">Bibtex</a><br/>
        </div>
        -->
      </li>
    </ul>
							
<p style="margin:-8px 0px 0px 0px;"></p>


   <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.0%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:11px 2% 0 0; width:14.2%;" href="papers/cvpr12seg.pdf"><img width="100%" src="papers/figs/cvpr12holistic.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
            <h4 style="font-size:14.5px; line-height:120%">Describing the Scene as a Whole: Joint Object Detection, Scene Classification and Semantic Segmentation</h4>

	     <p style="margin:-8px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">Jian Yao, <strong>Sanja Fidler</strong>, Raquel Urtasun</p>

		<p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">In <em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>), Providence, USA, June 2012</p>
<p style="margin:-10px 0px 0px 0px;"></p>
            <p style="font-size:14.0px; color:#E62E00;">Code, trained models and annotated bounding boxes for MSRC in Project page</p>
<p style="margin:-8px 0px 0px 0px;"></p>
<a href="papers/cvpr12seg.pdf" class="buttonTT">Paper</a>&nbsp
 <a class="buttonAA" data-toggle="collapse" data-parent="#YaoCVPR12abs" href="#YaoCVPR12abs-list">Abstract</a>&nbsp
<a href="http://ttic.uchicago.edu/~yaojian/HolisticSceneUnderstanding.html" class="buttonPP">Project page.</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#YaoCVPR12" href="#YaoCVPR12-list">Bibtex</a><br/>
 
<p style="margin:2px 0px 0px 0px;"></p>
<div id="YaoCVPR12-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
          <p style="font-size:15px;">@inproceedings{YaoCVPR12,<br/>
    author = {Jian Yao and Sanja Fidler and Raquel Urtasun},<br/>
    title = {Describing the Scene as a Whole: Joint Object Detection, Scene Classification and Semantic Segmentation},<br/>
    booktitle = {CVPR},<br/>
    year = {2012}<br/>}
</p>
</div>
<div id="YaoCVPR12abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.5px;">
In this paper we propose an approach to holistic scene
understanding that reasons jointly about regions, location,
class and spatial extent of objects, presence of a class in the
image, as well as the scene type. Learning and inference in
our model are efficient as we reason at the segment level,
and introduce auxiliary variables that allow us to decompose
the inherent high-order potentials into pairwise potentials
between a few variables with small number of states (at
most the number of classes). Inference is done via a convergent
message-passing algorithm, which, unlike graph-cuts
inference, has no submodularity restrictions and does not
require potential specific moves. We believe this is very important,
as it allows us to encode our ideas and prior knowledge
about the problem without the need to change the inference
engine every time we introduce a new potential. Our
approach outperforms the state-of-the-art on the MSRC-21
benchmark, while being much faster. Importantly, our holistic
model is able to improve performance in all tasks.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
      </li>
    </ul>
<p style="margin:-8px 0px 0px 0px;"></p>


            <a data-toggle="collapse" data-parent="#scene" href="#scene-info">close window</a>
      </div>
      </div>
      
      
      <div id="fashion-info" class="panel-collapse collapse out" style="padding:0% 3% 1% 3%; border-radius:6px; border-style: solid; border-color: #666666;">

<p style="margin:10px 0px 0px 0px;"></p>
<font style="font-size:1.34em; font-weight:500;">Clothing and Fashion</font>
      <div class="media">

<p style="margin:5px 0px 0px 0px;"></p>

<div class="panel" align="center" style="background-color:#C2F0FF; width:94%; margin-left:3%; margin-right:3%; padding:9px 2.5% 0px 2.5%; border-radius:10px;">
<font style="font-size:1em;"><p>"<em>The finest clothing made is a person's skin, but, of course, society demands something more than this.</em>''<p>

<p align="right">- Mark Twain&nbsp&nbsp&nbsp</p></font>
<p style="margin:-5px 0px 0px 0px;"></p>
</div>
      
      <p align="justify">Fashion has a tremendous impact on our society. Clothing typically reflects the
person's social status and thus puts pressure on how to dress to fit a
particular occasion. Its importance becomes even more pronounced  due to online social sites like
Facebook and Instagram where one's photographs are shared with the world. 
We also
live in a technological era where a significant portion of the population
looks for their dream partner on online dating sites. People want to look good;
business or casual, elegant or sporty, sexy but not slutty, and of course trendy,
particularly so when putting their picture online. This is reflected in the
growing  online retail sales, reaching 370 billion dollars in the US by 2017,
and 191 billion euros in Europe according to the Forbes magazine (2013).  </p>

<p align="justify">Our goals include clothing parsing from a photo, as well as to predict how fashionable a person looks on a
particular photograph. Moreover, we aim to give a rich feedback to the user: not only whether the
photograph is appealing or not, but also to make suggestions of what clothing
or even the scenery the user could change in order to improve her/his look. </p>
 
 <p style="margin:30px 0px 0px 0px;"></p>  
      
       <h3> Relevant Publications</h3>

<br/>
<p style="margin:-10px 0px 0px 0px;"></p>      

 <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.0%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:11px 2.2% 0 0; width:14.2%;" href=""><img width="100%" src="papers/figs/cvpr15fashion_small.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:66%;">
            <h4 style="font-size:14.5px; line-height:120%">Neuroaesthetics in Fashion: Modeling the Perception of Beauty</h4>

	     <p style="margin:-8px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">Edgar Simo-Serra, <strong>Sanja Fidler</strong>, Francesc Moreno-Noguer, Raquel Urtasun</p>

	     <p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">In <em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>), Boston, 2015</p>
            <p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px; color:#E62E00;"> How fashionable do you look in a photo? And how can you improve?</p>
            
            <p style="margin:-8px 0px 0px 0px;"></p>
<a href="papers/fashionCVPR15.pdf" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#SimoCVPR15abs" href="#SimoCVPR15abs-list">Abstract</a>&nbsp
<a href="http://hi.cs.waseda.ac.jp/~esimo/en/research/fashionability/" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#SimoCVPR15" href="#SimoCVPR15-list">Bibtex</a><br/>
 
<p style="margin:2px 0px 0px 0px;"></p>
<div id="SimoCVPR15-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{SimoCVPR15,<br/>
    title = {Neuroaesthetics in Fashion: Modeling the Perception of Beauty},<br/>
    author = {Edgar Simo-Serra and Sanja Fidler and Francesc Moreno-Noguer and Raquel Urtasun},<br/>
    booktitle = {CVPR},<br/>
    year = {2015}<br/>}
</p>      
</div>
<div id="SimoCVPR15abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>

            <p style="font-size:14.5px;">
In this paper, we analyze the fashion of clothing of a large social website. Our goal is to learn and predict how fashionable 
a person looks on a photograph and suggest subtle improvements the user could make to improve her/his appeal. We propose a 
Conditional Random Field model that jointly reasons about several fashionability factors such as the type of outfit and garments
 the user is wearing, the type of the user, the photograph's setting (e.g., the scenery behind the user), and the fashionability score. 
Importantly, our model is able to give rich feedback back to the user, conveying which garments or even scenery she/he should change 
in order to improve fashionability. We demonstrate that our joint approach significantly outperforms a variety of intelligent baselines. 
We additionally collected a novel heterogeneous dataset with 144,169 user posts containing diverse image, textual and meta information 
which can be exploited for our task. We also provide a detailed analysis of the data, showing different outfit trends and fashionability 
scores across the globe and across a span of 6 years.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
<div class="pull-right text-center" style="margin-top:3px; margin-right:4px; padding:5px 0.5% 1px 0.5%; width:13.5%; border-radius: 5px; border-style: solid; border-width: 1.5px; border-color: #666666;"><a data-toggle="collapse" data-parent="#SimoCVPR15" href="#Fashion15media"><img width="100%" src="news/torontostar-fashionlife.jpg"></a><br/><font size="2.5">[<a data-toggle="collapse" data-parent="#SimoCVPR15" href="#Fashion15media">MEDIA</a>]</font></div>
</div>
<p style="margin:2px 0px 0px 0px;"></p>
<div id="Fashion15media" class="panel-collapse collapse out" style="padding:0% 3% 1% 3%; border-radius:6px; border-style: solid; border-color: #D119A3;">

<p style="margin:10px 0px 0px 0px;"></p>
      <div class="media">
      <p style="margin:-14px 0px 0px 0px;"></p>
<h3><u>News and Tech websites</u></h3>
</br>
<p style="margin:14px 0px 0px 0px;"></p>
<table class="table table-borderless" style="text-align: center;">
            <tbody>
<tr><td><a href="http://www.newscientist.com/article/dn27729-style-software-gives-fashion-tips-after-judging-what-you-wear.html#.VYCIbEaZIq9"><img style="height:29px;" src="news/ns_logo.jpg"></a></td>
<td><a href="http://qz.com/429399/computer-scientists-developed-an-algorithm-that-can-show-you-how-to-be-fashionable/"><img style="height:24px;" src="news/quartz.jpg"></a></td>
<td><a href="http://www.techtimes.com/articles/62288/20150620/look-better-on-instagram-algorithm.htm"><img src="news/techtimes.png" style="height:29px;"></td>
<td><a href="http://www.wired.co.uk/news/archive/2015-06/23/fashion-solving-algorithm-judges-your-instagram-photos"><img src="news/wired.png" style="height:29px;"></a></td>
<td><a href="http://mashable.com/2015/06/20/algorithm-how-to-take-fashionable-instagrams/"><img src="news/mashable.jpg" style="height:29px;"></a></td>
</tr><tr><td><a href="http://www.newscientist.com/article/dn27729-style-software-gives-fashion-tips-after-judging-what-you-wear.html#.VYCIbEaZIq9">New Scientist</a></td>
<td> <a href="http://qz.com/429399/computer-scientists-developed-an-algorithm-that-can-show-you-how-to-be-fashionable/">Quartz</a></td>
<td><a href="http://www.techtimes.com/articles/62288/20150620/look-better-on-instagram-algorithm.htm">Tech Times</a></td>
<td><a href="http://www.wired.co.uk/news/archive/2015-06/23/fashion-solving-algorithm-judges-your-instagram-photos">Wired</a>, UK</td>
<td><a href="http://mashable.com/2015/06/20/algorithm-how-to-take-fashionable-instagrams/">Mashable</a></td>
</tr><tr>
<td height="34"> </td>
<td> </td>
<td> </td>
<td> </td>
<td </td>
</tr><tr><td><a href="http://on.aol.com/video/new-algorithm-for-instagram-will-help-you-dress-your-best-518908473"><img src="news/aol.png" style="height:29px;"></a></td>
<td><a href="http://www.huffingtonpost.co.uk/2015/06/22/instagram-update-algorithm-fashion-outfit_n_7634344.html?utm_hp_ref=uk&ir=UK"><img src="news/huff.png" style="height:25px;"></a></td>
<td><a href="http://www.huffingtonpost.ca/2015/06/23/new-study-instagram-outfits_n_7648476.html"><img src="news/huffca.png" style="height:25px;"></a></td>
<td><a href="http://www.msn.com/en-ca/news/other/computer-scientists-developed-an-algorithm-that-can-show-you-how-to-be-fashionable/ar-AAbH1Mf"><img src="news/msn.png" style="height:29px;"></a></td>
<td><a href="https://www.prote.in/en/feed/2015/06/this-fashion-bot-knows-what-s-in-vogue"><img src="news/protein.png" style="height:29px;"></a></td>
</tr><tr><td><a href="http://on.aol.com/video/new-algorithm-for-instagram-will-help-you-dress-your-best-518908473">AOL News</a> (video)</td>
<td><a href="http://www.huffingtonpost.co.uk/2015/06/22/instagram-update-algorithm-fashion-outfit_n_7634344.html?utm_hp_ref=uk&ir=UK">Huffington Post</a>, UK (video)</td>
<td><a href="http://www.huffingtonpost.ca/2015/06/23/new-study-instagram-outfits_n_7648476.html">Huffington Post</a>, Canada</td>
<td><a href="http://www.msn.com/en-ca/news/other/computer-scientists-developed-an-algorithm-that-can-show-you-how-to-be-fashionable/ar-AAbH1Mf">MSN</a>, Canada</td>
<td><a href="https://www.prote.in/en/feed/2015/06/this-fashion-bot-knows-what-s-in-vogue">Protein</a></td>
</tr><tr>
<td height="34"> </td>
<td> </td>
<td> </td>
<td> </td>
<td </td>
</tr><tr><td><a href="https://ca.news.yahoo.com/blogs/dailybrew/the-algorithm-that-judges-your-fashion-sense-will-soon-be-an-app-195234022.html"><img src="news/yahooca.jpg" style="height:29px;"></a></td>
<td><a href="http://www.sciencedaily.com/releases/2015/07/150714083033.htm"><img src="news/sciencedaily.png" style="height:25px;"></a></td>
<td><a href="http://www.dailymail.co.uk/sciencetech/article-3163124/Would-fashion-advice-robot-Researchers-develop-mathematical-model-help-dressed.html" style="height:29px;"><img src="news/dailymail.jpg" style="height:25px;"></a></td>
<td><a href="http://www.psfk.com/2015/07/fashion-advice-algorithm-computer-vision-foundation-university-of-toronto.html"><img src="news/psfk.jpg" style="height:29px;"></a></td>
<td><a href="http://www.thestar.com/life/fashion_style/2015/07/20/u-of-t-scientists-create-software-to-analyze-outfits.html"><img src="news/torontostar.jpg" style="height:29px;"></a></td>

</tr><tr><td><a href="https://ca.news.yahoo.com/blogs/dailybrew/the-algorithm-that-judges-your-fashion-sense-will-soon-be-an-app-195234022.html">Yahoo</a>, Canada</td>
<td><a href="http://www.sciencedaily.com/releases/2015/07/150714083033.htm">Science Daily</a></td>
<td><a href="http://www.dailymail.co.uk/sciencetech/article-3163124/Would-fashion-advice-robot-Researchers-develop-mathematical-model-help-dressed.html">Daily Mail</a>,  UK</td>
<td><a href="http://www.psfk.com/2015/07/fashion-advice-algorithm-computer-vision-foundation-university-of-toronto.html">PSFK</a></td>
<td><a href="http://www.thestar.com/life/fashion_style/2015/07/20/u-of-t-scientists-create-software-to-analyze-outfits.html">Toronto Star</a></td>
</tr><tr>
<td height="34"> </td>
<td> </td>
<td> </td>
<td> </td>
<td </td>
</tr><tr><td><a href="http://www.gizmag.com/computer-algorithm-fashion-advice/38470/"><img src="news/gizmag.jpg" style="height:29px;"></a></td>
<td><a href="http://www.therecord.com/living-story/5742841-can-a-software-program-make-you-stylish-/"><img src="news/therecord.png" style="height:29px;"></a></td>
<td><a href="http://www.idigitaltimes.com/researchers-create-robot-can-give-fashion-advice-458744" style="height:29px;"><img src="news/idigital.jpg" style="height:29px;"></a></td>
<td></td>
<td></td>

</tr><tr><td><a href="http://www.gizmag.com/computer-algorithm-fashion-advice/38470/">Gizmag</a></td>
<td><a href="http://www.therecord.com/living-story/5742841-can-a-software-program-make-you-stylish-/">TheRecord.com</a></td>
<td><a href="http://www.idigitaltimes.com/researchers-create-robot-can-give-fashion-advice-458744">iDigitalTimes</a></td>
<td></td>
<td></td>
</tr></tbody></table>
</div>
<hr>
</br>
<p style="margin:-20px 0px 0px 0px;"></p>

      <div class="media">
      <p style="margin:-14px 0px 0px 0px;"></p>
<h3><u>Fashion magazines (online)</u></h3>
</br>
<p style="margin:14px 0px 0px 0px;"></p>
<table class="table table-borderless" style="text-align: center;">
            <tbody>
<tr><td>
<a href="http://www.harpersbazaar.com/fashion/trends/a11271/fashion-algorithm-suggests-outfits-for-better-instagram-photos/"><img src="news/bazaar.png" style="height:29px;"></a></td>
<td><a href="http://www.glamour.com/fashion/blogs/dressed/2015/06/outfit-making-computer-algorithm"><img src="news/glamour.png" style="height:29px;"></a></td>
<td><a href="http://www.elle.com/fashion/personal-style/a28974/fashion-algorithm-suggests-outfits-for-better-instagram-photos/"><img src="news/elle.jpg" style="height:29px;"></a></td>
<td><a href="http://www.cosmopolitan.co.uk/fashion/style/news/a36721/algorithm-solves-fashion-problems/"><img src="news/cosmopolitan.jpg" style="height:29px;"></a></td>
<td><a href="http://www.marieclaire.com/fashion/street-style/a14806/fashion-algorithm-suggests-outfits-for-better-instagram-photos/"><img src="news/marieclaire.jpg" style="height:26px;"></a></td>
</tr><tr><td><a href="http://www.harpersbazaar.com/fashion/trends/a11271/fashion-algorithm-suggests-outfits-for-better-instagram-photos/">Harper's Bazaar</a></td>
<td><a href="http://www.glamour.com/fashion/blogs/dressed/2015/06/outfit-making-computer-algorithm">Glamour</a></td>
<td><a href="http://www.elle.com/fashion/personal-style/a28974/fashion-algorithm-suggests-outfits-for-better-instagram-photos/">Elle</a></td>
<td><a href="http://www.cosmopolitan.co.uk/fashion/style/news/a36721/algorithm-solves-fashion-problems/">Cosmopolitan</a>, UK</td>
<td><a href="http://www.marieclaire.com/fashion/street-style/a14806/fashion-algorithm-suggests-outfits-for-better-instagram-photos/">Marie Claire</a></td>
</tr><tr>
<td height="34"> </td>
<td> </td>
<td> </td>
<td> </td>
<td </td>
</tr><tr><td><a href="http://www.fashionmagazine.com/fashion/2015/06/24/best-instagram-items/"><img src="news/fashion.png" style="height:29px;"></a></td></td>
<td><a href="https://www.yahoo.com/style/thanks-to-science-ootd-posts-will-never-be-the-122177373293.html"><img src="news/yahoostyle.png" style="height:30px;"></a></td>
<td><a href="http://www.redonline.co.uk/fashion/fashion_news/instagram-algorithm-to-dress-better11"><img src="news/red.png" style="height:29px;"></a></td>
<td><a href="https://www.the-pool.com/news-views/fashion-news/2015/24/can-the-internet-help-you-get-dressed-"><img src="news/pool.png" style="height:26px;"></a></td>
<td><a href="http://www.fashionotes.com/content/2015/06/instagram-wants-to-help-you-dress-better/"><img src="news/fn.png" style="height:29px;"></a></td>
</tr><tr><td><a href="http://www.fashionmagazine.com/fashion/2015/06/24/best-instagram-items/">Fashion Magazine</a></td>
<td><a href="https://www.yahoo.com/style/thanks-to-science-ootd-posts-will-never-be-the-122177373293.html">Yahoo style</a></td>
<td><a href="http://www.redonline.co.uk/fashion/fashion_news/instagram-algorithm-to-dress-better11">Red Magazine</a>, UK</td>
<td><a href="https://www.the-pool.com/news-views/fashion-news/2015/24/can-the-internet-help-you-get-dressed-">The Pool</a>, UK</td>
<td><a href="http://www.fashionotes.com/content/2015/06/instagram-wants-to-help-you-dress-better/">FashionNotes</a></td>
</tr></tbody></table>
</div>
<hr>
</br>
<p style="margin:-20px 0px 0px 0px;"></p>

      <div class="media">
      <p style="margin:-14px 0px 0px 0px;"></p>
<h3><u>International news</u></h3>
</br>
<p style="margin:14px 0px 0px 0px;"></p>
<table class="table table-borderless" style="text-align: center;">
            <tbody>
<tr><td>
<a href="http://www.vogue.es/moda/news/articulos/programa-ordenador-calcula-nivel-estilo/22801"><img src="news/voguespain.png" style="height:29px;"></a></td>
<td><a href="http://www.stylebook.de/fashion/Outfit-Selfies-auf-Instagram-neuer-Algorithmus-zieht-uns-magisch-an-659850.html"><img src="news/stylebook.png" style="height:25px;"></a></td>
<td><a href="http://www.ansa.it/sito/notizie/tecnologia/software_app/2015/06/23/algoritmo-suggerira-outfit-piu-di-moda_7a973ee6-81c3-44e3-9c7a-55338107c627.html"><img src="news/ansa.png" style="height:29px;"></a></td>
<td><a href="http://www.cenariomt.com.br/noticia/455713/as-melhores-roupas-e-acessorios-para-ficar-incrivel-no-instagram-segundo-a-ciencia.html"><img src="news/cenario.png" style="height:29px;"></a></td>
<td><a href="http://amsterdam-ftv-blog.com/archives/37103"><img src="news/ams1.png" style="height:31px;"></a></td>
</tr><tr><td><a href="http://www.vogue.es/moda/news/articulos/programa-ordenador-calcula-nivel-estilo/22801">Vogue</a> (Spain)</td>
<td><a href="http://www.stylebook.de/fashion/Outfit-Selfies-auf-Instagram-neuer-Algorithmus-zieht-uns-magisch-an-659850.html">Stylebook</a> (Germany)</td>
<td><a href="http://www.ansa.it/sito/notizie/tecnologia/software_app/2015/06/23/algoritmo-suggerira-outfit-piu-di-moda_7a973ee6-81c3-44e3-9c7a-55338107c627.html">Ansa</a> (Italy)</td>
<td><a href="http://www.cenariomt.com.br/noticia/455713/as-melhores-roupas-e-acessorios-para-ficar-incrivel-no-instagram-segundo-a-ciencia.html">CenarioMT</a>  (Brazil)</td>
<td><a href="http://amsterdam-ftv-blog.com/archives/37103">Amsterdam Fashion</a> (NL)</td>
</tr><tr>
<td height="34"> </td>
<td> </td>
<td> </td>
<td> </td>
<td </td>
</tr><tr><td><a href="http://www.marieclaire.fr/,on-a-trouve-le-secret-pour-etre-mieux-habillees-que-vos-amies,738591.asp"><img src="news/marieclaire.jpg" style="height:29px;"></a></td></td>
<td><a href="http://fashionpoliceng.com/there-is-a-new-algorithm-for-instagram-to-help-you-imrove-your-dress-sense/"><img src="news/fpn.png" style="height:29px;"></a></td>
<td><a href="http://naukawpolsce.pap.pl/aktualnosci/news,405480,komputer-zamiast-stylisty.html"><img src="news/nauka.png" style="height:29px;"></a></td>
<td><a href="http://www.pluska.sk/spravy/zo-zahranicia/instagram-vie-co-si-mate-obliect-moda-podla-matematiky.html"><img src="news/pluska.png" style="height:29px;"></a></td></td>
<td><a href="http://www.pressetext.com/news/20150701001"><img src="news/pressetext.png" style="height:29px;"></a></td>
</tr><tr><td><a href="http://www.marieclaire.fr/,on-a-trouve-le-secret-pour-etre-mieux-habillees-que-vos-amies,738591.asp">Marie Claire</a> (France)</td>
<td><a href="http://fashionpoliceng.com/there-is-a-new-algorithm-for-instagram-to-help-you-imrove-your-dress-sense/">Fashion Police</a> (Nigeria)</td>
<td><a href="http://naukawpolsce.pap.pl/aktualnosci/news,405480,komputer-zamiast-stylisty.html">Nauka</a> (Poland)</td>
<td><a href="http://www.pluska.sk/spravy/zo-zahranicia/instagram-vie-co-si-mate-obliect-moda-podla-matematiky.html">Pluska</a> (Slovakia)</td>
<td><a href="http://www.pressetext.com/news/20150701001">Pressetext</a> (Austria)</td>
</tr><tr>
<td height="34"> </td>
<td> </td>
<td> </td>
<td> </td>
<td </td>
</tr><tr><td><a href="https://www.wired.de/collection/latest/diese-software-kennst-sich-aus-sachen-fashion"><img src="news/wired_de.png" style="height:29px;"></a></td>
<td><a href="http://jetzt.sueddeutsche.de/texte/anzeigen/593320/Was-zieh-ich-an"><img src="news/jetzt.png" style="height:29px;"></a></td>
<td><a href="http://www.gazzetta.it/Sportlife/Moda/01-07-2015/dammi-foto-ti-diro-come-vestirti-l-algoritmo-look-che-piace-120388048293.shtml?refresh_ce-cp"><img src="news/gazzetta.png" style="height:29px;"></a></td>
<td><a href="http://www.popsugar.com.au/fashion/Best-Outfits-Wear-Instagram-37781122"><img src="news/popsugar.png" style="height:23px;"></a></td>
<td><a href="http://www.sinembargo.mx/19-06-2015/1384780"><img src="news/sinembargo.png" style="height:26px;"></a></td>
</tr><tr>
<td><a href="https://www.wired.de/collection/latest/diese-software-kennst-sich-aus-sachen-fashion">Wired</a> (Germany)</td>
<td><a href="http://jetzt.sueddeutsche.de/texte/anzeigen/593320/Was-zieh-ich-an">Jetzt</a> (Germany)</td>
<td><a href="http://www.gazzetta.it/Sportlife/Moda/01-07-2015/dammi-foto-ti-diro-come-vestirti-l-algoritmo-look-che-piace-120388048293.shtml?refresh_ce-cp">La Gazzetta</a> (Italy)</td>
<td><a href="http://www.popsugar.com.au/fashion/Best-Outfits-Wear-Instagram-37781122">PopSugar</a> (Australia)</td>
<td><a href="http://www.sinembargo.mx/19-06-2015/1384780">SinEmbargo</a> (Mexico)</td>
</tr><tr>
</tr></tbody></table>
</div>
<hr>
<p style="margin:25px 0px 0px 0px;"></p>
<p><b>A more complete list is maintained on our <a href="http://www.iri.upc.edu/people/esimo/en/research/fashionability/">project webpage</a>.</b></p>
<a data-toggle="collapse" data-parent="#SimoCVPR15" href="#Fashion15media">close window</a>
</div>
      </li>
    </ul>

<p style="margin:-8px 0px 0px 0px;"></p>


    <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.0%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:10px 3.2% 0 0; width:14.2%;" href="papers/simo_et_al_accv14.pdf"><img width="100%" src="papers/figs/accv14clothing.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
            <h4 style="font-size:14.5px; line-height:120%">A High Performance CRF Model for Clothes Parsing</h4>

	     <p style="margin:-8px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">Edgar Simo-Serra, <strong>Sanja Fidler</strong>, Francesc Moreno-Noguer, Raquel Urtasun</p>

	     <p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">In <em>Asian Conference on Computer Vision</em> (<strong>ACCV</strong>), Singapore, November, 2014</p>
<p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px; color:#E62E00;">Significant performance gain over state-of-the-art. Code, features & models in Project page!</p>
<p style="margin:-8px 0px 0px 0px;"></p>

<a href="papers/simo_et_al_accv14.pdf" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#EdgarACCV14abs" href="#EdgarACCV14abs-list">Abstract</a>&nbsp
<a href="http://www.iri.upc.edu/people/esimo/research/fashion/" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#EdgarACCV14" href="#EdgarACCV14-list">Bibtex</a><br/>
 
<p style="margin:2px 0px 0px 0px;"></p>
<div id="EdgarACCV14-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
<p style="line-height:8px"> &nbsp</p>
            <p style="font-size:15px;">@inproceedings{SimoACCV14,<br/>
    title = {A High Performance CRF Model for Clothes Parsing},<br/>
    author = {Edgar Simo-Serra and Sanja Fidler and Francesc Moreno-Noguer and Raquel Urtasun},<br/>
    booktitle = {ACCV},<br/>
    year = {2014}}
</p>       
</div>
<div id="EdgarACCV14abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.5px;">
In this paper we tackle the problem of clothing parsing: Our goal is to
segment and classify different garments a person is wearing. We frame the problem
as the one of inference in a pose-aware Conditional Random Field (CRF)
which exploits appearance, figure/ground segmentation, shape and location priors
for each garment as well as similarities between segments, and symmetries
between different human body parts. We demonstrate the effectiveness of our approach
on the Fashionista dataset [Yamaguchi et al., CVPR'12] and show that we can obtain a significant
improvement over the state-of-the-art.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
      </li>
    </ul>
    
                  <a data-toggle="collapse" data-parent="#fashion" href="#fashion-info">close window</a>
      </div>
      </div>
 
 
 <!-- IMAGES AND TEXT-->
       <div id="imagetext-info" class="panel-collapse collapse out" style="padding:0% 3% 1% 3%; border-radius:6px; border-style: solid; border-color: #666666;">

<p style="margin:10px 0px 0px 0px;"></p>
<font style="font-size:1.34em; font-weight:500;">Vision and Language</font>
      <div class="media">

      
      <p align="justify">A successful robotic platform needs to be able to understand both the visual world and the
human's (lingual) instructions and communicate its understanding back to the user in a
natural way. One of my main scientific interests lies in the integration of vision and language in
order to develop high-performance autonomous systems that can interact with the humans
through language. Such solutions are of particular importance for the blind or visually impaired
where language is one of the only means of human-robot interaction.
      </p>
 
 <p style="margin:30px 0px 0px 0px;"></p>  
      
       <h3> Relevant Publications</h3>

<br/>
 <p style="margin:-10px 0px 0px 0px;"></p>


<font style="font-size:1.11em; font-weight:500;">Representation of Text</font>
<br/>
<br/>
 <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.4%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:11px 2% 0 0; width:14%;" href=""><img width="100%" src="papers/figs/skipthoughts.png"><br/><p style="margin:12px 0px 0px 0px;"></p><img width="100%" src="papers/figs/skipthoughts_res.jpg"></div>          
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:70%;">
            <h4 style="font-size:14.5px; line-height:120%">Skip-Thought Vectors</h4>

	     <p style="margin:-8px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">Ryan Kiros, Yukun Zhu, Ruslan Salakhutdinov, Richard Zemel, Antonio Torralba, Raquel Urtasun, <strong>Sanja Fidler</strong></p>

	     <p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">arXiv preprint <em>arXiv:1506.06726</em>, 2015</p>
<p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px; color:#E62E00;"> Sent2vec neural representation trained on 11K books</p>

<p style="margin:-8px 0px 0px 0px;"></p>
            
            <a href="http://arxiv.org/abs/1506.06726" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#SkipThoughtsabs" href="#SkipThoughtsabs-list">Abstract</a>&nbsp
<a href="https://github.com/ryankiros/skip-thoughts" class="buttonPP">Code</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#SkipThoughts" href="#SkipThoughts-list">Bibtex</a><br/>

 
<p style="margin:2px 0px 0px 0px;"></p>
<div id="SkipThoughts-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{moviebook,<br/>
    title = {Skip-Thought Vectors},<br/>
    author = {Ryan Kiros and Yukun Zhu and Ruslan Salakhutdinov and Richard Zemel and Antonio Torralba and Raquel Urtasun and Sanja Fidler},<br/>
    booktitle = {arXiv preprint arXiv:1506.06726},<br/>
    year = {2015}</br>}
</p>      
</div>
<div id="SkipThoughtsabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.5px;">
coming soon 
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>  
</div>
                              <div class="pull-right text-center" style="padding:4px 0.5% 0 0; width:6.5%;" href=""><img width="100%" src="papers/figs/new_burst.jpg"></div> 
                              </div>
      </li>
    </ul>
<br/>
 <p style="margin:-10px 0px 0px 0px;"></p>


<font style="font-size:1.11em; font-weight:500;">Vision-Text Alignment and Retrieval</font>
<br/>
<br/>
<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.4%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:11px 1.5% 0 0; width:14%;" href=""><img width="100%" src="papers/figs/moviebook.jpg"><br/><p style="margin:4px 0px 0px 0px;"></p><img width="100%" src="papers/figs/harrypotter_small.png"></div>            
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:70%;">
            <h4 style="font-size:14.5px; line-height:120%">Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books</h4>

	     <p style="margin:-8px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">Yukun Zhu, Ryan Kiros, Richard Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, <strong>Sanja Fidler</strong></p>

	     <p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">arXiv preprint <em>arXiv:1506.06724</em>, 2015</p>
<p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px; color:#E62E00;"> Aligning movies and books for story-like captioning</p>

<p style="margin:-8px 0px 0px 0px;"></p>
            
            <a href="http://arxiv.org/abs/1506.06724" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#MovieBookabs" href="#MovieBookabs-list">Abstract</a>&nbsp
<a href="http://www.cs.toronto.edu/~mbweb/" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#MovieBook" href="#MovieBook-list">Bibtex</a><br/>

 
<p style="margin:2px 0px 0px 0px;"></p>
<div id="MovieBook-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{moviebook,<br/>
    title = {Aligning Books and Movies: Towards Story-like Visual Explanations by Watching Movies and Reading Books},<br/>
    author = {Yukun Zhu and Ryan Kiros and Richard Zemel and Ruslan Salakhutdinov and Raquel Urtasun and Antonio Torralba and Sanja Fidler},<br/>
    booktitle = {arXiv preprint arXiv:1506.06724},<br/>
    year = {2015}</br>}
</p>      
</div>
<div id="MovieBookabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.5px;">
coming soon 
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div> 
</div>
<div class="pull-right text-center" style="padding:4px 0.5% 0 0; width:6.5%;" href=""><img width="100%" src="papers/figs/new_burst.jpg"></div> 
      </li>
    </ul>
<p style="margin:-8px 0px 0px 0px;"></p>


 <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.0%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:10px 2.7% 0 0; width:14.2%;" href="papers/kong_et_al_cvpr14.pdf"><img src="papers/figs/cvpr14coref.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
            <h4 style="font-size:14.5px; line-height:120%">What are you talking about? Text-to-Image Coreference</h4>

	     <p style="margin:-8px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">Chen Kong, Dahua Lin, Mohit Bansal, Raquel Urtasun, <strong>Sanja Fidler</strong></p>

	     <p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">In <em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>), Columbus, USA, June, 2014</p>
<p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px; color:#E62E00;">Exploits text for visual parsing and aligns nouns to objects. Code and data out soon!</p>
            <p style="margin:-8px 0px 0px 0px;"></p>
            <a href="papers/kong_et_al_cvpr14.pdf" class="buttonTT">Paper</a>&nbsp
             <a class="buttonAA" data-toggle="collapse" data-parent="#KongCVPR14abs" href="#KongCVPR14abs-list">Abstract</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#KongCVPR14" href="#KongCVPR14-list">Bibtex</a><br/>
 
<p style="margin:2px 0px 0px 0px;"></p>
<div id="KongCVPR14-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{KongCVPR14,<br/>
    title = {What are you talking about? Text-to-Image Coreference},<br/>
    author = {Chen Kong and Dahua Lin and Mohit Bansal and Raquel Urtasun and Sanja Fidler},<br/>
    booktitle = {CVPR},<br/>
    year = {2014}<br/>}
</p>  
</div>
<div id="KongCVPR14abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.5px;">
In this paper we exploit natural sentential descriptions
of RGB-D scenes in order to improve 3D semantic parsing.
Importantly, in doing so, we reason about which particular
object each noun/pronoun is referring to in the image. This
allows us to utilize visual information in order to disambiguate
the so-called coreference resolution problem that
arises in text. Towards this goal, we propose a structure
prediction model that exploits potentials computed from text
and RGB-D imagery to reason about the class of the 3D objects,
the scene type, as well as to align the nouns/pronouns
with the referred visual objects. We demonstrate the effectiveness
of our approach on the challenging NYU-RGBD v2
dataset, which we enrich with natural lingual descriptions.
We show that our approach significantly improves 3D detection
and scene classification accuracy, and is able to reliably
estimate the text-to-image alignment. Furthermore,
by using textual and visual information, we are also able to
successfully deal with coreference in text, improving upon
the state-of-the-art Stanford coreference system.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
<!--
<div class="pull-right text-center" style="padding:0.5% 1% 0% 0%; width:11%;">
<a href="papers/kong_et_al_cvpr14.pdf" class="buttonT" style="width:100%;">Paper</a><br/>
<p style="margin:0px 0px 0px 0px;"></p>
<a class="buttonS" data-toggle="collapse" data-parent="#KongCVPR14" href="#KongCVPR14-list" style="width:100%; margin-top:6px; text-decoration: none; color:#FF9933;">Bibtex</a><br/>
        </div>
        -->
      </li>
    </ul>
<p style="margin:-8px 0px 0px 0px;"></p>

 <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.0%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:12px 1.7% 0 0; width:14.2%;" href="papers/lin_et_al_cvpr14.pdf"><img width="100%" src="papers/figs/cvpr14vsearch.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
            <h4 style="font-size:14.5px; line-height:120%">Visual Semantic Search: Retrieving Videos via Complex Textual Queries</h4>

	     <p style="margin:-8px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">Dahua Lin, <strong>Sanja Fidler</strong>, Chen Kong, Raquel Urtasun</p>

	    <p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">In <em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>), Columbus, USA, June, 2014</p>
 
<p style="margin:-10px 0px 0px 0px;"></p>
            <p style="font-size:14.0px; color:#E62E00;">Video retrieval when a query is a longer sentence or a multi-sentence description</p>
<p style="margin:-8px 0px 0px 0px;"></p>
<a href="papers/lin_et_al_cvpr14.pdf" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#LinCVPR14abs" href="#LinCVPR14abs-list">Abstract</a>&nbsp
<a href="papers/cvpr14_retrieval_supplemental.pdf" class="buttonMM" >Suppl. Mat.</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#LinCVPR14" href="#LinCVPR14-list">Bibtex</a><br/>

<p style="margin:2px 0px 0px 0px;"></p>
<div id="LinCVPR14-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{LinCVPR14,<br/>
    author = {Dahua Lin and  Sanja Fidler and Chen Kong and Raquel Urtasun},<br/>
    title = {Visual Semantic Search: Retrieving Videos via Complex Textual Queries},<br/>
    booktitle = {CVPR},<br/>
    year = {2014}<br/>}</p>
</div>
<div id="LinCVPR14abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.5px;">
In this paper, we tackle the problem of retrieving videos
using complex natural language queries. Towards this goal,
we first parse the sentential descriptions into a semantic
graph, which is then matched to visual concepts using a
generalized bipartite matching algorithm. Our approach
exploits object appearance, motion and spatial relations,
and learns the importance of each term using structure prediction.
We demonstrate the effectiveness of our approach
on a new dataset designed for semantic search in the context
of autonomous driving, which exhibits complex and highly
dynamic scenes with many objects. We show that our approach
is able to locate a major portion of the objects described
in the query with high accuracy, and improve the
relevance in video retrieval.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
      </li>
    </ul>
    <br/>
 <p style="margin:-10px 0px 0px 0px;"></p>


<font style="font-size:1.11em; font-weight:500;">Exploiting Text for Image Parsing</font>
<br/>
<br/>
    <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.0%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:11px 2.2% 0 0; width:14.2%;" href=""><img width="100%" src="papers/figs/cvpr15fashion_small.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:66%;">
            <h4 style="font-size:14.5px; line-height:120%">Neuroaesthetics in Fashion: Modeling the Perception of Beauty</h4>

	     <p style="margin:-8px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">Edgar Simo-Serra, <strong>Sanja Fidler</strong>, Francesc Moreno-Noguer, Raquel Urtasun</p>

	     <p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">In <em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>), Boston, 2015</p>
            <p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px; color:#E62E00;"> How fashionable do you look in a photo? And how can you improve? This paper exploits image information as well as tags and user's comments to predict fashionability.</p>
            
            <p style="margin:-8px 0px 0px 0px;"></p>
<a href="papers/fashionCVPR15.pdf" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#SimoCVPR15abs" href="#SimoCVPR15abs-list">Abstract</a>&nbsp
<a href="http://www.iri.upc.edu/people/esimo/research/fashionability/" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#SimoCVPR15" href="#SimoCVPR15-list">Bibtex</a><br/>
 
<p style="margin:2px 0px 0px 0px;"></p>
<div id="SimoCVPR15-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{SimoCVPR15,<br/>
    title = {Neuroaesthetics in Fashion: Modeling the Perception of Beauty},<br/>
    author = {Edgar Simo-Serra and Sanja Fidler and Francesc Moreno-Noguer and Raquel Urtasun},<br/>
    booktitle = {CVPR},<br/>
    year = {2015}<br/>}
</p>      
</div>
<div id="SimoCVPR15abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.5px;">
In this paper, we analyze the fashion of clothing of a large social website. Our goal is to learn and predict how fashionable 
a person looks on a photograph and suggest subtle improvements the user could make to improve her/his appeal. We propose a 
Conditional Random Field model that jointly reasons about several fashionability factors such as the type of outfit and garments
 the user is wearing, the type of the user, the photograph's setting (e.g., the scenery behind the user), and the fashionability score. 
Importantly, our model is able to give rich feedback back to the user, conveying which garments or even scenery she/he should change 
in order to improve fashionability. We demonstrate that our joint approach significantly outperforms a variety of intelligent baselines. 
We additionally collected a novel heterogeneous dataset with 144,169 user posts containing diverse image, textual and meta information 
which can be exploited for our task. We also provide a detailed analysis of the data, showing different outfit trends and fashionability 
scores across the globe and across a span of 6 years.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
<div class="pull-right text-center" style="margin-top:3px; margin-right:4px; padding:5px 0.5% 1px 0.5%; width:13.5%; border-radius: 5px; border-style: solid; border-width: 1.5px; border-color: #666666;"><a data-toggle="collapse" data-parent="#SimoCVPR15" href="#Fashion15media"><img width="100%" src="news/torontostar-fashionlife.jpg"></a><br/><font size="2.5">[<a data-toggle="collapse" data-parent="#SimoCVPR15" href="#Fashion15media">MEDIA</a>]</font></div>
</div>
<p style="margin:2px 0px 0px 0px;"></p>
<div id="Fashion15media" class="panel-collapse collapse out" style="padding:0% 3% 1% 3%; border-radius:6px; border-style: solid; border-color: #666666;">

<p style="margin:10px 0px 0px 0px;"></p>
      <div class="media">
      <p style="margin:-14px 0px 0px 0px;"></p>
<h3><u>News and Tech websites</u></h3>
</br>
<p style="margin:14px 0px 0px 0px;"></p>
<table class="table table-borderless" style="text-align: center;">
            <tbody>
<tr><td><a href="http://www.newscientist.com/article/dn27729-style-software-gives-fashion-tips-after-judging-what-you-wear.html#.VYCIbEaZIq9"><img style="height:29px;" src="news/ns_logo.jpg"></a></td>
<td><a href="http://qz.com/429399/computer-scientists-developed-an-algorithm-that-can-show-you-how-to-be-fashionable/"><img style="height:24px;" src="news/quartz.jpg"></a></td>
<td><a href="http://www.techtimes.com/articles/62288/20150620/look-better-on-instagram-algorithm.htm"><img src="news/techtimes.png" style="height:29px;"></td>
<td><a href="http://www.wired.co.uk/news/archive/2015-06/23/fashion-solving-algorithm-judges-your-instagram-photos"><img src="news/wired.png" style="height:29px;"></a></td>
<td><a href="http://mashable.com/2015/06/20/algorithm-how-to-take-fashionable-instagrams/"><img src="news/mashable.jpg" style="height:29px;"></a></td>
</tr><tr><td><a href="http://www.newscientist.com/article/dn27729-style-software-gives-fashion-tips-after-judging-what-you-wear.html#.VYCIbEaZIq9">New Scientist</a></td>
<td> <a href="http://qz.com/429399/computer-scientists-developed-an-algorithm-that-can-show-you-how-to-be-fashionable/">Quartz</a></td>
<td><a href="http://www.techtimes.com/articles/62288/20150620/look-better-on-instagram-algorithm.htm">Tech Times</a></td>
<td><a href="http://www.wired.co.uk/news/archive/2015-06/23/fashion-solving-algorithm-judges-your-instagram-photos">Wired</a>, UK</td>
<td><a href="http://mashable.com/2015/06/20/algorithm-how-to-take-fashionable-instagrams/">Mashable</a></td>
</tr><tr>
<td height="34"> </td>
<td> </td>
<td> </td>
<td> </td>
<td </td>
</tr><tr><td><a href="http://on.aol.com/video/new-algorithm-for-instagram-will-help-you-dress-your-best-518908473"><img src="news/aol.png" style="height:29px;"></a></td>
<td><a href="http://www.huffingtonpost.co.uk/2015/06/22/instagram-update-algorithm-fashion-outfit_n_7634344.html?utm_hp_ref=uk&ir=UK"><img src="news/huff.png" style="height:25px;"></a></td>
<td><a href="http://www.huffingtonpost.ca/2015/06/23/new-study-instagram-outfits_n_7648476.html"><img src="news/huffca.png" style="height:25px;"></a></td>
<td><a href="http://www.msn.com/en-ca/news/other/computer-scientists-developed-an-algorithm-that-can-show-you-how-to-be-fashionable/ar-AAbH1Mf"><img src="news/msn.png" style="height:29px;"></a></td>
<td><a href="https://www.prote.in/en/feed/2015/06/this-fashion-bot-knows-what-s-in-vogue"><img src="news/protein.png" style="height:29px;"></a></td>
</tr><tr><td><a href="http://on.aol.com/video/new-algorithm-for-instagram-will-help-you-dress-your-best-518908473">AOL News</a> (video)</td>
<td><a href="http://www.huffingtonpost.co.uk/2015/06/22/instagram-update-algorithm-fashion-outfit_n_7634344.html?utm_hp_ref=uk&ir=UK">Huffington Post</a>, UK (video)</td>
<td><a href="http://www.huffingtonpost.ca/2015/06/23/new-study-instagram-outfits_n_7648476.html">Huffington Post</a>, Canada</td>
<td><a href="http://www.msn.com/en-ca/news/other/computer-scientists-developed-an-algorithm-that-can-show-you-how-to-be-fashionable/ar-AAbH1Mf">MSN</a>, Canada</td>
<td><a href="https://www.prote.in/en/feed/2015/06/this-fashion-bot-knows-what-s-in-vogue">Protein</a></td>
</tr><tr>
<td height="34"> </td>
<td> </td>
<td> </td>
<td> </td>
<td </td>
</tr><tr><td><a href="https://ca.news.yahoo.com/blogs/dailybrew/the-algorithm-that-judges-your-fashion-sense-will-soon-be-an-app-195234022.html"><img src="news/yahooca.jpg" style="height:29px;"></a></td>
<td><a href="http://www.sciencedaily.com/releases/2015/07/150714083033.htm"><img src="news/sciencedaily.png" style="height:25px;"></a></td>
<td><a href="http://www.dailymail.co.uk/sciencetech/article-3163124/Would-fashion-advice-robot-Researchers-develop-mathematical-model-help-dressed.html" style="height:29px;"><img src="news/dailymail.jpg" style="height:25px;"></a></td>
<td><a href="http://www.psfk.com/2015/07/fashion-advice-algorithm-computer-vision-foundation-university-of-toronto.html"><img src="news/psfk.jpg" style="height:29px;"></a></td>
<td><a href="http://www.thestar.com/life/fashion_style/2015/07/20/u-of-t-scientists-create-software-to-analyze-outfits.html"><img src="news/torontostar.jpg" style="height:29px;"></a></td>

</tr><tr><td><a href="https://ca.news.yahoo.com/blogs/dailybrew/the-algorithm-that-judges-your-fashion-sense-will-soon-be-an-app-195234022.html">Yahoo</a>, Canada</td>
<td><a href="http://www.sciencedaily.com/releases/2015/07/150714083033.htm">Science Daily</a></td>
<td><a href="http://www.dailymail.co.uk/sciencetech/article-3163124/Would-fashion-advice-robot-Researchers-develop-mathematical-model-help-dressed.html">Daily Mail</a>,  UK</td>
<td><a href="http://www.psfk.com/2015/07/fashion-advice-algorithm-computer-vision-foundation-university-of-toronto.html">PSFK</a></td>
<td><a href="http://www.thestar.com/life/fashion_style/2015/07/20/u-of-t-scientists-create-software-to-analyze-outfits.html">Toronto Star</a></td>
</tr><tr>
<td height="34"> </td>
<td> </td>
<td> </td>
<td> </td>
<td </td>
</tr><tr><td><a href="http://www.gizmag.com/computer-algorithm-fashion-advice/38470/"><img src="news/gizmag.jpg" style="height:29px;"></a></td>
<td><a href="http://www.therecord.com/living-story/5742841-can-a-software-program-make-you-stylish-/"><img src="news/therecord.png" style="height:29px;"></a></td>
<td><a href="http://www.idigitaltimes.com/researchers-create-robot-can-give-fashion-advice-458744" style="height:29px;"><img src="news/idigital.jpg" style="height:29px;"></a></td>
<td></td>
<td></td>

</tr><tr><td><a href="http://www.gizmag.com/computer-algorithm-fashion-advice/38470/">Gizmag</a></td>
<td><a href="http://www.therecord.com/living-story/5742841-can-a-software-program-make-you-stylish-/">TheRecord.com</a></td>
<td><a href="http://www.idigitaltimes.com/researchers-create-robot-can-give-fashion-advice-458744">iDigitalTimes</a></td>
<td></td>
<td></td>
</tr></tbody></table>
</div>
<hr>
</br>
<p style="margin:-20px 0px 0px 0px;"></p>

      <div class="media">
      <p style="margin:-14px 0px 0px 0px;"></p>
<h3><u>Fashion magazines (online)</u></h3>
</br>
<p style="margin:14px 0px 0px 0px;"></p>
<table class="table table-borderless" style="text-align: center;">
            <tbody>
<tr><td>
<a href="http://www.harpersbazaar.com/fashion/trends/a11271/fashion-algorithm-suggests-outfits-for-better-instagram-photos/"><img src="news/bazaar.png" style="height:29px;"></a></td>
<td><a href="http://www.glamour.com/fashion/blogs/dressed/2015/06/outfit-making-computer-algorithm"><img src="news/glamour.png" style="height:29px;"></a></td>
<td><a href="http://www.elle.com/fashion/personal-style/a28974/fashion-algorithm-suggests-outfits-for-better-instagram-photos/"><img src="news/elle.jpg" style="height:29px;"></a></td>
<td><a href="http://www.cosmopolitan.co.uk/fashion/style/news/a36721/algorithm-solves-fashion-problems/"><img src="news/cosmopolitan.jpg" style="height:29px;"></a></td>
<td><a href="http://www.marieclaire.com/fashion/street-style/a14806/fashion-algorithm-suggests-outfits-for-better-instagram-photos/"><img src="news/marieclaire.jpg" style="height:26px;"></a></td>
</tr><tr><td><a href="http://www.harpersbazaar.com/fashion/trends/a11271/fashion-algorithm-suggests-outfits-for-better-instagram-photos/">Harper's Bazaar</a></td>
<td><a href="http://www.glamour.com/fashion/blogs/dressed/2015/06/outfit-making-computer-algorithm">Glamour</a></td>
<td><a href="http://www.elle.com/fashion/personal-style/a28974/fashion-algorithm-suggests-outfits-for-better-instagram-photos/">Elle</a></td>
<td><a href="http://www.cosmopolitan.co.uk/fashion/style/news/a36721/algorithm-solves-fashion-problems/">Cosmopolitan</a>, UK</td>
<td><a href="http://www.marieclaire.com/fashion/street-style/a14806/fashion-algorithm-suggests-outfits-for-better-instagram-photos/">Marie Claire</a></td>
</tr><tr>
<td height="34"> </td>
<td> </td>
<td> </td>
<td> </td>
<td </td>
</tr><tr><td><a href="http://www.fashionmagazine.com/fashion/2015/06/24/best-instagram-items/"><img src="news/fashion.png" style="height:29px;"></a></td></td>
<td><a href="https://www.yahoo.com/style/thanks-to-science-ootd-posts-will-never-be-the-122177373293.html"><img src="news/yahoostyle.png" style="height:30px;"></a></td>
<td><a href="http://www.redonline.co.uk/fashion/fashion_news/instagram-algorithm-to-dress-better11"><img src="news/red.png" style="height:29px;"></a></td>
<td><a href="https://www.the-pool.com/news-views/fashion-news/2015/24/can-the-internet-help-you-get-dressed-"><img src="news/pool.png" style="height:26px;"></a></td>
<td><a href="http://www.fashionotes.com/content/2015/06/instagram-wants-to-help-you-dress-better/"><img src="news/fn.png" style="height:29px;"></a></td>
</tr><tr><td><a href="http://www.fashionmagazine.com/fashion/2015/06/24/best-instagram-items/">Fashion Magazine</a></td>
<td><a href="https://www.yahoo.com/style/thanks-to-science-ootd-posts-will-never-be-the-122177373293.html">Yahoo style</a></td>
<td><a href="http://www.redonline.co.uk/fashion/fashion_news/instagram-algorithm-to-dress-better11">Red Magazine</a>, UK</td>
<td><a href="https://www.the-pool.com/news-views/fashion-news/2015/24/can-the-internet-help-you-get-dressed-">The Pool</a>, UK</td>
<td><a href="http://www.fashionotes.com/content/2015/06/instagram-wants-to-help-you-dress-better/">FashionNotes</a></td>
</tr></tbody></table>
</div>
<hr>
</br>
<p style="margin:-20px 0px 0px 0px;"></p>

      <div class="media">
      <p style="margin:-14px 0px 0px 0px;"></p>
<h3><u>International news</u></h3>
</br>
<p style="margin:14px 0px 0px 0px;"></p>
<table class="table table-borderless" style="text-align: center;">
            <tbody>
<tr><td>
<a href="http://www.vogue.es/moda/news/articulos/programa-ordenador-calcula-nivel-estilo/22801"><img src="news/voguespain.png" style="height:29px;"></a></td>
<td><a href="http://www.stylebook.de/fashion/Outfit-Selfies-auf-Instagram-neuer-Algorithmus-zieht-uns-magisch-an-659850.html"><img src="news/stylebook.png" style="height:25px;"></a></td>
<td><a href="http://www.ansa.it/sito/notizie/tecnologia/software_app/2015/06/23/algoritmo-suggerira-outfit-piu-di-moda_7a973ee6-81c3-44e3-9c7a-55338107c627.html"><img src="news/ansa.png" style="height:29px;"></a></td>
<td><a href="http://www.cenariomt.com.br/noticia/455713/as-melhores-roupas-e-acessorios-para-ficar-incrivel-no-instagram-segundo-a-ciencia.html"><img src="news/cenario.png" style="height:29px;"></a></td>
<td><a href="http://amsterdam-ftv-blog.com/archives/37103"><img src="news/ams1.png" style="height:31px;"></a></td>
</tr><tr><td><a href="http://www.vogue.es/moda/news/articulos/programa-ordenador-calcula-nivel-estilo/22801">Vogue</a> (Spain)</td>
<td><a href="http://www.stylebook.de/fashion/Outfit-Selfies-auf-Instagram-neuer-Algorithmus-zieht-uns-magisch-an-659850.html">Stylebook</a> (Germany)</td>
<td><a href="http://www.ansa.it/sito/notizie/tecnologia/software_app/2015/06/23/algoritmo-suggerira-outfit-piu-di-moda_7a973ee6-81c3-44e3-9c7a-55338107c627.html">Ansa</a> (Italy)</td>
<td><a href="http://www.cenariomt.com.br/noticia/455713/as-melhores-roupas-e-acessorios-para-ficar-incrivel-no-instagram-segundo-a-ciencia.html">CenarioMT</a>  (Brazil)</td>
<td><a href="http://amsterdam-ftv-blog.com/archives/37103">Amsterdam Fashion</a> (NL)</td>
</tr><tr>
<td height="34"> </td>
<td> </td>
<td> </td>
<td> </td>
<td </td>
</tr><tr><td><a href="http://www.marieclaire.fr/,on-a-trouve-le-secret-pour-etre-mieux-habillees-que-vos-amies,738591.asp"><img src="news/marieclaire.jpg" style="height:29px;"></a></td></td>
<td><a href="http://fashionpoliceng.com/there-is-a-new-algorithm-for-instagram-to-help-you-imrove-your-dress-sense/"><img src="news/fpn.png" style="height:29px;"></a></td>
<td><a href="http://naukawpolsce.pap.pl/aktualnosci/news,405480,komputer-zamiast-stylisty.html"><img src="news/nauka.png" style="height:29px;"></a></td>
<td><a href="http://www.pluska.sk/spravy/zo-zahranicia/instagram-vie-co-si-mate-obliect-moda-podla-matematiky.html"><img src="news/pluska.png" style="height:29px;"></a></td></td>
<td><a href="http://www.pressetext.com/news/20150701001"><img src="news/pressetext.png" style="height:29px;"></a></td>
</tr><tr><td><a href="http://www.marieclaire.fr/,on-a-trouve-le-secret-pour-etre-mieux-habillees-que-vos-amies,738591.asp">Marie Claire</a> (France)</td>
<td><a href="http://fashionpoliceng.com/there-is-a-new-algorithm-for-instagram-to-help-you-imrove-your-dress-sense/">Fashion Police</a> (Nigeria)</td>
<td><a href="http://naukawpolsce.pap.pl/aktualnosci/news,405480,komputer-zamiast-stylisty.html">Nauka</a> (Poland)</td>
<td><a href="http://www.pluska.sk/spravy/zo-zahranicia/instagram-vie-co-si-mate-obliect-moda-podla-matematiky.html">Pluska</a> (Slovakia)</td>
<td><a href="http://www.pressetext.com/news/20150701001">Pressetext</a> (Austria)</td>
</tr><tr>
<td height="34"> </td>
<td> </td>
<td> </td>
<td> </td>
<td </td>
</tr><tr><td><a href="https://www.wired.de/collection/latest/diese-software-kennst-sich-aus-sachen-fashion"><img src="news/wired_de.png" style="height:29px;"></a></td>
<td><a href="http://jetzt.sueddeutsche.de/texte/anzeigen/593320/Was-zieh-ich-an"><img src="news/jetzt.png" style="height:29px;"></a></td>
<td><a href="http://www.gazzetta.it/Sportlife/Moda/01-07-2015/dammi-foto-ti-diro-come-vestirti-l-algoritmo-look-che-piace-120388048293.shtml?refresh_ce-cp"><img src="news/gazzetta.png" style="height:29px;"></a></td>
<td><a href="http://www.popsugar.com.au/fashion/Best-Outfits-Wear-Instagram-37781122"><img src="news/popsugar.png" style="height:23px;"></a></td>
<td><a href="http://www.sinembargo.mx/19-06-2015/1384780"><img src="news/sinembargo.png" style="height:26px;"></a></td>
</tr><tr>
<td><a href="https://www.wired.de/collection/latest/diese-software-kennst-sich-aus-sachen-fashion">Wired</a> (Germany)</td>
<td><a href="http://jetzt.sueddeutsche.de/texte/anzeigen/593320/Was-zieh-ich-an">Jetzt</a> (Germany)</td>
<td><a href="http://www.gazzetta.it/Sportlife/Moda/01-07-2015/dammi-foto-ti-diro-come-vestirti-l-algoritmo-look-che-piace-120388048293.shtml?refresh_ce-cp">La Gazzetta</a> (Italy)</td>
<td><a href="http://www.popsugar.com.au/fashion/Best-Outfits-Wear-Instagram-37781122">PopSugar</a> (Australia)</td>
<td><a href="http://www.sinembargo.mx/19-06-2015/1384780">SinEmbargo</a> (Mexico)</td>
</tr><tr>
</tr></tbody></table>
</div>
<hr>
<p style="margin:25px 0px 0px 0px;"></p>
<p><b>A more complete list is maintained on our <a href="http://www.iri.upc.edu/people/esimo/en/research/fashionability/">project webpage</a>.</b></p>
<a data-toggle="collapse" data-parent="#SimoCVPR15" href="#Fashion15media">close window</a>
</div>
      </li>
    </ul>

<p style="margin:-8px 0px 0px 0px;"></p>


  <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.0%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:11px 1.7% 0 0; width:14.2%;" href="papers/fidler_et_al_cvpr13b.pdf"><img width="100%" src="papers/figs/cvpr13sent.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
            <h4 style="font-size:14.5px; line-height:120%">A Sentence is Worth a Thousand Pixels</h4>

	     <p style="margin:-8px 0px 0px 0px;"></p>

            <p style="font-size:14.0px"><strong>Sanja Fidler</strong>, Abhishek Sharma,  Raquel Urtasun</p>

		<p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">In <em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>), Portland, USA, June 2013</p>
                        <p style="margin:-10px 0px 0px 0px;"></p>
            <p style="font-size:14.0px; color:#E62E00;">Exploits textual descriptions of images to improve visual parsing</p>
 <p style="margin:-8px 0px 0px 0px;"></p>
 <a href="papers/fidler_et_al_cvpr13b.pdf" class="buttonTT">Paper</a>&nbsp
 <a class="buttonAA" data-toggle="collapse" data-parent="#FidlerCVPR13abs" href="#FidlerCVPR13abs-list">Abstract</a>&nbsp
<a href="cvpr13_sent_supplemental.pdf" class="buttonMM">Suppl. Mat.</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#FidlerCVPR13" href="#FidlerCVPR13-list">Bibtex</a><br/>
 
<p style="margin:2px 0px 0px 0px;"></p>
<div id="FidlerCVPR13-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
           <p style="font-size:15px;">@inproceedings{FidlerCVPR13,<br/>
    author = {Sanja Fidler and Abhishek Sharma and Raquel Urtasun},<br/>
    title = {A Sentence is Worth a Thousand Pixels},<br/>
    booktitle = {CVPR},<br/>
    year = {2013}<br/>}
</p>
</div>
<div id="FidlerCVPR13abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.5px;">
We are interested in holistic scene understanding where
images are accompanied with text in the form of complex
sentential descriptions. We propose a holistic conditional
random field model for semantic parsing which reasons
jointly about which objects are present in the scene, their
spatial extent as well as semantic segmentation, and employs
text as well as image information as input. We automatically
parse the sentences and extract objects and their
relationships, and incorporate them into the model, both via
potentials as well as by re-ranking candidate detections. We
demonstrate the effectiveness of our approach in the challenging
UIUC sentences dataset and show segmentation improvements
of 12.5% over the visual only model and detection
improvements of 5% AP over deformable part-based
models.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
      </li>
    </ul>
    <br/>
 <p style="margin:-10px 0px 0px 0px;"></p>

 


<font style="font-size:1.11em; font-weight:500;">Learning Visual Models from Text</font>
<br/>
<br/>
 <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.4%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:11px 2% 0 0; width:14%;" href=""><img width="100%" src="papers/figs/zeroshot_small.jpg"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:72%;">
            <h4 style="font-size:14.5px; line-height:120%">Predicting Deep Zero-Shot Convolutional Neural Networks using Textual Descriptions</h4>

	     <p style="margin:-8px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">Jimmy Ba, Kevin Swersky, <strong>Sanja Fidler</strong>, Ruslan Salakhutdinov</p>

	     <p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">arXiv preprint <em>arXiv:1506.00511</em>, 2015</p>
<p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px; color:#E62E00;">Classification of unseen categories from their textual description (Wiki articles)</p>

<p style="margin:-8px 0px 0px 0px;"></p>
            
            <a href="http://arxiv.org/abs/1506.00511" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#ZeroShotabs" href="#ZeroShotabs-list">Abstract</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#ZeroShot" href="#ZeroShot-list">Bibtex</a><br/>

 
<p style="margin:2px 0px 0px 0px;"></p>
<div id="ZeroShot-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{moviebook,<br/>
    title = {Predicting Deep Zero-Shot Convolutional Neural Networks using Textual Descriptions},<br/>
    author = {Jimmy Ba and Kevin Swersky and Sanja Fidler and Ruslan Salakhutdinov},<br/>
    booktitle = {arXiv preprint arXiv:1506.00511},<br/>
    year = {2015}</br>}
</p>      
</div>
<div id="ZeroShotabs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.5px;">
One of the main challenges in Zero-Shot Learning of visual categories is gathering semantic attributes to accompany images. Recent work has shown that learning from textual descriptions, such as Wikipedia articles, avoids the problem of having to explicitly define these attributes. We present a new model that can classify unseen categories from their textual description. Specifically, we use text features to predict the output weights of both the convolutional and the fully connected layers in a deep convolutional neural network (CNN). We take advantage of the architecture of CNNs and learn features at different layers, rather than just learning an embedding space for both modalities, as is common with existing approaches. The proposed model also allows us to automatically generate a list of pseudo- attributes for each visual category consisting of words from Wikipedia articles. We train our models end-to-end us- ing the Caltech-UCSD bird and flower datasets and evaluate both ROC and Precision-Recall curves. Our empirical results show that the proposed model significantly outperforms previous methods.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
<div class="pull-right text-center" style="padding:4px 0.5% 0 0; width:6.5%;" href=""><img width="100%" src="papers/figs/new_burst.jpg"></div>
      </li>
    </ul>
     <br/>   
 <p style="margin:-10px 0px 0px 0px;"></p>
     
    <font style="font-size:1.11em; font-weight:500;">Caption Generation</font>
<br/>
<br/>
  <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.0%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:11px 2.2% 0 0; width:14.2%;" href=""><img width="100%" src="papers/figs/arxiv2015generation_small.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
            <h4 style="font-size:14.5px; line-height:120%">Generating Multi-Sentence Lingual Descriptions of Indoor Scenes&nbsp&nbsp&nbsp&nbsp&nbsp(<strong><em>oral presentation</em></strong>)</h4>

	     <p style="margin:-8px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">Dahua Lin, Chen Kong, <strong>Sanja Fidler</strong>, Raquel Urtasun</p>

	     <p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">In <em>British Machine Vision Conference</em> (<strong>BMVC</strong>), To appear, 2015</p>
            <!--<p style="font-size:14.0px"><em>arXiv:1503.00064</em>, Feb 28, 2015</p>-->
            
            <p style="margin:-8px 0px 0px 0px;"></p>
            
            
            <a href="http://arxiv.org/abs/1503.00064" class="buttonTT">Paper</a>&nbsp
 <a class="buttonAA" data-toggle="collapse" data-parent="#Lin15abs" href="#Lin15abs-list">Abstract</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#Lin15" href="#Lin15-list">Bibtex</a><br/>
 
<p style="margin:2px 0px 0px 0px;"></p>
<div id="Lin15-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{Lin15,<br/>
    title = {Generating Multi-Sentence Lingual Descriptions of Indoor Scenes},<br/>
    author = {Dahua Lin and Chen Kong and Sanja Fidler and Raquel Urtasun},<br/>
    booktitle = {arXiv:1503.00064},<br/>
    year = {2015}}
</p>      
</div>
<div id="Lin15abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.5px;">
This paper proposes a novel framework for generating lingual descriptions of
indoor scenes. Whereas substantial efforts have been made to tackle this
problem, previous approaches focusing primarily on generating a single sentence
for each image, which is not sufficient for describing complex scenes. We
attempt to go beyond this, by generating coherent descriptions with multiple
sentences. Our approach is distinguished from conventional ones in several
aspects: (1) a 3D visual parsing system that jointly infers objects,
attributes, and relations; (2) a generative grammar learned automatically from
training text; and (3) a text generation algorithm that takes into account the
coherence among sentences. Experiments on the augmented NYU-v2 dataset show
that our framework can generate natural descriptions with substantially higher
ROGUE scores compared to those produced by the baseline.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
      </li>
    </ul>
  <p style="margin:-8px 0px 0px 0px;"></p> 
  
    <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.0%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:6px 3.5% 0 0; width:14.2%;" href="http://www.cs.toronto.edu/~sven/Papers/UAI2012.pdf"><img width="100%" src="papers/figs/uai12sent.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
            <h4 style="font-size:14.5px; line-height:120%">Video In Sentences Out &nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp&nbsp <em>(<b>oral presentation</b>)</em></h4>

	     <p style="margin:-8px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">Andrei Barbu, Alexander Bridge, Zachary Burchill, Dan Coroian, Sven Dickinson, <strong>Sanja Fidler</strong>, Aaron Michaux, Sam Mussman, Siddharth Narayanaswamy, Dhaval Salvi, Lara Schmidt, Jiangnan Shangguan, Jeffrey Mark Siskind, Jarrell Waggoner, Song Wang, Jinlian Wei, Yifan Yin, Zhiqi Zhang</p>

		<p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">In <em>Uncertainty in Artificial Intelligence</em> (<strong>UAI</strong>), Catalina Island, USA, 2012</p>
            <p style="margin:-8px 0px 0px 0px;"></p>
            <a href="http://www.cs.toronto.edu/~sven/Papers/UAI2012.pdf" class="buttonTT">Paper</a>&nbsp
             <a class="buttonAA" data-toggle="collapse" data-parent="#BarbuUAI12abs" href="#BarbuUAI12abs-list">Abstract</a>&nbsp
<a href="https://engineering.purdue.edu/~qobi/mindseye/" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#BarbuUAI12" href="#BarbuUAI12-list">Bibtex</a><br/>
 
<p style="margin:2px 0px 0px 0px;"></p>
<div id="BarbuUAI12-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
          <p style="font-size:15px;">@inproceedings{BarbuUAI12,<br/>
    author = {Andrei Barbu and Alexander Bridge and Zachary Burchill and Dan Coroian and Sven Dickinson and Sanja Fidler and Aaron Michaux and Sam Mussman and Siddharth Narayanaswamy and Dhaval Salvi and Lara Schmidt and Jiangnan Shangguan and Jeffrey Mark Siskind and Jarrell Waggoner and Song Wang and Jinlian Wei and Yifan Yin and Zhiqi Zhang},<br/>
    title = {Video In Sentences Out},<br/>
    booktitle = {UAI},<br/>
    year = {2012}<br/>}
</p>
</div>
<div id="BarbuUAI12abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.5px;">
We present a system that produces sentential
descriptions of video: who did what to whom,
and where and how they did it. Action class
is rendered as a verb, participant objects as
noun phrases, properties of those objects as
adjectival modifiers in those noun phrases,
spatial relations between those participants
as prepositional phrases, and characteristics
of the event as prepositional-phrase adjuncts
and adverbial modifiers. Extracting the information
needed to render these linguistic
entities requires an approach to event recognition
that recovers object tracks, the track-to-role
assignments, and changing body posture.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
      </li>
    </ul>
    <br/>
 <p style="margin:-10px 0px 0px 0px;"></p>


<font style="font-size:1.11em; font-weight:500;">Word-Sense Disambiguation</font>
<br/>
<br/>
   <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.0%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:9px 4% 0 0; width:14.2%;" href="papers/SEM2012.pdf"><img width="100%" src="papers/figs/sem12wsd.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
            <h4 style="font-size:14.5px; line-height:120%">Unsupervised Disambiguation of Image Captions</h4>

	     <p style="margin:-8px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">Wesley May, <strong>Sanja Fidler</strong>, Afsaneh Fazly, Suzanne Stevenson, Sven Dickinson</p>

		<p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px"><em>First Joint Conference on Lexical and Computational Semantics (*SEM)</em>, 2012</p>
            <p style="margin:-8px 0px 0px 0px;"></p>
            <a href="papers/SEM2012.pdf" class="buttonTT">Paper</a>&nbsp
             <a class="buttonAA" data-toggle="collapse" data-parent="#MaySEM12abs" href="#MaySEM12abs-list">Abstract</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#MaySEM12" href="#MaySEM12-list">Bibtex</a><br/>
 
<p style="margin:2px 0px 0px 0px;"></p>
<div id="MaySEM12-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
           <p style="font-size:15px;">@inproceedings{MaySEM12,<br/>
    author = {Wesley May and Sanja Fidler and Afsaneh Fazly and Suzanne Stevenson and Sven Dickinson},<br/>
    title = {Unsupervised Disambiguation of Image Captions},<br/>
    booktitle = {First Joint Conference on Lexical and Computational Semantics (*SEM)},<br/>
    year = {2012}<br/>}
</p>
</div>
<div id="MaySEM12abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.5px;">
Given a set of images with related captions,
our goal is to show how visual features can
improve the accuracy of unsupervised word
sense disambiguation when the textual context
is very small, as this sort of data is common
in news and social media. We extend
previous work in unsupervised text-only disambiguation
with methods that integrate text
and images. We construct a corpus by using
Amazon Mechanical Turk to caption sense-tagged
images gathered from ImageNet. Using
a Yarowsky-inspired algorithm, we show
that gains can be made over text-only disambiguation,
as well as multimodal approaches
such as Latent Dirichlet Allocation.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
      </li>
    </ul>
 
                    <a data-toggle="collapse" data-parent="#imagetext" href="#imagetext-info">close window</a>
      </div>
      </div>     
 
 <!-- SEGMENTATION -->
       <div id="segmentation-info" class="panel-collapse collapse out" style="padding:0% 3% 1% 3%; border-radius:6px; border-style: solid; border-color: #666666;">

<p style="margin:10px 0px 0px 0px;"></p>
<font style="font-size:1.34em; font-weight:500;"> Segmentation</font>
      <div class="media">

      
      <p align="justify">Image segmentation takes on many forms varying from low-level grouping of pixels into superpixels, grouping (super)pixels into object proposals, and image labeling. Each task is important in its own way; grouping pixels into superpixels in an efficient way subserves high-level semantic tasks since it reduces the complexity of the input. Generating a small set of object proposals facilities object detection as it prunes down the exhaustive set of possible windows down to a small number of plausible candidates. Image-labeling, on the other hand, tries to assign a semantic label to each (super)pixel. Our work falls into all three of these domains, providing a link between low-level image information and more high-level reasoning in other domain of our research.</p>
 
 <p style="margin:30px 0px 0px 0px;"></p>  
      
       <h3> Relevant Publications</h3>

<br/>
<p style="margin:-10px 0px 0px 0px;"></p>


<font style="font-size:1.11em; font-weight:500;">Superpixels and Superpixel Grouping</font>
<br/>
<br/>
<p style="margin:-12px 0px 0px 0px;"></p>
  <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.0%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:11px 2.2% 0 0; width:14.2%;" href=""><img width="100%" src="papers/figs/cvpr15suppixels_small.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
            <h4 style="font-size:14.5px; line-height:120%">Real-Time Coarse-to-fine Topologically Preserving Segmentation</h4>

	     <p style="margin:-8px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">Jian Yao, Marko Boben, <strong>Sanja Fidler</strong>, Raquel Urtasun</p>

	     <p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">In <em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>), Boston, 2015</p>
            <!--<p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px; color:#E62E00;">Exploiting map priors for segmentation and monocular depth estimation</p>-->

<p style="margin:-8px 0px 0px 0px;">
 <a href="papers/yaoCVPR15.pdf" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#YaoCVPR15abs" href="#YaoCVPR15abs-list">Abstract</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#YaoCVPR15" href="#YaoCVPR15-list">Bibtex</a><br/>

<p style="margin:2px 0px 0px 0px;"></p>
<div id="YaoCVPR15-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@inproceedings{YaoCVPR15,<br/>
    title = {Real-Time Coarse-to-fine Topologically Preserving Segmentation},<br/>
    author = {Jian Yao and Marko Boben and Sanja Fidler and Raquel Urtasun},<br/>
    booktitle = {CVPR},<br/>
    year = {2015}<br/>}
</p>      
</div>
<div id="YaoCVPR15abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.5px;">
In this paper, we tackle the problem of unsupervised segmentation
in the form of superpixels. Our main emphasis is
on speed and accuracy. We build on [Yamaguchi et al., ECCV'14] to define the problem
as a boundary and topology preserving Markov random
field. We propose a coarse to fine optimization technique
that speeds up inference in terms of the number of updates
by an order of magnitude. Our approach is shown to outperform
[Yamaguchi et al., ECCV'14] while employing a single iteration. We evaluate
and compare our approach to state-of-the-art superpixel algorithms
on the BSD and KITTI benchmarks. Our approach
significantly outperforms the baselines in the segmentation
metrics and achieves the lowest error on the stereo task.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
<!--
<div class="pull-right text-center" style="padding:0.5% 1% 0% 0%; width:11%;">
<a href="papers/yaoCVPR15.pdf" class="buttonT" style="width:100%;">Paper</a><br/>
<p style="margin:1.5px 0px 0px 0px;"></p>
<a class="buttonA" data-toggle="collapse" data-parent="#YaoCVPR15abs" href="#YaoCVPR15abs-list" style="width:100%; margin-top:6px; text-decoration: none; color:#00CFCF;">Abstract</a><br/>
<p style="margin:0px 0px 0px 0px;"></p>
<a class="buttonS" data-toggle="collapse" data-parent="#YaoCVPR15" href="#YaoCVPR15-list" style="width:100%; margin-top:6px; text-decoration: none; color:#FF9933;">Bibtex</a><br/>
        </div>
        -->
      </li>
    </ul>
 
 <p style="margin:-8px 0px 0px 0px;"></p>
 
   <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.0%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:10px 3.1% 0 0; width:14.2%;" href=""><img width="100%" src="papers/figs/lee_symmetry15.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
            <h4 style="font-size:14.5px; line-height:120%">A Framework for Symmetric Part Detection in Cluttered Scenes</h4>

	     <p style="margin:-8px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">Tom Lee, <strong>Sanja Fidler</strong>, Alex Levinshtein, Cristian Sminchisescu, Sven Dickinson</p>

	     <p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px"><em>Symmetry</em>, Vol. 7, 2015, pp 1333-1351</p>
            <p style="margin:-8px 0px 0px 0px;"></p>

    <a href="papers/Symmetry2015.pdf" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#LeeSymmetry15abs" href="#LeeSymmetry15abs-list">Abstract</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#LeeSymmetry15" href="#LeeSymmetry15-list">Bibtex</a><br/>
 
<p style="margin:2px 0px 0px 0px;"></p>
<div id="LeeSymmetry15-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@article{LeeSymmetry2015,<br/>
    title = {A Framework for Symmetric Part Detection in Cluttered Scenes},<br/>
    author = {Tom Lee and Sanja Fidler and Alex Levinshtein and Cristian Sminchisescu and Sven Dickinson},<br/>
    journal = {Symmetry},<br/>
    volume = {7},<br/>
    pages = {1333-1351},<br/>
    year = {2015}<br/>}
</p>      
</div>
<div id="LeeSymmetry15abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.5px;">
The role of symmetry in computer vision has waxed and waned in importance
during the evolution of the field from its earliest days. At first figuring prominently in support
of bottom-up indexing, it fell out of favour as shape gave way to appearance and recognition
gave way to detection. With a strong prior in the form of a target object, the role of the
weaker priors offered by perceptual grouping was greatly diminished. However, as the field
returns to the problem of recognition from a large database, the bottom-up recovery of the
parts that make up the objects in a cluttered scene is critical for their recognition. The medial
axis community has long exploited the ubiquitous regularity of symmetry as a basis for the
decomposition of a closed contour into medial parts. However, today's recognition systems
are faced with cluttered scenes and the assumption that a closed contour exists, i.e., that
figure-ground segmentation has been solved, rendering much of the medial axis community's
work inapplicable. In this article, we review a computational framework, previously reported
in [Lee et al., ICCV'13, Levinshtein et al., ICCV'09, Levinshtein et al., IJCV'13], that bridges the representation power of the medial axis and the need to recover and
group an object's parts in a cluttered scene. Our framework is rooted in the idea that a
maximally-inscribed disc, the building block of a medial axis, can be modelled as a compact
superpixel in the image. We evaluate the method on images of cluttered scenes.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
      </li>
    </ul>
 <p style="margin:-8px 0px 0px 0px;"></p>
 
    <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.0%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:11px 1.7% 0 0; width:14.2%;" href="papers/lee_et_al_iccv13.pdf"><img width="100%" src="papers/figs/iccv13symmetrya.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
            <h4 style="font-size:14.5px; line-height:120%">Detecting Curved Symmetric Parts using a Deformable Disc Model</h4>

	     <p style="margin:-8px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">Tom Lee, <strong>Sanja Fidler</strong>, Sven Dickinson</p>

		<p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">In <em>International Conference on Computer Vision</em> (<strong>ICCV</strong>), Sydney, Australia, 2013</p>
 <p style="margin:-8px 0px 0px 0px;"></p>
 <a href="papers/lee_et_al_iccv13.pdf" class="buttonTT">Paper</a>&nbsp
 <a class="buttonAA" data-toggle="collapse" data-parent="#LeeICCV13abs" href="#LeeICCV13abs-list">Abstract</a>&nbsp
<a href="http://www.cs.toronto.edu/~tshlee/SymmetricParts/" class="buttonPP">Project page</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#LeeICCV13" href="#LeeICCV13-list">Bibtex</a><br/>
 
<p style="margin:2px 0px 0px 0px;"></p>
<div id="LeeICCV13-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
          <p style="font-size:15px;">@inproceedings{LeeICCV13,<br/>
    author = {Tom Lee and Sanja Fidler and Sven Dickinson},<br/>
    title = {Detecting Curved Symmetric Parts using a Deformable Disc Model},<br/>
    booktitle = {ICCV},<br/>
    year = {2013}<br/>}
</p>
</div>
<div id="LeeICCV13abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.5px;">
Symmetry is a powerful shape regularity that's been exploited
by perceptual grouping researchers in both human
and computer vision to recover part structure from an image
without a priori knowledge of scene content. Drawing
on the concept of a medial axis, defined as the locus of
centers of maximal inscribed discs that sweep out a symmetric
part, we model part recovery as the search for a
sequence of deformable maximal inscribed disc hypotheses
generated from a multiscale superpixel segmentation,
a framework proposed by [Levinshtein et al., ICCV'09]. However, we learn affinities
between adjacent superpixels in a space that's invariant to
bending and tapering along the symmetry axis, enabling us
to capture a wider class of symmetric parts. Moreover, we
introduce a global cost that perceptually integrates the hypothesis
space by combining a pairwise and a higher-level
smoothing term, which we minimize globally using dynamic
programming. The new framework is demonstrated on two
datasets, and is shown to significantly outperform the baseline
[Levinshtein et al., ICCV'09].
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
<!--
<div class="pull-right text-center" style="padding:0.5% 1% 0% 0%; width:11%;">
<a href="papers/lee_et_al_iccv13.pdf" class="buttonT" style="width:100%;">Paper</a><br/>
<p style="margin:1.3px 0px 0px 0px;"></p>
<a href="http://www.cs.toronto.edu/~tshlee/SymmetricParts/" class="buttonP" style="width:100%;">Project page</a><br/>
<p style="margin:0px 0px 0px 0px;"></p>
<a class="buttonS" data-toggle="collapse" data-parent="#LeeICCV13" href="#LeeICCV13-list" style="width:100%; margin-top:5px; text-decoration: none; color:#FF9933;">Bibtex</a><br/>
        </div>
        -->
      </li>
    </ul>	
<br/>
<p style="margin:-15px 0px 0px 0px;"></p>



<font style="font-size:1.11em; font-weight:500;">Object Proposals</font>
<br/>
<br/>
<p style="margin:-15px 0px 0px 0px;"></p>
  <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.0%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:11px 3.6% 0 0; width:14.2%;" href="papers/lee_et_al_accv14.pdf"><img width="100%" src="papers/figs/accv14multicue.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
            <h4 style="font-size:14.5px; line-height:120%">Multi-cue Mid-level Grouping</h4>

	     <p style="margin:-8px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">Tom Lee, <strong>Sanja Fidler</strong>, Sven Dickinson</p>

	     <p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">In <em>Asian Conference on Computer Vision</em> (<strong>ACCV</strong>), Singapore, November, 2014</p>
            <p style="margin:-8px 0px 0px 0px;"></p>
            
 <a href="papers/lee_et_al_accv14.pdf" class="buttonTT">Paper</a>&nbsp
 <a class="buttonAA" data-toggle="collapse" data-parent="#LeeACCV14abs" href="#LeeACCV14abs-list">Abstract</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#LeeACCV14" href="#LeeACCV14-list">Bibtex</a><br/>

<p style="margin:2px 0px 0px 0px;"></p>
<div id="LeeACCV14-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
<p style="line-height:8px"> &nbsp</p>
            <p style="font-size:15px;">@inproceedings{LeeACCV14,<br/>
    title = {Multi-cue mid-level grouping},<br/>
    author = {Tom Lee and Sanja Fidler and Sven Dickinson},<br/>
    booktitle = {ACCV},<br/>
    year = {2014}<br/>}
</p>    
</div>
<div id="LeeACCV14abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.5px;">
Region proposal methods provide richer object hypotheses
than sliding windows with dramatically fewer proposals, yet they still
number in the thousands. This large quantity of proposals typically results
from a diversification step that propagates bottom-up ambiguity in
the form of proposals to the next processing stage. In this paper, we take
a complementary approach in which mid-level knowledge is used to resolve
bottom-up ambiguity at an earlier stage to allow a further reduction
in the number of proposals. We present a method for generating regions
using the mid-level grouping cues of closure and symmetry. In doing so,
we combine mid-level cues that are typically used only in isolation, and
leverage them to produce fewer but higher quality proposals. We emphasize
that our model is mid-level by learning it on a limited number of
objects while applying it to different objects, thus demonstrating that
it is transferable to other objects. In our quantitative evaluation, we 1)
establish the usefulness of each grouping cue by demonstrating incremental
improvement, and 2) demonstrate improvement on two leading
region proposal methods with a limited budget of proposals.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>

      </li>
    </ul>
<br/>
<p style="margin:-15px 0px 0px 0px;"></p>


<font style="font-size:1.11em; font-weight:500;">Image Labeling</font>
<br/>
<br/>
<p style="margin:-12px 0px 0px 0px;"></p>
<ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.0%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:11px 1.7% 0 0; width:14.2%;" href=""><img width="100%" src="papers/figs/pami15small.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
            <h4 style="font-size:14.5px; line-height:120%">Human-Machine CRFs for Identifying Bottlenecks in Scene Understanding</h4>

	     <p style="margin:-8px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">Roozbeh Mottaghi, <strong>Sanja Fidler</strong>, Alan Yuille, Raquel Urtasun, Devi Parikh</p>

	     <p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px"><em>Transactions on Pattern Analysis and Machine Intelligence</em> (<strong>TPAMI</strong>), To appear 2015</p>
            <p style="margin:-8px 0px 0px 0px;"></p>

    <a href="papers/mottaghiPAMI15.pdf" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#Mottaghi15abs" href="#Mottaghi15abs-list">Abstract</a>&nbsp
    <a href="papers/mottaghiPAMI15supmat.pdf" class="buttonMM">Suppl. Mat.</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#Mottaghi15" href="#Mottaghi15-list">Bibtex</a><br/>
 
<p style="margin:2px 0px 0px 0px;"></p>
<div id="Mottaghi15-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:0% 3% 1% 3%; border-radius:10px;">
            <p style="font-size:15px;">@article{MottaghiPAMI15,<br/>
    title = {Human-Machine CRFs for Identifying Bottlenecks in Scene Understanding},<br/>
    author = {Roozbeh Mottaghi and Sanja Fidler and Alan Yuille and Raquel Urtasun and Devi Parikh},<br/>
    journal = {Trans. on Pattern Analysis and Machine Intelligence},<br/>
    year = {2015}<br/>}
</p>      
</div>
<div id="Mottaghi15abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.5px;">
Recent trends in image understanding have pushed for scene understanding models that jointly reason about various
tasks such as object detection, scene recognition, shape analysis, contextual reasoning, and local appearance based classifiers.
In this work, we are interested in understanding the roles of these different tasks in improved scene understanding, in particular
semantic segmentation, object detection and scene recognition. Towards this goal, we "plug-in" human subjects for each of the
various components in a conditional random field model. Comparisons among various hybrid human-machine CRFs give us
indications of how much "head room" there is to improve scene understanding by focusing research efforts on various individual
tasks.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
</div>
<!--
<div class="pull-right text-center" style="padding:0.1% 1% 0% 0%; width:11%;">
    <a href="papers/mottaghiPAMI15.pdf" class="buttonT" style="width:100%; height:14.6px; font-size:11px;">Paper</a><br/>
    <p style="margin:-10px 0px 0px 0px;"></p>
<a class="buttonA" data-toggle="collapse" data-parent="#Mottaghi15abs" href="#Mottaghi15abs-list" style="width:100%; margin-top:6px; text-decoration: none; color:#00CFCF; height:14.6px; font-size:11px;">Abstract</a><br/>
<p style="margin:1px 0px 0px 0px;"></p>
    <a href="papers/mottaghiPAMI15supmat.pdf" class="buttonM" style="width:100%; padding:0 0 -5px 0; height:14.6px; font-size:10.5px;">Suppl. Mat.</a><br/>
    <p style="margin:-8px 0px 0px 0px;"></p>
<a class="buttonS" data-toggle="collapse" data-parent="#Mottaghi15" href="#Mottaghi15-list" style="width:100%; margin-top:6px; text-decoration: none; color:#FF9933; height:14.6px; font-size:11px;">Bibtex</a><br/>
        </div>
        -->
      </li>
    </ul>

<p style="margin:-8px 0px 0px 0px;"></p>

 <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.0%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:11px 1.5% 0 0; width:14.2%;" href="papers/chen_et_al_cvpr14b.pdf"><img width="100%" src="papers/figs/cvpr14segkitti.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
            <h4 style="font-size:14.5px; line-height:120%">Beat the MTurkers: Automatic Image Labeling from Weak 3D Supervision</h4>

	     <p style="margin:-8px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">Liang-Chieh Chen, <strong>Sanja Fidler</strong>, Alan Yuille, Raquel Urtasun</p>

		<p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">In <em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>), Columbus, USA, June, 2014</p>
<p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px; color:#E62E00;">Ground-truth segmentations provided for a subset of KITTI cars in Project page</p>
<p style="margin:-8px 0px 0px 0px;"></p>
<a href="papers/chen_et_al_cvpr14b.pdf" class="buttonTT">Paper</a>&nbsp
<a class="buttonAA" data-toggle="collapse" data-parent="#ChenCVPR14abs" href="#ChenCVPR14abs-list">Abstract</a>&nbsp
<a href="http://www.cs.ucla.edu/~lcchen/beat_the_MTurkers.html" class="buttonPP">Project page</a>&nbsp
<a href="projects/CAD.html" class="buttonPP">CAD models</a>&nbsp
<a href="papers/cvpr14_seg_supplemental.pdf" class="buttonMM">Suppl. Mat.</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#ChenCVPR14" href="#ChenCVPR14-list">Bibtex</a><br/>

 
<p style="margin:2px 0px 0px 0px;"></p>
<div id="ChenCVPR14-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
<p style="font-size:15px;">@inproceedings{ChenCVPR14,<br/>
    author = {Liang-Chieh Chen and  Sanja Fidler and Alan Yuille and Raquel Urtasun},<br/>
    title = {Beat the MTurkers: Automatic Image Labeling from Weak 3D Supervision},<br/>
    booktitle = {CVPR},<br/>
    year = {2014}<br/>}</p>
</div>
<div id="ChenCVPR14abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.5px;">
Labeling large-scale datasets with very accurate object
segmentations is an elaborate task that requires a high degree
of quality control and a budget of tens or hundreds of
thousands of dollars. Thus, developing solutions that can
automatically perform the labeling given only weak supervision
is key to reduce this cost. In this paper, we show how
to exploit 3D information to automatically generate very accurate
object segmentations given annotated 3D bounding
boxes. We formulate the problem as the one of inference in
a binary Markov random field which exploits appearance
models, stereo and/or noisy point clouds, a repository of 3D
CAD models as well as topological constraints. We demonstrate
the effectiveness of our approach in the context of autonomous
driving, and show that we can segment cars with
the accuracy of 86% intersection-over-union, performing as
well as highly recommended MTurkers!
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
      </li>
    </ul>
<p style="margin:-8px 0px 0px 0px;"></p>


  <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.0%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:11px 1.7% 0 0; width:14.2%;" href="papers/fidler_et_al_cvpr13b.pdf"><img width="100%" src="papers/figs/cvpr13sent.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
            <h4 style="font-size:14.5px; line-height:120%">A Sentence is Worth a Thousand Pixels</h4>

	     <p style="margin:-8px 0px 0px 0px;"></p>

            <p style="font-size:14.0px"><strong>Sanja Fidler</strong>, Abhishek Sharma,  Raquel Urtasun</p>

		<p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">In <em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>), Portland, USA, June 2013</p>
            <p style="margin:-10px 0px 0px 0px;"></p>
            <p style="font-size:14.0px; color:#E62E00;">Reasons about object detection, segmentation, scene-type and sentence descriptions to improve image parsing.</p>
 <p style="margin:-8px 0px 0px 0px;"></p>
 <a href="papers/fidler_et_al_cvpr13b.pdf" class="buttonTT">Paper</a>&nbsp
 <a class="buttonAA" data-toggle="collapse" data-parent="#FidlerCVPR13abs" href="#FidlerCVPR13abs-list">Abstract</a>&nbsp
<a href="cvpr13_sent_supplemental.pdf" class="buttonMM">Suppl. Mat.</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#FidlerCVPR13" href="#FidlerCVPR13-list">Bibtex</a><br/>
 
<p style="margin:2px 0px 0px 0px;"></p>
<div id="FidlerCVPR13-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
           <p style="font-size:15px;">@inproceedings{FidlerCVPR13,<br/>
    author = {Sanja Fidler and Abhishek Sharma and Raquel Urtasun},<br/>
    title = {A Sentence is Worth a Thousand Pixels},<br/>
    booktitle = {CVPR},<br/>
    year = {2013}<br/>}
</p>
</div>
<div id="FidlerCVPR13abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.5px;">
We are interested in holistic scene understanding where
images are accompanied with text in the form of complex
sentential descriptions. We propose a holistic conditional
random field model for semantic parsing which reasons
jointly about which objects are present in the scene, their
spatial extent as well as semantic segmentation, and employs
text as well as image information as input. We automatically
parse the sentences and extract objects and their
relationships, and incorporate them into the model, both via
potentials as well as by re-ranking candidate detections. We
demonstrate the effectiveness of our approach in the challenging
UIUC sentences dataset and show segmentation improvements
of 12.5% over the visual only model and detection
improvements of 5% AP over deformable part-based
models.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
<!--
<div class="pull-right text-center" style="padding:0.5% 1% 0% 0%; width:11%;">
<a href="papers/fidler_et_al_cvpr13b.pdf" class="buttonT" style="width:100%;">Paper</a><br/>
<p style="margin:1.4px 0px 0px 0px;"></p>
<a href="cvpr13_sent_supplemental.pdf" class="buttonM" style="width:100%;">Suppl. Mat.</a><br/>
<p style="margin:0px 0px 0px 0px;"></p>
<a class="buttonS" data-toggle="collapse" data-parent="#FidlerCVPR13" href="#FidlerCVPR13-list" style="width:100%; margin-top:5px; text-decoration: none; color:#FF9933;">Bibtex</a><br/>
        </div>
        -->
      </li>
    </ul>
							
<p style="margin:-8px 0px 0px 0px;"></p>

 <ul class="list-group">
      <li class="list-group-item" style="padding:0 0 0.2% 1.0%;">
        <div class="media">
          <div class="pull-left text-center" style="padding:11px 2% 0 0; width:14.2%;" href="papers/cvpr12seg.pdf"><img width="100%" src="papers/figs/cvpr12holistic.png"></div>           
<div class="pull-left text-left" style="padding:0% 0% 0% 0.1%; width:80%;">
            <h4 style="font-size:14.5px; line-height:120%">Describing the Scene as a Whole: Joint Object Detection, Scene Classification and Semantic Segmentation</h4>

	     <p style="margin:-8px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">Jian Yao, <strong>Sanja Fidler</strong>, Raquel Urtasun</p>

		<p style="margin:-10px 0px 0px 0px;"></p>

            <p style="font-size:14.0px">In <em>Computer Vision and Pattern Recognition</em> (<strong>CVPR</strong>), Providence, USA, June 2012</p>
<p style="margin:-10px 0px 0px 0px;"></p>
            <p style="font-size:14.0px; color:#E62E00;">Code, trained models and annotated bounding boxes for MSRC in Project page</p>
<p style="margin:-8px 0px 0px 0px;"></p>
<a href="papers/cvpr12seg.pdf" class="buttonTT">Paper</a>&nbsp
 <a class="buttonAA" data-toggle="collapse" data-parent="#YaoCVPR12abs" href="#YaoCVPR12abs-list">Abstract</a>&nbsp
<a href="http://ttic.uchicago.edu/~yaojian/HolisticSceneUnderstanding.html" class="buttonPP">Project page.</a>&nbsp
<a class="buttonSS" data-toggle="collapse" data-parent="#YaoCVPR12" href="#YaoCVPR12-list">Bibtex</a><br/>
 
<p style="margin:2px 0px 0px 0px;"></p>
<div id="YaoCVPR12-list" class="panel-collapse collapse out" style="background-color:#FFE0C2; padding:2% 3% 1% 3%; border-radius:10px;">
          <p style="font-size:15px;">@inproceedings{YaoCVPR12,<br/>
    author = {Jian Yao and Sanja Fidler and Raquel Urtasun},<br/>
    title = {Describing the Scene as a Whole: Joint Object Detection, Scene Classification and Semantic Segmentation},<br/>
    booktitle = {CVPR},<br/>
    year = {2012}<br/>}
</p>
</div>
<div id="YaoCVPR12abs-list" class="panel-collapse collapse out" style="background-color:#ADEBFF; padding:9px 2.5% 3px 2.5%; border-radius:10px;">
<p style="margin:0px 0px 0px 0px;"></p>
            <p style="font-size:14.5px;">
In this paper we propose an approach to holistic scene
understanding that reasons jointly about regions, location,
class and spatial extent of objects, presence of a class in the
image, as well as the scene type. Learning and inference in
our model are efficient as we reason at the segment level,
and introduce auxiliary variables that allow us to decompose
the inherent high-order potentials into pairwise potentials
between a few variables with small number of states (at
most the number of classes). Inference is done via a convergent
message-passing algorithm, which, unlike graph-cuts
inference, has no submodularity restrictions and does not
require potential specific moves. We believe this is very important,
as it allows us to encode our ideas and prior knowledge
about the problem without the need to change the inference
engine every time we introduce a new potential. Our
approach outperforms the state-of-the-art on the MSRC-21
benchmark, while being much faster. Importantly, our holistic
model is able to improve performance in all tasks.
</p>      
<p style="margin:0px 0px 0px 0px;"></p>
</div>
<!--
<div class="pull-right text-center" style="padding:0.5% 1% 0% 0%; width:11%;">
<a href="papers/cvpr12seg.pdf" class="buttonT" style="width:100%;">Paper</a><br/>
<p style="margin:1.5px 0px 0px 0px;"></p>
<a href="http://ttic.uchicago.edu/~yaojian/HolisticSceneUnderstanding.html" class="buttonP" style="width:100%;">Project page.</a><br/>
<p style="margin:0px 0px 0px 0px;"></p>
<a class="buttonS" data-toggle="collapse" data-parent="#YaoCVPR12" href="#YaoCVPR12-list" style="width:100%; margin-top:6px; text-decoration: none; color:#FF9933;">Bibtex</a><br/>
        </div>
        -->
      </li>
    </ul>


                   <a data-toggle="collapse" data-parent="#segmentation" href="#segmentation-info">close window</a>
      </div>
      </div>     
      <br/><br/>
</div>


</body></html>